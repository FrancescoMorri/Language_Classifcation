{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_&_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMv4xLHOcyKAwdNUif2juEr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoMorri/Language_Classifcation/blob/main/Notebooks/Model_%26_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKDzDhjHxpql"
      },
      "source": [
        "Define here your **PATH**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taGo7b63wL8l",
        "outputId": "d5a51b36-8b16-486e-efc7-efdedc03598b"
      },
      "source": [
        "#this if you work with Google Drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EIns3jJxoJZ"
      },
      "source": [
        "PATH = \"/content/drive/MyDrive/language\""
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGSTBKL6xxmj"
      },
      "source": [
        "## Importing all the libraries and the class of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQpj_ZgLcmbF"
      },
      "source": [
        "import torch\r\n",
        "import torchvision\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torchvision import datasets, transforms\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.optim.lr_scheduler import StepLR\r\n",
        "import time\r\n",
        "from torchsummary import summary\r\n",
        "import torch.optim as optim\r\n",
        "from torch.optim.lr_scheduler import MultiStepLR\r\n",
        "import numpy as np"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAeF3boPxZW1"
      },
      "source": [
        "class WordsDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.samples = []\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.samples)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        input = self.samples[idx][0]\r\n",
        "        label = self.samples[idx][1]\r\n",
        "        sample = [input, label]\r\n",
        "        return sample\r\n",
        "    \r\n",
        "    def __addsample__(self, spelling, lexicality, max_length=22):\r\n",
        "        if (type(spelling) == type(\"str\")):\r\n",
        "            input = torch.tensor(word_to_vec(spelling, max_length=max_length))\r\n",
        "            label = torch.tensor(label_maker(lexicality))\r\n",
        "            self.samples.append([input, label])\r\n",
        "        else:\r\n",
        "            print(\"Something Strange:\", end='\\t')\r\n",
        "            print(spelling)\r\n",
        "\r\n",
        "    def __removesample__(self, idx=0, value=None):\r\n",
        "        '''\r\n",
        "        If value is something, the element corresponding to that value is removed.\r\n",
        "        Else the element at index idx is popped.\r\n",
        "        '''\r\n",
        "        if (value is not None):\r\n",
        "            self.samples.remove(value)\r\n",
        "        else:\r\n",
        "            self.samples.pop(0)\r\n",
        "        "
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL2z7_hxyE5y"
      },
      "source": [
        "# Loading Dataset and Creating Network Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPBHbbt0wMdW"
      },
      "source": [
        "trainloader = torch.load(PATH+\"/trainloader.pth\")\r\n",
        "testloader = torch.load(PATH+\"/testloader.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W87AaGIDxKKG"
      },
      "source": [
        "class Words_Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.linear1 = nn.Linear(in_features=598, out_features=1024)\r\n",
        "        self.drop1 = nn.Dropout(p=0.2)\r\n",
        "\r\n",
        "        self.linear2 = nn.Linear(in_features=1024, out_features=256)\r\n",
        "        self.drop2 = nn.Dropout(p=0.2)\r\n",
        "\r\n",
        "        self.linear3 = nn.Linear(in_features=256, out_features=128)\r\n",
        "        self.drop3 = nn.Dropout(p=0.2)\r\n",
        "\r\n",
        "        self.linear4 = nn.Linear(in_features=128, out_features=2)\r\n",
        "\r\n",
        "        self.sig = nn.Sigmoid()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = torch.tensor(x)\r\n",
        "        out = self.linear1(x)\r\n",
        "        out = self.drop1(out)\r\n",
        "        out = self.sig(out)\r\n",
        "\r\n",
        "        out = self.linear2(out)\r\n",
        "        out = self.drop2(out)\r\n",
        "        out = self.sig(out)\r\n",
        "\r\n",
        "        out = self.linear3(out)\r\n",
        "        out = self.drop3(out)\r\n",
        "        out = self.sig(out)\r\n",
        "\r\n",
        "        out = self.linear4(out)\r\n",
        "\r\n",
        "        return out"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EniQLxvR3M78"
      },
      "source": [
        "For the training we will use the GPU, even though is a fairly small network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTgxuw3u2k4t",
        "outputId": "d576ba06-fc0b-4d51-edfc-e9a04d589c74"
      },
      "source": [
        "net = Words_Net()\r\n",
        "want_cuda = True\r\n",
        "have_cuda = torch.cuda.is_available()\r\n",
        "if want_cuda and have_cuda:\r\n",
        "    net.cuda()\r\n",
        "    print(torch.cuda.get_device_name())\r\n",
        "else:\r\n",
        "    print (\"No cuda available!\\n\")\r\n",
        "summary(net, (598,))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No cuda available!\n",
            "\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 1024]         613,376\n",
            "           Dropout-2                 [-1, 1024]               0\n",
            "           Sigmoid-3                 [-1, 1024]               0\n",
            "            Linear-4                  [-1, 256]         262,400\n",
            "           Dropout-5                  [-1, 256]               0\n",
            "           Sigmoid-6                  [-1, 256]               0\n",
            "            Linear-7                  [-1, 128]          32,896\n",
            "           Dropout-8                  [-1, 128]               0\n",
            "           Sigmoid-9                  [-1, 128]               0\n",
            "           Linear-10                    [-1, 2]             258\n",
            "================================================================\n",
            "Total params: 908,930\n",
            "Trainable params: 908,930\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.03\n",
            "Params size (MB): 3.47\n",
            "Estimated Total Size (MB): 3.50\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl261nA_3m2A"
      },
      "source": [
        "We define the training parameters, in this case we will use the CrossEntropy loss and the SGD algorithm to train the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lBZcMoL22iL"
      },
      "source": [
        "EPOCH = 1\r\n",
        "learn = 0.1\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=learn, weight_decay = 4e-3)\r\n",
        "#scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [300,600], gamma=0.1)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMdHSvyg4i77"
      },
      "source": [
        "The training function will return the loss of that epoch, mediated over the iteration on the dataset. It may also be added the option to acquire interesting data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56PfvBJT3waO"
      },
      "source": [
        "def training(acquire = False , PATH = None):\r\n",
        "    running_loss = 0.0\r\n",
        "    losst = 0\r\n",
        "    index = 0\r\n",
        "    for i, data in enumerate(trainloader, 0):\r\n",
        "        # get the inputs, maybe they need to be tensors?\r\n",
        "        inputs, labels = data\r\n",
        "        #print(labels.shape)\r\n",
        "        if want_cuda and have_cuda:\r\n",
        "          inputs = inputs.cuda()\r\n",
        "          labels = labels.cuda()\r\n",
        "\r\n",
        "        outputs = net(inputs)\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        losst +=loss.item()\r\n",
        "        index +=1\r\n",
        "    return losst/index"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwY0Q2qF4_8d"
      },
      "source": [
        "Now we can do the actual training of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmPIGtBE46Wd",
        "outputId": "cf2394b0-7aff-47a1-8409-4b029b2cd407"
      },
      "source": [
        "start =time.time()\r\n",
        "graph_data = []\r\n",
        "mean_loss = 0\r\n",
        "for epoch in range(EPOCH):\r\n",
        "    loss = training()    \r\n",
        "    print(\"Epoch: \", epoch, \" Loss: %.10f\"%(loss))\r\n",
        "    mean_loss += loss\r\n",
        "    graph_data.append((epoch, loss))\r\n",
        "    np.savetxt( PATH+\"/graphs/loss_data.csv\", graph_data, delimiter=',')\r\n",
        "    \r\n",
        "    if (epoch % 50 == 49):\r\n",
        "        #net.eval()\r\n",
        "        #y = validate()\r\n",
        "        #val_graph = np.append(val_graph, [[epoch, y]], axis=0)\r\n",
        "        print(\"Mean loss: %.10f\"%(mean_loss/50))\r\n",
        "        mean_loss = 0\r\n",
        "        print('Estimated time: %.3f min' %((EPOCH- epoch)*(time.time() - start)/(60*epoch)) )\r\n",
        "        #torch.save(net.state_dict(), \"/content/drive/My Drive/Saved_Sets/esperimento_7/32b/withLR2e-5/32_lr2e5_1000\")\r\n",
        "    #scheduler.step()\r\n",
        "    net.train()\r\n",
        "        \r\n",
        "elapsed_time = time.time() - start\r\n",
        "print('Finished Training (elapsed time %.3f min)' %(elapsed_time/60))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0  Loss: 0.7036838910\n",
            "Finished Training (elapsed time 0.293 min)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_SQCBtu6RIM"
      },
      "source": [
        "correct = 0\r\n",
        "total = 0\r\n",
        "vloss = 0\r\n",
        "with torch.no_grad():\r\n",
        "    for i, data in enumerate(testloader, 0):\r\n",
        "        inputs, labels = data\r\n",
        "        if want_cuda and torch.cuda.is_available():\r\n",
        "            inputs = inputs.cuda()\r\n",
        "            labels = labels.cuda()\r\n",
        "        outputs = net(inputs)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss += loss.item()\r\n",
        "        _, predicted = torch.max(outputs.data, 1)\r\n",
        "        total += labels.size(0)\r\n",
        "        correct += (predicted == labels).sum().item()\r\n",
        "print(\"Accuracy: \", round(correct/total *100, 4), \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}