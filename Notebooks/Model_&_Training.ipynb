{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_&_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnAyBtgdr+tGxoCV/05Pb3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoMorri/Language_Classification/blob/main/Notebooks/Model_%26_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKDzDhjHxpql"
      },
      "source": [
        "Define here your **PATH**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taGo7b63wL8l",
        "outputId": "569f220a-12fc-4fc9-c695-ec7561d4a325"
      },
      "source": [
        "#this if you work with Google Drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EIns3jJxoJZ"
      },
      "source": [
        "PATH = \"/content/drive/MyDrive/language\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGSTBKL6xxmj"
      },
      "source": [
        "## Importing all the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQpj_ZgLcmbF"
      },
      "source": [
        "import torch\r\n",
        "import torchvision\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torchvision import datasets, transforms\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.optim.lr_scheduler import StepLR\r\n",
        "import time\r\n",
        "from torchsummary import summary\r\n",
        "import torch.optim as optim\r\n",
        "from torch.optim.lr_scheduler import MultiStepLR\r\n",
        "import numpy as np"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J3iMVaJD8BG"
      },
      "source": [
        "# Making the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgTPUBvUEJbB"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x_W4bkzENOV"
      },
      "source": [
        "words = pd.read_csv(PATH+\"/words_all_unique.csv\", usecols=['spelling', 'lexicality'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNDUU61kRoA7"
      },
      "source": [
        "<p> In order to make the dataset we need to encode the words in some way. We will use a simple method: simply assigning a binary vector to each letter, then putting together all the vector that make a word, eventually adding 0s at the end if the word is shorter that the longest word in the dataset</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOmuD6-6EbfP"
      },
      "source": [
        "def word_to_vec(word, max_length):\r\n",
        "    n = len(word)\r\n",
        "    vec = ''\r\n",
        "    for i in range(n):\r\n",
        "        cur_char = word[i]\r\n",
        "        idx = ord(cur_char) - 97\r\n",
        "        tmp = (str(0)*idx) + str(1) + (str(0)*(25-idx))\r\n",
        "        vec = vec + tmp\r\n",
        "    if n < max_length:\r\n",
        "        exce = max_length - n\r\n",
        "        vec = vec + (str(0)*26*exce)\r\n",
        "    output = []\r\n",
        "    for v in vec:\r\n",
        "        output.append(float(v))\r\n",
        "    return output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kUcwybeEnbY"
      },
      "source": [
        "<p> In order to make the dataset we need to encode the words in some way. We will use a simple method: simply assigning a binary vector to each letter, then putting together all the vector that make a word, eventually adding 0s at the end if the word is shorter that the longest word in the dataset</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMhbpTvhTP4g"
      },
      "source": [
        "Now we need a basic function to make the labels vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eQnj_uXEgd4"
      },
      "source": [
        "def label_maker(lexicality):\r\n",
        "    if lexicality == 'W':\r\n",
        "        return 0\r\n",
        "    elif lexicality == 'N':\r\n",
        "        return 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O30vuD1gX3K0"
      },
      "source": [
        "We can now define the Dataset class in the standard way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCBz0c0rKkhm"
      },
      "source": [
        "class WordsDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.samples = []\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.samples)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.to_list()\r\n",
        "        \r\n",
        "        input = self.samples[idx][0]\r\n",
        "        label = self.samples[idx][1]\r\n",
        "        return input, label\r\n",
        "    \r\n",
        "    def __addsample__(self, spelling, lexicality, max_length=5):\r\n",
        "        if (type(spelling) == type(\"str\")):\r\n",
        "            input = torch.tensor(word_to_vec(spelling, max_length=max_length), dtype=torch.float32)\r\n",
        "            if (len(input) > max_length*26):\r\n",
        "                print(\"PROBLEM\")\r\n",
        "                print(len(input), spelling)\r\n",
        "            else:\r\n",
        "                # here if it is torch is for the MSELoss, if it is an INT is for the crossentropy\r\n",
        "                #label = torch.tensor(label_maker(lexicality), dtype=torch.float32)\r\n",
        "                label = label_maker(lexicality)\r\n",
        "                self.samples.append([input, label])\r\n",
        "        else:\r\n",
        "            print(\"Something Strange:\", end='\\t')\r\n",
        "            print(spelling)\r\n",
        "\r\n",
        "    def __removesample__(self, idx=0, value=None):\r\n",
        "        '''\r\n",
        "        If value is something, the element corresponding to that value is removed.\r\n",
        "        Else the element at index idx is popped.\r\n",
        "        '''\r\n",
        "        if (value is not None):\r\n",
        "            self.samples.remove(value)\r\n",
        "        else:\r\n",
        "            self.samples.pop(0)\r\n",
        "        "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEwraSVgFD2u"
      },
      "source": [
        "dataset = WordsDataset()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKhzwqykX9on"
      },
      "source": [
        "The dataset is now empty, we can fill it with all our words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaxCiCzMViZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2458899f-351a-420d-bdef-133c8f24b0e7"
      },
      "source": [
        "MAX_LENGTH = 5\r\n",
        "\r\n",
        "counting = 0\r\n",
        "for w in words[words['lexicality']=='W']['spelling']:\r\n",
        "    if (type(w) == type(\"str\")):\r\n",
        "        if (len(w) > MAX_LENGTH):\r\n",
        "            pass\r\n",
        "        else:\r\n",
        "            dataset.__addsample__(w, 'W')\r\n",
        "            counting += 1\r\n",
        "\r\n",
        "count_non = 0\r\n",
        "for w in words[words['lexicality']=='N']['spelling']:\r\n",
        "    if (type(w) == type(\"str\")):\r\n",
        "        if (len(w) > MAX_LENGTH):\r\n",
        "            pass\r\n",
        "        else:\r\n",
        "            dataset.__addsample__(w, 'N')\r\n",
        "            count_non +=1\r\n",
        "\r\n",
        "    if count_non == counting:\r\n",
        "        break\r\n",
        "\r\n",
        "\r\n",
        "print(\"\\n\\nWords: \",counting)\r\n",
        "print(\"Non-Words: \",count_non)\r\n",
        "print(\"Tot Elements in dataset: \", dataset.__len__())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PROBLEM\n",
            "182 yo-yo\n",
            "PROBLEM\n",
            "198 TRUE\n",
            "PROBLEM\n",
            "154 I\n",
            "PROBLEM\n",
            "154 Inf\n",
            "PROBLEM\n",
            "252 FALSE\n",
            "PROBLEM\n",
            "164 he?te\n",
            "PROBLEM\n",
            "164 we?te\n",
            "PROBLEM\n",
            "164 e?tal\n",
            "\n",
            "\n",
            "Words:  7033\n",
            "Non-Words:  7033\n",
            "Tot Elements in dataset:  42174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJKuajC3JHPh"
      },
      "source": [
        "ratio_test_train = 0.2\r\n",
        "test_el = round(dataset.__len__()*ratio_test_train)\r\n",
        "train_el = dataset.__len__() - test_el\r\n",
        "\r\n",
        "trainset, testset = torch.utils.data.random_split(dataset, [train_el, test_el])\r\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32)\r\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL2z7_hxyE5y"
      },
      "source": [
        "# Creating Network Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R8kF1IA2Xsu"
      },
      "source": [
        "class Words_Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.linear1 = nn.Linear(in_features=130, out_features=64)\r\n",
        "        self.linear2 = nn.Linear(in_features=64, out_features=32)\r\n",
        "        self.linear3 = nn.Linear(in_features=32, out_features=2)\r\n",
        "\r\n",
        "        self.act = nn.ReLU()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.linear1(x)\r\n",
        "        out = self.act(out)\r\n",
        "        out = self.linear2(out)\r\n",
        "        out = self.act(out)\r\n",
        "        out = self.linear3(out)\r\n",
        "\r\n",
        "        return out"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EniQLxvR3M78"
      },
      "source": [
        "For the training we will use the GPU, even though is a fairly small network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTgxuw3u2k4t",
        "outputId": "6218fd56-3f80-4b9f-c76a-6f53f912192c"
      },
      "source": [
        "net = Words_Net()\r\n",
        "want_cuda = True\r\n",
        "have_cuda = torch.cuda.is_available()\r\n",
        "if want_cuda and have_cuda:\r\n",
        "    net.cuda()\r\n",
        "    print(torch.cuda.get_device_name())\r\n",
        "else:\r\n",
        "    print (\"No cuda available!\\n\")\r\n",
        "summary(net, (130,))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla T4\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                   [-1, 64]           8,384\n",
            "              ReLU-2                   [-1, 64]               0\n",
            "            Linear-3                   [-1, 32]           2,080\n",
            "              ReLU-4                   [-1, 32]               0\n",
            "            Linear-5                    [-1, 2]              66\n",
            "================================================================\n",
            "Total params: 10,530\n",
            "Trainable params: 10,530\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.04\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl261nA_3m2A"
      },
      "source": [
        "We define the training parameters, in this case we will use the CrossEntropy loss and the SGD algorithm to train the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lBZcMoL22iL"
      },
      "source": [
        "EPOCH = 900\r\n",
        "learn = 0.1\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=learn, weight_decay = 4e-3)\r\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [300,600], gamma=0.1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMdHSvyg4i77"
      },
      "source": [
        "The training function will return the loss of that epoch, mediated over the iteration on the dataset. It may also be added the option to acquire interesting data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56PfvBJT3waO"
      },
      "source": [
        "def training(acquire = False , PATH = None):\r\n",
        "    running_loss = 0.0\r\n",
        "    losst = 0\r\n",
        "    index = 0\r\n",
        "    for i, data in enumerate(trainloader, 0):\r\n",
        "        # get the inputs, maybe they need to be tensors?\r\n",
        "        inputs, labels = data\r\n",
        "\r\n",
        "        if want_cuda and have_cuda:\r\n",
        "          inputs = inputs.cuda()\r\n",
        "          labels = labels.cuda()\r\n",
        "\r\n",
        "        # need to squeeze if MSELoss\r\n",
        "        outputs = net(inputs)\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        losst +=loss.item()\r\n",
        "        index +=1\r\n",
        "    return losst/index"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwY0Q2qF4_8d"
      },
      "source": [
        "Now we can do the actual training of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmPIGtBE46Wd",
        "outputId": "15e43333-2c5e-4d67-9011-988e5978e5f8"
      },
      "source": [
        "start =time.time()\r\n",
        "graph_data = []\r\n",
        "mean_loss = 0\r\n",
        "for epoch in range(EPOCH):\r\n",
        "    loss = training()    \r\n",
        "    print(\"Epoch: \", epoch, \" Loss: %.10f\"%(loss))\r\n",
        "    mean_loss += loss\r\n",
        "    graph_data.append((epoch, loss))\r\n",
        "    np.savetxt( PATH+\"/graphs/loss_data.csv\", graph_data, delimiter=',')\r\n",
        "    \r\n",
        "    if (epoch % 100 == 99):\r\n",
        "        #net.eval()\r\n",
        "        #y = validate()\r\n",
        "        #val_graph = np.append(val_graph, [[epoch, y]], axis=0)\r\n",
        "        print(\"Mean loss: %.10f\"%(mean_loss/100))\r\n",
        "        mean_loss = 0\r\n",
        "        print('Estimated time: %.3f min' %((EPOCH- epoch)*(time.time() - start)/(60*epoch)) )\r\n",
        "        #torch.save(net.state_dict(), \"/content/drive/My Drive/Saved_Sets/esperimento_7/32b/withLR2e-5/32_lr2e5_1000\")\r\n",
        "    scheduler.step()\r\n",
        "    #net.train()\r\n",
        "        \r\n",
        "elapsed_time = time.time() - start\r\n",
        "torch.save(net.state_dict(), PATH+\"/net/feed_forward_1\")\r\n",
        "print('Finished Training (elapsed time %.3f min)' %(elapsed_time/60))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0  Loss: 0.6873655858\n",
            "Epoch:  1  Loss: 0.6498368101\n",
            "Epoch:  2  Loss: 0.6342645798\n",
            "Epoch:  3  Loss: 0.6229574554\n",
            "Epoch:  4  Loss: 0.6097975092\n",
            "Epoch:  5  Loss: 0.5961713341\n",
            "Epoch:  6  Loss: 0.5838199671\n",
            "Epoch:  7  Loss: 0.5732778877\n",
            "Epoch:  8  Loss: 0.5640498589\n",
            "Epoch:  9  Loss: 0.5555606156\n",
            "Epoch:  10  Loss: 0.5482720257\n",
            "Epoch:  11  Loss: 0.5417290872\n",
            "Epoch:  12  Loss: 0.5359343747\n",
            "Epoch:  13  Loss: 0.5312890830\n",
            "Epoch:  14  Loss: 0.5264382416\n",
            "Epoch:  15  Loss: 0.5224703237\n",
            "Epoch:  16  Loss: 0.5196510894\n",
            "Epoch:  17  Loss: 0.5168048670\n",
            "Epoch:  18  Loss: 0.5145952646\n",
            "Epoch:  19  Loss: 0.5128606592\n",
            "Epoch:  20  Loss: 0.5108685769\n",
            "Epoch:  21  Loss: 0.5089443077\n",
            "Epoch:  22  Loss: 0.5074048250\n",
            "Epoch:  23  Loss: 0.5055898273\n",
            "Epoch:  24  Loss: 0.5046509256\n",
            "Epoch:  25  Loss: 0.5031963109\n",
            "Epoch:  26  Loss: 0.5021120718\n",
            "Epoch:  27  Loss: 0.5004744379\n",
            "Epoch:  28  Loss: 0.4993738315\n",
            "Epoch:  29  Loss: 0.4979964234\n",
            "Epoch:  30  Loss: 0.4968324009\n",
            "Epoch:  31  Loss: 0.4956372948\n",
            "Epoch:  32  Loss: 0.4947534596\n",
            "Epoch:  33  Loss: 0.4938762005\n",
            "Epoch:  34  Loss: 0.4934232376\n",
            "Epoch:  35  Loss: 0.4928033488\n",
            "Epoch:  36  Loss: 0.4919214878\n",
            "Epoch:  37  Loss: 0.4911853152\n",
            "Epoch:  38  Loss: 0.4905316013\n",
            "Epoch:  39  Loss: 0.4900536121\n",
            "Epoch:  40  Loss: 0.4889505050\n",
            "Epoch:  41  Loss: 0.4881542810\n",
            "Epoch:  42  Loss: 0.4871218410\n",
            "Epoch:  43  Loss: 0.4863052404\n",
            "Epoch:  44  Loss: 0.4856253553\n",
            "Epoch:  45  Loss: 0.4850839323\n",
            "Epoch:  46  Loss: 0.4844814224\n",
            "Epoch:  47  Loss: 0.4842529253\n",
            "Epoch:  48  Loss: 0.4837807830\n",
            "Epoch:  49  Loss: 0.4835035943\n",
            "Epoch:  50  Loss: 0.4829000140\n",
            "Epoch:  51  Loss: 0.4823641764\n",
            "Epoch:  52  Loss: 0.4819439935\n",
            "Epoch:  53  Loss: 0.4816935075\n",
            "Epoch:  54  Loss: 0.4805414236\n",
            "Epoch:  55  Loss: 0.4802402816\n",
            "Epoch:  56  Loss: 0.4791882943\n",
            "Epoch:  57  Loss: 0.4792227070\n",
            "Epoch:  58  Loss: 0.4789045519\n",
            "Epoch:  59  Loss: 0.4785632259\n",
            "Epoch:  60  Loss: 0.4780873945\n",
            "Epoch:  61  Loss: 0.4778302676\n",
            "Epoch:  62  Loss: 0.4776236915\n",
            "Epoch:  63  Loss: 0.4772095079\n",
            "Epoch:  64  Loss: 0.4768183249\n",
            "Epoch:  65  Loss: 0.4769742146\n",
            "Epoch:  66  Loss: 0.4771746407\n",
            "Epoch:  67  Loss: 0.4763295168\n",
            "Epoch:  68  Loss: 0.4767823362\n",
            "Epoch:  69  Loss: 0.4760428163\n",
            "Epoch:  70  Loss: 0.4757021758\n",
            "Epoch:  71  Loss: 0.4757813471\n",
            "Epoch:  72  Loss: 0.4758411374\n",
            "Epoch:  73  Loss: 0.4752035220\n",
            "Epoch:  74  Loss: 0.4751196030\n",
            "Epoch:  75  Loss: 0.4752571045\n",
            "Epoch:  76  Loss: 0.4747643658\n",
            "Epoch:  77  Loss: 0.4741841997\n",
            "Epoch:  78  Loss: 0.4740374550\n",
            "Epoch:  79  Loss: 0.4741870006\n",
            "Epoch:  80  Loss: 0.4740972468\n",
            "Epoch:  81  Loss: 0.4735389190\n",
            "Epoch:  82  Loss: 0.4733923050\n",
            "Epoch:  83  Loss: 0.4736595818\n",
            "Epoch:  84  Loss: 0.4737005393\n",
            "Epoch:  85  Loss: 0.4739348032\n",
            "Epoch:  86  Loss: 0.4733433682\n",
            "Epoch:  87  Loss: 0.4731481730\n",
            "Epoch:  88  Loss: 0.4726641396\n",
            "Epoch:  89  Loss: 0.4736622086\n",
            "Epoch:  90  Loss: 0.4736734312\n",
            "Epoch:  91  Loss: 0.4734227028\n",
            "Epoch:  92  Loss: 0.4732141885\n",
            "Epoch:  93  Loss: 0.4730165142\n",
            "Epoch:  94  Loss: 0.4726909215\n",
            "Epoch:  95  Loss: 0.4725185326\n",
            "Epoch:  96  Loss: 0.4729859241\n",
            "Epoch:  97  Loss: 0.4721877464\n",
            "Epoch:  98  Loss: 0.4725360521\n",
            "Epoch:  99  Loss: 0.4725825922\n",
            "Mean loss: 0.4996651868\n",
            "Estimated time: 21.391 min\n",
            "Epoch:  100  Loss: 0.4726709501\n",
            "Epoch:  101  Loss: 0.4728640011\n",
            "Epoch:  102  Loss: 0.4727714359\n",
            "Epoch:  103  Loss: 0.4724545846\n",
            "Epoch:  104  Loss: 0.4724555399\n",
            "Epoch:  105  Loss: 0.4718219322\n",
            "Epoch:  106  Loss: 0.4718747063\n",
            "Epoch:  107  Loss: 0.4719009492\n",
            "Epoch:  108  Loss: 0.4717834264\n",
            "Epoch:  109  Loss: 0.4715466222\n",
            "Epoch:  110  Loss: 0.4707518648\n",
            "Epoch:  111  Loss: 0.4708671062\n",
            "Epoch:  112  Loss: 0.4712049004\n",
            "Epoch:  113  Loss: 0.4713435430\n",
            "Epoch:  114  Loss: 0.4711592140\n",
            "Epoch:  115  Loss: 0.4707936373\n",
            "Epoch:  116  Loss: 0.4710883748\n",
            "Epoch:  117  Loss: 0.4711170329\n",
            "Epoch:  118  Loss: 0.4711322881\n",
            "Epoch:  119  Loss: 0.4717230067\n",
            "Epoch:  120  Loss: 0.4710036063\n",
            "Epoch:  121  Loss: 0.4715353969\n",
            "Epoch:  122  Loss: 0.4707312831\n",
            "Epoch:  123  Loss: 0.4709648194\n",
            "Epoch:  124  Loss: 0.4705633985\n",
            "Epoch:  125  Loss: 0.4705027658\n",
            "Epoch:  126  Loss: 0.4699010750\n",
            "Epoch:  127  Loss: 0.4701265989\n",
            "Epoch:  128  Loss: 0.4704566767\n",
            "Epoch:  129  Loss: 0.4700145203\n",
            "Epoch:  130  Loss: 0.4703865826\n",
            "Epoch:  131  Loss: 0.4698579550\n",
            "Epoch:  132  Loss: 0.4699431503\n",
            "Epoch:  133  Loss: 0.4697241856\n",
            "Epoch:  134  Loss: 0.4697049190\n",
            "Epoch:  135  Loss: 0.4692917598\n",
            "Epoch:  136  Loss: 0.4694306852\n",
            "Epoch:  137  Loss: 0.4692751729\n",
            "Epoch:  138  Loss: 0.4692474669\n",
            "Epoch:  139  Loss: 0.4691394328\n",
            "Epoch:  140  Loss: 0.4690577432\n",
            "Epoch:  141  Loss: 0.4683065275\n",
            "Epoch:  142  Loss: 0.4687805047\n",
            "Epoch:  143  Loss: 0.4686439992\n",
            "Epoch:  144  Loss: 0.4690255061\n",
            "Epoch:  145  Loss: 0.4686981787\n",
            "Epoch:  146  Loss: 0.4686259747\n",
            "Epoch:  147  Loss: 0.4686994398\n",
            "Epoch:  148  Loss: 0.4684770930\n",
            "Epoch:  149  Loss: 0.4687691465\n",
            "Epoch:  150  Loss: 0.4682638603\n",
            "Epoch:  151  Loss: 0.4682067679\n",
            "Epoch:  152  Loss: 0.4684097170\n",
            "Epoch:  153  Loss: 0.4686977224\n",
            "Epoch:  154  Loss: 0.4682596639\n",
            "Epoch:  155  Loss: 0.4681800096\n",
            "Epoch:  156  Loss: 0.4681473506\n",
            "Epoch:  157  Loss: 0.4679504788\n",
            "Epoch:  158  Loss: 0.4679835127\n",
            "Epoch:  159  Loss: 0.4681211981\n",
            "Epoch:  160  Loss: 0.4677949871\n",
            "Epoch:  161  Loss: 0.4674798766\n",
            "Epoch:  162  Loss: 0.4675174323\n",
            "Epoch:  163  Loss: 0.4672792801\n",
            "Epoch:  164  Loss: 0.4662605243\n",
            "Epoch:  165  Loss: 0.4676049719\n",
            "Epoch:  166  Loss: 0.4670472293\n",
            "Epoch:  167  Loss: 0.4673043888\n",
            "Epoch:  168  Loss: 0.4668671483\n",
            "Epoch:  169  Loss: 0.4667640856\n",
            "Epoch:  170  Loss: 0.4668367191\n",
            "Epoch:  171  Loss: 0.4671110984\n",
            "Epoch:  172  Loss: 0.4666882566\n",
            "Epoch:  173  Loss: 0.4670783558\n",
            "Epoch:  174  Loss: 0.4668883410\n",
            "Epoch:  175  Loss: 0.4669544094\n",
            "Epoch:  176  Loss: 0.4665888215\n",
            "Epoch:  177  Loss: 0.4666212123\n",
            "Epoch:  178  Loss: 0.4663080101\n",
            "Epoch:  179  Loss: 0.4663907817\n",
            "Epoch:  180  Loss: 0.4659355592\n",
            "Epoch:  181  Loss: 0.4660774919\n",
            "Epoch:  182  Loss: 0.4662091020\n",
            "Epoch:  183  Loss: 0.4659455861\n",
            "Epoch:  184  Loss: 0.4656067778\n",
            "Epoch:  185  Loss: 0.4661967634\n",
            "Epoch:  186  Loss: 0.4656522178\n",
            "Epoch:  187  Loss: 0.4658916876\n",
            "Epoch:  188  Loss: 0.4661650015\n",
            "Epoch:  189  Loss: 0.4657130357\n",
            "Epoch:  190  Loss: 0.4658231512\n",
            "Epoch:  191  Loss: 0.4649730366\n",
            "Epoch:  192  Loss: 0.4654319377\n",
            "Epoch:  193  Loss: 0.4654705819\n",
            "Epoch:  194  Loss: 0.4659317233\n",
            "Epoch:  195  Loss: 0.4656406014\n",
            "Epoch:  196  Loss: 0.4650548466\n",
            "Epoch:  197  Loss: 0.4648896338\n",
            "Epoch:  198  Loss: 0.4654038618\n",
            "Epoch:  199  Loss: 0.4652008754\n",
            "Mean loss: 0.4685703036\n",
            "Estimated time: 18.625 min\n",
            "Epoch:  200  Loss: 0.4650895501\n",
            "Epoch:  201  Loss: 0.4648511394\n",
            "Epoch:  202  Loss: 0.4648874154\n",
            "Epoch:  203  Loss: 0.4647409067\n",
            "Epoch:  204  Loss: 0.4650810276\n",
            "Epoch:  205  Loss: 0.4649787393\n",
            "Epoch:  206  Loss: 0.4642525440\n",
            "Epoch:  207  Loss: 0.4641253630\n",
            "Epoch:  208  Loss: 0.4641344981\n",
            "Epoch:  209  Loss: 0.4637998625\n",
            "Epoch:  210  Loss: 0.4641095097\n",
            "Epoch:  211  Loss: 0.4640040851\n",
            "Epoch:  212  Loss: 0.4640084678\n",
            "Epoch:  213  Loss: 0.4639418957\n",
            "Epoch:  214  Loss: 0.4634718373\n",
            "Epoch:  215  Loss: 0.4627740469\n",
            "Epoch:  216  Loss: 0.4630418775\n",
            "Epoch:  217  Loss: 0.4633152844\n",
            "Epoch:  218  Loss: 0.4631588511\n",
            "Epoch:  219  Loss: 0.4629551523\n",
            "Epoch:  220  Loss: 0.4630703346\n",
            "Epoch:  221  Loss: 0.4623686485\n",
            "Epoch:  222  Loss: 0.4630073576\n",
            "Epoch:  223  Loss: 0.4626343479\n",
            "Epoch:  224  Loss: 0.4625310328\n",
            "Epoch:  225  Loss: 0.4619896549\n",
            "Epoch:  226  Loss: 0.4615805236\n",
            "Epoch:  227  Loss: 0.4615780229\n",
            "Epoch:  228  Loss: 0.4617069365\n",
            "Epoch:  229  Loss: 0.4611875654\n",
            "Epoch:  230  Loss: 0.4613131074\n",
            "Epoch:  231  Loss: 0.4613928426\n",
            "Epoch:  232  Loss: 0.4609966555\n",
            "Epoch:  233  Loss: 0.4609411665\n",
            "Epoch:  234  Loss: 0.4606561565\n",
            "Epoch:  235  Loss: 0.4608322937\n",
            "Epoch:  236  Loss: 0.4605008269\n",
            "Epoch:  237  Loss: 0.4605153296\n",
            "Epoch:  238  Loss: 0.4610041446\n",
            "Epoch:  239  Loss: 0.4600341684\n",
            "Epoch:  240  Loss: 0.4604100940\n",
            "Epoch:  241  Loss: 0.4610777244\n",
            "Epoch:  242  Loss: 0.4607898085\n",
            "Epoch:  243  Loss: 0.4606831913\n",
            "Epoch:  244  Loss: 0.4606917900\n",
            "Epoch:  245  Loss: 0.4605247874\n",
            "Epoch:  246  Loss: 0.4605559167\n",
            "Epoch:  247  Loss: 0.4604935091\n",
            "Epoch:  248  Loss: 0.4606574312\n",
            "Epoch:  249  Loss: 0.4607948005\n",
            "Epoch:  250  Loss: 0.4601951214\n",
            "Epoch:  251  Loss: 0.4602795653\n",
            "Epoch:  252  Loss: 0.4594149646\n",
            "Epoch:  253  Loss: 0.4604090737\n",
            "Epoch:  254  Loss: 0.4604612076\n",
            "Epoch:  255  Loss: 0.4594867357\n",
            "Epoch:  256  Loss: 0.4597251033\n",
            "Epoch:  257  Loss: 0.4593237058\n",
            "Epoch:  258  Loss: 0.4594586946\n",
            "Epoch:  259  Loss: 0.4590948244\n",
            "Epoch:  260  Loss: 0.4595857845\n",
            "Epoch:  261  Loss: 0.4587824475\n",
            "Epoch:  262  Loss: 0.4591863294\n",
            "Epoch:  263  Loss: 0.4597191160\n",
            "Epoch:  264  Loss: 0.4588795042\n",
            "Epoch:  265  Loss: 0.4592978137\n",
            "Epoch:  266  Loss: 0.4588975530\n",
            "Epoch:  267  Loss: 0.4588151398\n",
            "Epoch:  268  Loss: 0.4592335148\n",
            "Epoch:  269  Loss: 0.4585632158\n",
            "Epoch:  270  Loss: 0.4588029931\n",
            "Epoch:  271  Loss: 0.4587183241\n",
            "Epoch:  272  Loss: 0.4585525224\n",
            "Epoch:  273  Loss: 0.4587154716\n",
            "Epoch:  274  Loss: 0.4584735468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_SQCBtu6RIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef3169d7-8b3c-4964-85e2-29fd583e168d"
      },
      "source": [
        "correct = 0\r\n",
        "total = 0\r\n",
        "vloss = 0\r\n",
        "with torch.no_grad():\r\n",
        "    for i, data in enumerate(testloader, 0):\r\n",
        "        inputs, labels = data\r\n",
        "        if want_cuda and torch.cuda.is_available():\r\n",
        "            inputs = inputs.cuda()\r\n",
        "            labels = labels.cuda()\r\n",
        "        outputs = net(inputs).squeeze()\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss += loss.item()\r\n",
        "        #this for MSELoss\r\n",
        "        # predicted = torch.round(outputs)\r\n",
        "        #this for crossentropy\r\n",
        "        _, predicted = torch.max(outputs,1)\r\n",
        "        total += labels.size(0)\r\n",
        "        correct += (predicted == labels).sum().item()\r\n",
        "print(\"Accuracy: \", round(correct/total *100, 4), \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  65.5333 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPtVhLN3yeww"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}