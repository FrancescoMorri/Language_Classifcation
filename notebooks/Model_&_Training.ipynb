{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_&_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ciycA-wzt_tl",
        "oLNDyzTmDV2n",
        "aROaOz7Xxggl"
      ],
      "authorship_tag": "ABX9TyOpOgcwiNXTU7eGXo8T6XbT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoMorri/Language_Classification/blob/main/notebooks/Model_%26_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKDzDhjHxpql"
      },
      "source": [
        "Define here your **PATH**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taGo7b63wL8l",
        "outputId": "c6ceff5d-a565-432c-b743-59746d8fc6d3"
      },
      "source": [
        "#this if you work with Google Drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EIns3jJxoJZ"
      },
      "source": [
        "PATH = \"/content/drive/MyDrive/language/\""
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGSTBKL6xxmj"
      },
      "source": [
        "## Importing all the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQpj_ZgLcmbF"
      },
      "source": [
        "import torch\r\n",
        "import torchvision\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torchvision import datasets, transforms\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.optim.lr_scheduler import StepLR\r\n",
        "import time\r\n",
        "from torchsummary import summary\r\n",
        "import torch.optim as optim\r\n",
        "from torch.optim.lr_scheduler import MultiStepLR\r\n",
        "import numpy as np"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J3iMVaJD8BG"
      },
      "source": [
        "# Making the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Avb8jr8t7F5"
      },
      "source": [
        "## Basic Word Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgTPUBvUEJbB"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x_W4bkzENOV"
      },
      "source": [
        "words = pd.read_csv(PATH+\"/words_all_unique.csv\", usecols=['spelling', 'lexicality'])"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNDUU61kRoA7"
      },
      "source": [
        "In order to make the dataset we need to encode the words in some way. We will use a simple method: simply assigning a binary vector to each letter, then putting together all the vector that make a word, eventually adding 0s at the end if the word is shorter that the longest word in the dataset.</br>\r\n",
        "</br>\r\n",
        "If we already have everything we just need to load the dataloader.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOmuD6-6EbfP"
      },
      "source": [
        "def word_to_vec(word, max_length):\r\n",
        "    n = len(word)\r\n",
        "    vec = ''\r\n",
        "    for i in range(n):\r\n",
        "        cur_char = word[i]\r\n",
        "        idx = ord(cur_char) - 97\r\n",
        "        tmp = (str(0)*idx) + str(1) + (str(0)*(25-idx))\r\n",
        "        vec = vec + tmp\r\n",
        "    if n < max_length:\r\n",
        "        exce = max_length - n\r\n",
        "        vec = vec + (str(0)*26*exce)\r\n",
        "    output = []\r\n",
        "    for v in vec:\r\n",
        "        output.append(float(v))\r\n",
        "    return output"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7PP7_3BMOJb"
      },
      "source": [
        "def word_to_vec2(word, max_length):\r\n",
        "    len_w = len(word)\r\n",
        "    chars = [ord(c) for c in word]\r\n",
        "    max_char = 122. # this is z\r\n",
        "    normal = [round(c/max_char, 5) for c in chars]\r\n",
        "    if len_w < max_length:\r\n",
        "        diff = max_length - len_w\r\n",
        "        zeros =  [0 for i in range(diff)]\r\n",
        "        normal.extend(zeros)\r\n",
        "    return normal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isICQ_zwNXDM",
        "outputId": "f8ad1abd-8612-4b59-d268-a9474261297a"
      },
      "source": [
        "print(word_to_vec2(\"hello\", max_length=5), word_to_vec2(\"hellk\", max_length=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.85246, 0.82787, 0.88525, 0.88525, 0.90984] [0.85246, 0.82787, 0.88525, 0.88525, 0.87705]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMhbpTvhTP4g"
      },
      "source": [
        "Now we need a basic function to make the labels vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eQnj_uXEgd4"
      },
      "source": [
        "def label_maker(lexicality):\r\n",
        "    if lexicality == 'W':\r\n",
        "        return 0\r\n",
        "    elif lexicality == 'N':\r\n",
        "        return 1"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O30vuD1gX3K0"
      },
      "source": [
        "We can now define the Dataset class in the standard way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCBz0c0rKkhm"
      },
      "source": [
        "class WordsDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.samples = []\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.samples)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.to_list()\r\n",
        "        \r\n",
        "        input = self.samples[idx][0]\r\n",
        "        label = self.samples[idx][1]\r\n",
        "        return input, label\r\n",
        "    \r\n",
        "    def __addsample__(self, spelling, lexicality, max_length=4):\r\n",
        "        if (type(spelling) == type(\"str\")):\r\n",
        "            input = torch.tensor(word_to_vec(spelling, max_length=max_length), dtype=torch.float32)\r\n",
        "            #check = [False if (i > 1. or i < 0.) else True for i in input]\r\n",
        "            #if not all(check):\r\n",
        "            #    print(\"PROBLEM\")\r\n",
        "            #    print(len(input), spelling)\r\n",
        "            if (len(input) > max_length*26):\r\n",
        "                print(\"PROBLEM\")\r\n",
        "                print(len(input), spelling)\r\n",
        "            else:\r\n",
        "                # here if it is torch is for the MSELoss, if it is an INT is for the crossentropy\r\n",
        "                label = torch.tensor(label_maker(lexicality), dtype=torch.float32)\r\n",
        "                #label = label_maker(lexicality)\r\n",
        "                self.samples.append([input, label])\r\n",
        "        else:\r\n",
        "            print(\"Something Strange:\", end='\\t')\r\n",
        "            print(spelling)\r\n",
        "\r\n",
        "    def __removesample__(self, idx=0, value=None):\r\n",
        "        '''\r\n",
        "        If value is something, the element corresponding to that value is removed.\r\n",
        "        Else the element at index idx is popped.\r\n",
        "        '''\r\n",
        "        if (value is not None):\r\n",
        "            self.samples.remove(value)\r\n",
        "        else:\r\n",
        "            self.samples.pop(0)\r\n",
        "        "
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEwraSVgFD2u"
      },
      "source": [
        "dataset = WordsDataset()"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKhzwqykX9on"
      },
      "source": [
        "The dataset is now empty, we can fill it with all our words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaxCiCzMViZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e03e03b4-5ed5-484f-c78a-6aa72df62035"
      },
      "source": [
        "MAX_LENGTH = 12\r\n",
        "\r\n",
        "counting = 0\r\n",
        "for w in words[words['lexicality']=='W']['spelling']:\r\n",
        "    if (type(w) == type(\"str\")):\r\n",
        "        if (len(w) > MAX_LENGTH):\r\n",
        "            pass\r\n",
        "        else:\r\n",
        "            dataset.__addsample__(w, 'W', max_length=MAX_LENGTH)\r\n",
        "            counting += 1\r\n",
        "\r\n",
        "count_non = 0\r\n",
        "for w in words[words['lexicality']=='N']['spelling']:\r\n",
        "    if (type(w) == type(\"str\")):\r\n",
        "        if (len(w) > MAX_LENGTH):\r\n",
        "            pass\r\n",
        "        else:\r\n",
        "            dataset.__addsample__(w, 'N', max_length=MAX_LENGTH)\r\n",
        "            count_non +=1\r\n",
        "\r\n",
        "    if count_non == counting:\r\n",
        "        break\r\n",
        "\r\n",
        "\r\n",
        "print(\"\\n\\nWords: \",counting)\r\n",
        "print(\"Non-Words: \",count_non)\r\n",
        "print(\"Tot Elements in dataset: \", dataset.__len__())"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PROBLEM\n",
            "364 itsy-bitsy\n",
            "PROBLEM\n",
            "364 t-shirt\n",
            "PROBLEM\n",
            "364 yo-yo\n",
            "PROBLEM\n",
            "364 itty-bitty\n",
            "PROBLEM\n",
            "380 TRUE\n",
            "PROBLEM\n",
            "336 I\n",
            "PROBLEM\n",
            "336 Inf\n",
            "PROBLEM\n",
            "434 FALSE\n",
            "PROBLEM\n",
            "364 teeny-weeny\n",
            "PROBLEM\n",
            "346 se?tes\n",
            "PROBLEM\n",
            "346 desple?tive\n",
            "PROBLEM\n",
            "346 clage?tet\n",
            "PROBLEM\n",
            "346 ete?tion\n",
            "PROBLEM\n",
            "346 he?te\n",
            "PROBLEM\n",
            "346 we?te\n",
            "PROBLEM\n",
            "314 f_ttered\n",
            "PROBLEM\n",
            "346 che?ting\n",
            "PROBLEM\n",
            "346 ase?tus\n",
            "PROBLEM\n",
            "346 apple?te\n",
            "PROBLEM\n",
            "346 se?tard\n",
            "PROBLEM\n",
            "346 duppe?tic\n",
            "PROBLEM\n",
            "346 line?te\n",
            "PROBLEM\n",
            "346 he?ting\n",
            "PROBLEM\n",
            "346 porre?te\n",
            "PROBLEM\n",
            "346 e?tal\n",
            "\n",
            "\n",
            "Words:  56069\n",
            "Non-Words:  56069\n",
            "Tot Elements in dataset:  112113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJKuajC3JHPh"
      },
      "source": [
        "ratio_test_train = 0.25\r\n",
        "test_el = round(dataset.__len__()*ratio_test_train)\r\n",
        "train_el = dataset.__len__() - test_el\r\n",
        "\r\n",
        "trainset, testset = torch.utils.data.random_split(dataset, [train_el, test_el])\r\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32)\r\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t57y0-OdxgMB"
      },
      "source": [
        "Here we can save the dataloader in order to have reproducible results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-S93brYxlQS"
      },
      "source": [
        "torch.save(trainloader, PATH+\"trainloader_12chars_MSE.pth\")\r\n",
        "torch.save(testloader, PATH+\"testloader_12chars_MSE.pth\")"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciycA-wzt_tl"
      },
      "source": [
        "## Char2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzkOHyUCxKgl"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKDWMkIIxKgn"
      },
      "source": [
        "words = pd.read_csv(PATH+\"/words_all_unique.csv\", usecols=['spelling', 'lexicality'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5OihxqSuQ0b",
        "outputId": "f4ea3b86-ebd3-494e-9214-9b0cb0083073"
      },
      "source": [
        "!pip install chars2vec\r\n",
        "import chars2vec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting chars2vec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/0a/8c327aae23e0532d239ec7b30446aca765eb5d9547b4c4b09cdd82e49797/chars2vec-0.1.7.tar.gz (8.1MB)\n",
            "\u001b[K     |████████████████████████████████| 8.1MB 6.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: chars2vec\n",
            "  Building wheel for chars2vec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chars2vec: filename=chars2vec-0.1.7-cp36-none-any.whl size=8111095 sha256=eb1052fd704a7884612f4c0f4bd07ccc748d84880e16f231696409a1c244ba33\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/b6/65/d7e778ef1213ec77d315aea0f536068b96e36cc94c02abbfde\n",
            "Successfully built chars2vec\n",
            "Installing collected packages: chars2vec\n",
            "Successfully installed chars2vec-0.1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0ksEU76rW3T"
      },
      "source": [
        "from torch.utils.data import Dataset\r\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOGWzCinrbMI"
      },
      "source": [
        "def label_maker(lexicality):\r\n",
        "    if lexicality == 'W':\r\n",
        "        return 0\r\n",
        "    elif lexicality == 'N':\r\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVEZdKmfroz9"
      },
      "source": [
        "class WordsDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.samples = []\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.samples)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.to_list()\r\n",
        "        \r\n",
        "        input = self.samples[idx][0]\r\n",
        "        label = self.samples[idx][1]\r\n",
        "        return input, label\r\n",
        "    \r\n",
        "    def __addsample__(self, vector_word, lexicality):\r\n",
        "\r\n",
        "        input = torch.tensor(vector_word, dtype=torch.float32)\r\n",
        "        #label = torch.tensor(label_maker(lexicality), dtype=torch.float32)\r\n",
        "        label = label_maker(lexicality)\r\n",
        "        self.samples.append([input, label])\r\n",
        "\r\n",
        "    def __removesample__(self, idx=0, value=None):\r\n",
        "        '''\r\n",
        "        If value is something, the element corresponding to that value is removed.\r\n",
        "        Else the element at index idx is popped.\r\n",
        "        '''\r\n",
        "        if (value is not None):\r\n",
        "            self.samples.remove(value)\r\n",
        "        else:\r\n",
        "            self.samples.pop(0)\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jieM3DBNsci2"
      },
      "source": [
        "c2v_model = chars2vec.load_model('eng_50')\r\n",
        "real = []\r\n",
        "for i,w in enumerate(words[words['lexicality']=='W']['spelling']):\r\n",
        "    if (type(w) == type(\"str\")):\r\n",
        "        real.append(w)\r\n",
        "\r\n",
        "\r\n",
        "nonw = []\r\n",
        "for i,w in enumerate(words[words['lexicality']=='N']['spelling']):\r\n",
        "    if (type(w) == type(\"str\")):\r\n",
        "        nonw.append(w)\r\n",
        "\r\n",
        "\r\n",
        "real_word_embed = c2v_model.vectorize_words(real)\r\n",
        "non_word_embed = c2v_model.vectorize_words(nonw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQHdYjWysnCs",
        "outputId": "26d67604-f0f3-4f4d-e04f-a4e4f3c0778a"
      },
      "source": [
        "print(real_word_embed.shape)\r\n",
        "print(non_word_embed.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(61853, 50)\n",
            "(329845, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLhDVJhGtHbG"
      },
      "source": [
        "dataset = WordsDataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3DdDfXos9jb",
        "outputId": "858e5ea8-a13e-46a7-933a-add66d927c67"
      },
      "source": [
        "count = 0\r\n",
        "for rw in real_word_embed:\r\n",
        "    dataset.__addsample__(rw, \"W\")\r\n",
        "    count += 1\r\n",
        "count2 = 0\r\n",
        "for nw in non_word_embed:\r\n",
        "    dataset.__addsample__(nw, \"N\")\r\n",
        "    count2 +=1\r\n",
        "    if count2 == count:\r\n",
        "        break\r\n",
        "\r\n",
        "print(dataset.__len__())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "123706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaitguTntjra"
      },
      "source": [
        "ratio_test_train = 0.2\r\n",
        "test_el = round(dataset.__len__()*ratio_test_train)\r\n",
        "train_el = dataset.__len__() - test_el\r\n",
        "\r\n",
        "trainset, testset = torch.utils.data.random_split(dataset, [train_el, test_el])\r\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128)\r\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0OXJtm6tvwZ"
      },
      "source": [
        "torch.save(trainloader, PATH+\"trainloader_char2vec_cross.pth\")\r\n",
        "torch.save(testloader, PATH+\"testloader_char2vec_cross.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLNDyzTmDV2n"
      },
      "source": [
        "## Bigrams Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQBF759RDYpd"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA6rMQf6DYpf"
      },
      "source": [
        "words = pd.read_csv(PATH+\"/words_all_unique.csv\", usecols=['spelling', 'lexicality'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OInnz_GN2sR8"
      },
      "source": [
        "def create_dict(text, normal=False):\r\n",
        "    bigrams = {}\r\n",
        "    for w in text:\r\n",
        "        for i in range(0, len(w)-1):\r\n",
        "            big = w[i]+w[i+1]\r\n",
        "            keys = bigrams.keys()\r\n",
        "            check = [k == big for k in bigrams.keys()]\r\n",
        "            if any(check):\r\n",
        "                bigrams[big] += 1\r\n",
        "            else:\r\n",
        "                bigrams[big] = 1\r\n",
        "\r\n",
        "    if normal:\r\n",
        "        max_dict = max(bigrams.values())\r\n",
        "        bigrams = {k: v/max_dict for k,v in bigrams.items()}\r\n",
        "    return bigrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb6J88GI23N6"
      },
      "source": [
        "def encode_words(words, dictio):\r\n",
        "    data = []\r\n",
        "    for w in words:\r\n",
        "        vect = []\r\n",
        "        for i in range(0, len(w)-1):\r\n",
        "            big = w[i] + w[i+1]\r\n",
        "            val = dictio[big]\r\n",
        "            vect.append(val)\r\n",
        "        data.append(vect)\r\n",
        "\r\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBjEQCDF2pT8"
      },
      "source": [
        "real = []\r\n",
        "for i,w in enumerate(words[words['lexicality']=='W']['spelling']):\r\n",
        "    if (type(w) == type(\"str\")):\r\n",
        "        real.append(w)\r\n",
        "\r\n",
        "\r\n",
        "nonw = []\r\n",
        "for i,w in enumerate(words[words['lexicality']=='N']['spelling']):\r\n",
        "    if (type(w) == type(\"str\")):\r\n",
        "        nonw.append(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p-MWZe_2q7N"
      },
      "source": [
        "real_count = len(real)\r\n",
        "select_non = nonw[:real_count]\r\n",
        "tot = []\r\n",
        "tot.extend(real)\r\n",
        "tot.extend(select_non)\r\n",
        "dictionary = create_dict(tot, normal=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d1eI4yTCDtt"
      },
      "source": [
        "real_encoded = encode_words(real, dictio=dictionary)\r\n",
        "non_encoded = encode_words(select_non, dictio=dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FAruodQCb0F"
      },
      "source": [
        "def label_maker(lexicality):\r\n",
        "    if lexicality == 'W':\r\n",
        "        return 0\r\n",
        "    elif lexicality == 'N':\r\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_zLRqksCNLQ"
      },
      "source": [
        "class WordsDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.samples = []\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.samples)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.to_list()\r\n",
        "        \r\n",
        "        input = self.samples[idx][0]\r\n",
        "        label = self.samples[idx][1]\r\n",
        "        return input, label\r\n",
        "    \r\n",
        "    def __addsample__(self, vector_word, lexicality, max_length):\r\n",
        "        \r\n",
        "        if len(vector_word) < max_length:\r\n",
        "            diff = max_length-len(vector_word)\r\n",
        "            zeros = [0 for i in range(diff)]\r\n",
        "            vector_word.extend(zeros)\r\n",
        "        \r\n",
        "        input = torch.tensor(vector_word, dtype=torch.float32)\r\n",
        "        #label = torch.tensor(label_maker(lexicality), dtype=torch.float32)\r\n",
        "        label = label_maker(lexicality)\r\n",
        "        self.samples.append([input, label])\r\n",
        "\r\n",
        "    def __removesample__(self, idx=0, value=None):\r\n",
        "        '''\r\n",
        "        If value is something, the element corresponding to that value is removed.\r\n",
        "        Else the element at index idx is popped.\r\n",
        "        '''\r\n",
        "        if (value is not None):\r\n",
        "            self.samples.remove(value)\r\n",
        "        else:\r\n",
        "            self.samples.pop(0)\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT6TeFhsCmig"
      },
      "source": [
        "dataset = WordsDataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQA-G-oaCmij",
        "outputId": "93c8d686-27a9-4a7e-df7c-32a46b33d4af"
      },
      "source": [
        "MAX_LENGTH = 21\r\n",
        "\r\n",
        "count = 0\r\n",
        "for rw in real_encoded:\r\n",
        "    if len(rw) < MAX_LENGTH:\r\n",
        "        dataset.__addsample__(rw, \"W\", MAX_LENGTH)\r\n",
        "        count += 1\r\n",
        "\r\n",
        "count2 = 0\r\n",
        "for nw in non_encoded:\r\n",
        "    if len(nw) < MAX_LENGTH:\r\n",
        "        dataset.__addsample__(nw, \"N\", MAX_LENGTH)\r\n",
        "        count2 +=1\r\n",
        "        if count2 == count:\r\n",
        "            break\r\n",
        "\r\n",
        "print(dataset.__len__())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "123700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7p_2J0jCmim"
      },
      "source": [
        "ratio_test_train = 0.2\r\n",
        "test_el = round(dataset.__len__()*ratio_test_train)\r\n",
        "train_el = dataset.__len__() - test_el\r\n",
        "\r\n",
        "trainset, testset = torch.utils.data.random_split(dataset, [train_el, test_el])\r\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128)\r\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVP1AI8LCmin"
      },
      "source": [
        "torch.save(trainloader, PATH+\"trainloader_bigrams_complete_cross.pth\")\r\n",
        "torch.save(testloader, PATH+\"testloader_bigrams_complete_cross.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL2z7_hxyE5y"
      },
      "source": [
        "## Creating Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aROaOz7Xxggl"
      },
      "source": [
        "### Conv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R8kF1IA2Xsu"
      },
      "source": [
        "class Conv_Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=5) #50*1 -> 46*8\r\n",
        "        self.drop1 = nn.Dropout()\r\n",
        "        self.batch1 = nn.BatchNorm1d(num_features=8)\r\n",
        "        self.max1 = nn.MaxPool1d(kernel_size=2, stride=2) #46*8 -> 23*8\r\n",
        "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3) #23*8 -> 21*16\r\n",
        "        self.drop2 = nn.Dropout()\r\n",
        "        self.batch2 = nn.BatchNorm1d(num_features=16)\r\n",
        "        self.max2 = nn.MaxPool1d(kernel_size=3, stride=2) #21*16 -> 10*16\r\n",
        "        #self.conv3 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5) #42*16 -> 42*16\r\n",
        "        #self.drop3 = nn.Dropout()\r\n",
        "        #self.batch3 = nn.BatchNorm1d(num_features=16)\r\n",
        "\r\n",
        "        self.linear1 = nn.Linear(in_features=10*16, out_features=32)\r\n",
        "        self.linear2 = nn.Linear(in_features=32, out_features=2)\r\n",
        "\r\n",
        "        self.act = nn.ReLU()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.drop1(out)\r\n",
        "        out = self.batch1(out)\r\n",
        "        out = self.act(out)\r\n",
        "        out = self.max1(out)\r\n",
        "\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.drop2(out)\r\n",
        "        out = self.batch2(out)\r\n",
        "        out = self.act(out)\r\n",
        "        out = self.max2(out)\r\n",
        "\r\n",
        "        out = torch.flatten(out, 1)\r\n",
        "\r\n",
        "        out = self.linear1(out)\r\n",
        "        out = self.act(out)\r\n",
        "        out = self.linear2(out)\r\n",
        "\r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRFxUO6qxjRk"
      },
      "source": [
        "### Feedforward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RUst0bu-ilw"
      },
      "source": [
        "class Words_Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.linear1 = nn.Linear(312, 400)\r\n",
        "        self.batch1 = nn.BatchNorm1d(400) \r\n",
        "        self.drop1 = nn.Dropout()\r\n",
        "        self.linear2 = nn.Linear(400, 256)\r\n",
        "        self.batch2 = nn.BatchNorm1d(256)\r\n",
        "        self.drop2 = nn.Dropout()\r\n",
        "        self.linear3 = nn.Linear(256, 64)\r\n",
        "        #self.batch3 = nn.BatchNorm1d(64)\r\n",
        "        #self.drop3 = nn.Dropout()\r\n",
        "        self.linear4 = nn.Linear(64, 32)\r\n",
        "        #self.drop4 = nn.Dropout()\r\n",
        "        self.linear5 = nn.Linear(32, 1)\r\n",
        "\r\n",
        "        self.act = nn.ReLU()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.linear1(x)\r\n",
        "        out = self.drop1(out)\r\n",
        "        out = self.batch1(out)\r\n",
        "        out = self.act(out)\r\n",
        "\r\n",
        "        out = self.linear2(out)\r\n",
        "        out = self.drop2(out)\r\n",
        "        out = self.batch2(out)\r\n",
        "        out = self.act(out)\r\n",
        "\r\n",
        "        out = self.linear3(out)\r\n",
        "        #out = self.drop3(out)\r\n",
        "        #out = self.batch3(out)\r\n",
        "        out = self.act(out)\r\n",
        "\r\n",
        "        out = self.linear4(out)\r\n",
        "        #out = self.drop4(out)\r\n",
        "        out = self.act(out)\r\n",
        "\r\n",
        "        out = self.linear5(out)\r\n",
        "\r\n",
        "        return out\r\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAceI2n7yGZv"
      },
      "source": [
        "### Load the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rW83HVmyJCT"
      },
      "source": [
        "trainloader = torch.load(PATH+\"trainloader_12chars_MSE.pth\")\r\n",
        "testloader = torch.load(PATH+\"testloader_12chars_MSE.pth\")"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EniQLxvR3M78"
      },
      "source": [
        "## Training\r\n",
        "For the training we will use the GPU, even though is a fairly small network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTgxuw3u2k4t",
        "outputId": "888d5c1b-6512-44cd-d569-fb0af00e3862"
      },
      "source": [
        "net = Words_Net()\r\n",
        "#net.load_state_dict(torch.load(PATH+\"/net/feed_forward_10chars_MSE_2_3\"))\r\n",
        "\r\n",
        "want_cuda = True\r\n",
        "have_cuda = torch.cuda.is_available()\r\n",
        "if want_cuda and have_cuda:\r\n",
        "    net.cuda()\r\n",
        "    print(torch.cuda.get_device_name())\r\n",
        "else:\r\n",
        "    print (\"No cuda available!\\n\")\r\n",
        "summary(net, (312,))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla T4\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 400]         125,200\n",
            "           Dropout-2                  [-1, 400]               0\n",
            "       BatchNorm1d-3                  [-1, 400]             800\n",
            "              ReLU-4                  [-1, 400]               0\n",
            "            Linear-5                  [-1, 256]         102,656\n",
            "           Dropout-6                  [-1, 256]               0\n",
            "       BatchNorm1d-7                  [-1, 256]             512\n",
            "              ReLU-8                  [-1, 256]               0\n",
            "            Linear-9                   [-1, 64]          16,448\n",
            "             ReLU-10                   [-1, 64]               0\n",
            "           Linear-11                   [-1, 32]           2,080\n",
            "             ReLU-12                   [-1, 32]               0\n",
            "           Linear-13                    [-1, 1]              33\n",
            "================================================================\n",
            "Total params: 247,729\n",
            "Trainable params: 247,729\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.02\n",
            "Params size (MB): 0.95\n",
            "Estimated Total Size (MB): 0.97\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl261nA_3m2A"
      },
      "source": [
        "We define the training parameters, in this case we will use the CrossEntropy loss and the SGD algorithm to train the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lBZcMoL22iL"
      },
      "source": [
        "START = 0\r\n",
        "EPOCH = 600\r\n",
        "learn = 0.1\r\n",
        "\r\n",
        "criterion = nn.MSELoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=learn, weight_decay=4e-3)\r\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [100,200, 400], gamma=0.1)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMdHSvyg4i77"
      },
      "source": [
        "The training function will return the loss of that epoch, mediated over the iteration on the dataset. It may also be added the option to acquire interesting data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWkh8AbojODh"
      },
      "source": [
        "def accuracy():\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for i, data in enumerate(testloader, 0):\r\n",
        "            inputs, labels = data\r\n",
        "            if want_cuda and torch.cuda.is_available():\r\n",
        "                inputs = inputs.cuda()\r\n",
        "                labels = labels.cuda()\r\n",
        "            # need to squeeze if MSELoss\r\n",
        "            outputs = net(inputs).squeeze()\r\n",
        "            #loss = criterion(outputs, labels)\r\n",
        "            #loss += loss.item()\r\n",
        "            #this for MSELoss\r\n",
        "            predicted = torch.round(outputs)\r\n",
        "            #this for crossentropy\r\n",
        "            #_, predicted = torch.max(outputs,1)\r\n",
        "            total += labels.size(0)\r\n",
        "            correct += (predicted == labels).sum().item()\r\n",
        "    \r\n",
        "    return (round(correct/total *100, 4))"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3m9gBBRUXXY"
      },
      "source": [
        "def overfit_check():\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for i, data in enumerate(trainloader, 0):\r\n",
        "            inputs, labels = data\r\n",
        "            if want_cuda and torch.cuda.is_available():\r\n",
        "                inputs = inputs.cuda()\r\n",
        "                labels = labels.cuda()\r\n",
        "            # need to squeeze if MSELoss\r\n",
        "            outputs = net(inputs).squeeze()\r\n",
        "            #loss = criterion(outputs, labels)\r\n",
        "            #loss += loss.item()\r\n",
        "            #this for MSELoss\r\n",
        "            predicted = torch.round(outputs)\r\n",
        "            #this for crossentropy\r\n",
        "            #_, predicted = torch.max(outputs,1)\r\n",
        "            total += labels.size(0)\r\n",
        "            correct += (predicted == labels).sum().item()\r\n",
        "    \r\n",
        "    return (round(correct/total *100, 4))   "
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56PfvBJT3waO"
      },
      "source": [
        "def training(acquire = False , PATH = None):\r\n",
        "    running_loss = 0.0\r\n",
        "    losst = 0\r\n",
        "    index = 0\r\n",
        "    for i, data in enumerate(trainloader, 0):\r\n",
        "        # get the inputs, maybe they need to be tensors?\r\n",
        "        inputs, labels = data\r\n",
        "\r\n",
        "        if want_cuda and have_cuda:\r\n",
        "          inputs = inputs.cuda()\r\n",
        "          labels = labels.cuda()\r\n",
        "\r\n",
        "        # need to squeeze if MSELoss\r\n",
        "        outputs = net(inputs).squeeze()\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        losst +=loss.item()\r\n",
        "        index +=1\r\n",
        "    return losst/index"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwY0Q2qF4_8d"
      },
      "source": [
        "Now we can do the actual training of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmPIGtBE46Wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2db33ae-6fad-4675-ba6a-af5d4d5891fb"
      },
      "source": [
        "start =time.time()\r\n",
        "graph_data = np.empty((0,4))\r\n",
        "old_data = False\r\n",
        "mean_loss = 0\r\n",
        "old_acc = 0\r\n",
        "acc_check = 0\r\n",
        "best_epoch = 0\r\n",
        "\r\n",
        "for epoch in range(START, EPOCH):\r\n",
        "    loss = training()    \r\n",
        "    print(\"Epoch: \", epoch, \" Loss: %.10f\"%(loss))\r\n",
        "    mean_loss += loss\r\n",
        "\r\n",
        "    if (epoch % 100 == 99):\r\n",
        "        print(\"Mean loss: %.10f\"%(mean_loss/100))\r\n",
        "        mean_loss = 0\r\n",
        "        print('Estimated time: %.3f min' %((EPOCH- epoch)*(time.time() - start)/(60*epoch)) )\r\n",
        "\r\n",
        "\r\n",
        "    if (epoch%5 == 0):\r\n",
        "        net.eval()\r\n",
        "        over_check = overfit_check()\r\n",
        "        acc = accuracy()\r\n",
        "        acc_check += 1\r\n",
        "        print(\"\\n------>  Accuracy on TestSet: %.4f  <------\"%(acc))\r\n",
        "        print(\"------>  Accuracy on TrainSet: %.4f  <------\\n\"%(over_check))\r\n",
        "\r\n",
        "        graph_data = np.append(graph_data, [[epoch, loss, acc, over_check]], axis=0)\r\n",
        "\r\n",
        "        if old_data:\r\n",
        "            f = open(PATH+\"graphs/feed_forward_12chars_MSE_1.csv\", 'a')\r\n",
        "            np.savetxt(f, graph_data)\r\n",
        "            f.close()\r\n",
        "            graph_data = np.empty((0,4))\r\n",
        "        else:\r\n",
        "            np.savetxt(PATH+\"graphs/feed_forward_12chars_MSE_1.csv\", graph_data)\r\n",
        "            graph_data = np.empty((0,4))\r\n",
        "            old_data = True\r\n",
        "        \r\n",
        "        if (acc > old_acc):\r\n",
        "            old_acc = acc\r\n",
        "            torch.save(net.state_dict(), PATH+\"net/feed_forward_12chars_MSE_1\")\r\n",
        "            acc_check = 0\r\n",
        "            best_epoch = epoch\r\n",
        "\r\n",
        "    '''\r\n",
        "    if (acc_check > 200):\r\n",
        "        print(\"NET STOPPED LEARNING!!\")\r\n",
        "        print(\"\\nEpoch:%.d \\nLoss:%.10f \\nBest Accuracy:%.4f\"%(epoch, loss, old_acc))\r\n",
        "        break\r\n",
        "    '''\r\n",
        "\r\n",
        "    scheduler.step()\r\n",
        "    net.train()\r\n",
        "        \r\n",
        "elapsed_time = time.time() - start\r\n",
        "print('Finished Training (elapsed time %.3f min)' %(elapsed_time/60))\r\n",
        "net.load_state_dict(torch.load(PATH+\"net/feed_forward_12chars_MSE_1\"))\r\n",
        "print(\"\\nBest net loaded!!\")\r\n",
        "print(\"\\nBest Test Accuracy:%.5f \\nEpoch of best accuracy:%.d \\nFinal Accuracy on Train:%.5f\"%(old_acc, best_epoch, over_check))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0  Loss: 0.2484271061\n",
            "\n",
            "------>  Accuracy on TestSet: 53.4965  <------\n",
            "------>  Accuracy on TrainSet: 54.3533  <------\n",
            "\n",
            "Epoch:  1  Loss: 0.2429579442\n",
            "Epoch:  2  Loss: 0.2408162436\n",
            "Epoch:  3  Loss: 0.2404575586\n",
            "Epoch:  4  Loss: 0.2400766886\n",
            "Epoch:  5  Loss: 0.2398237517\n",
            "\n",
            "------>  Accuracy on TestSet: 59.3728  <------\n",
            "------>  Accuracy on TrainSet: 61.0323  <------\n",
            "\n",
            "Epoch:  6  Loss: 0.2395697270\n",
            "Epoch:  7  Loss: 0.2395236165\n",
            "Epoch:  8  Loss: 0.2396345064\n",
            "Epoch:  9  Loss: 0.2395070399\n",
            "Epoch:  10  Loss: 0.2397915876\n",
            "\n",
            "------>  Accuracy on TestSet: 59.6404  <------\n",
            "------>  Accuracy on TrainSet: 61.2975  <------\n",
            "\n",
            "Epoch:  11  Loss: 0.2397199101\n",
            "Epoch:  12  Loss: 0.2396012527\n",
            "Epoch:  13  Loss: 0.2394333616\n",
            "Epoch:  14  Loss: 0.2395844782\n",
            "Epoch:  15  Loss: 0.2394443075\n",
            "\n",
            "------>  Accuracy on TestSet: 59.5191  <------\n",
            "------>  Accuracy on TrainSet: 60.8836  <------\n",
            "\n",
            "Epoch:  16  Loss: 0.2395601587\n",
            "Epoch:  17  Loss: 0.2391925038\n",
            "Epoch:  18  Loss: 0.2395507946\n",
            "Epoch:  19  Loss: 0.2396191405\n",
            "Epoch:  20  Loss: 0.2394853996\n",
            "\n",
            "------>  Accuracy on TestSet: 59.4406  <------\n",
            "------>  Accuracy on TrainSet: 61.2630  <------\n",
            "\n",
            "Epoch:  21  Loss: 0.2394850260\n",
            "Epoch:  22  Loss: 0.2393340922\n",
            "Epoch:  23  Loss: 0.2394778425\n",
            "Epoch:  24  Loss: 0.2393980975\n",
            "Epoch:  25  Loss: 0.2396013159\n",
            "\n",
            "------>  Accuracy on TestSet: 59.7296  <------\n",
            "------>  Accuracy on TrainSet: 61.3225  <------\n",
            "\n",
            "Epoch:  26  Loss: 0.2394504250\n",
            "Epoch:  27  Loss: 0.2395150422\n",
            "Epoch:  28  Loss: 0.2394710292\n",
            "Epoch:  29  Loss: 0.2394651689\n",
            "Epoch:  30  Loss: 0.2392904158\n",
            "\n",
            "------>  Accuracy on TestSet: 59.5868  <------\n",
            "------>  Accuracy on TrainSet: 61.4545  <------\n",
            "\n",
            "Epoch:  31  Loss: 0.2393333870\n",
            "Epoch:  32  Loss: 0.2392388615\n",
            "Epoch:  33  Loss: 0.2395617405\n",
            "Epoch:  34  Loss: 0.2393938569\n",
            "Epoch:  35  Loss: 0.2395407880\n",
            "\n",
            "------>  Accuracy on TestSet: 59.5405  <------\n",
            "------>  Accuracy on TrainSet: 61.1964  <------\n",
            "\n",
            "Epoch:  36  Loss: 0.2395350301\n",
            "Epoch:  37  Loss: 0.2393517763\n",
            "Epoch:  38  Loss: 0.2394815459\n",
            "Epoch:  39  Loss: 0.2393738870\n",
            "Epoch:  40  Loss: 0.2395429345\n",
            "\n",
            "------>  Accuracy on TestSet: 59.3407  <------\n",
            "------>  Accuracy on TrainSet: 60.9752  <------\n",
            "\n",
            "Epoch:  41  Loss: 0.2394841507\n",
            "Epoch:  42  Loss: 0.2390823114\n",
            "Epoch:  43  Loss: 0.2393391053\n",
            "Epoch:  44  Loss: 0.2393796247\n",
            "Epoch:  45  Loss: 0.2392400300\n",
            "\n",
            "------>  Accuracy on TestSet: 58.9696  <------\n",
            "------>  Accuracy on TrainSet: 60.7124  <------\n",
            "\n",
            "Epoch:  46  Loss: 0.2391988109\n",
            "Epoch:  47  Loss: 0.2392200154\n",
            "Epoch:  48  Loss: 0.2395218056\n",
            "Epoch:  49  Loss: 0.2391811121\n",
            "Epoch:  50  Loss: 0.2393497156\n",
            "\n",
            "------>  Accuracy on TestSet: 59.1230  <------\n",
            "------>  Accuracy on TrainSet: 60.8218  <------\n",
            "\n",
            "Epoch:  51  Loss: 0.2394418476\n",
            "Epoch:  52  Loss: 0.2392118302\n",
            "Epoch:  53  Loss: 0.2391944367\n",
            "Epoch:  54  Loss: 0.2396184719\n",
            "Epoch:  55  Loss: 0.2394776764\n",
            "\n",
            "------>  Accuracy on TestSet: 59.3549  <------\n",
            "------>  Accuracy on TrainSet: 60.9241  <------\n",
            "\n",
            "Epoch:  56  Loss: 0.2395396907\n",
            "Epoch:  57  Loss: 0.2392610829\n",
            "Epoch:  58  Loss: 0.2392834928\n",
            "Epoch:  59  Loss: 0.2394077827\n",
            "Epoch:  60  Loss: 0.2394805125\n",
            "\n",
            "------>  Accuracy on TestSet: 59.4084  <------\n",
            "------>  Accuracy on TrainSet: 60.7338  <------\n",
            "\n",
            "Epoch:  61  Loss: 0.2397084458\n",
            "Epoch:  62  Loss: 0.2394841840\n",
            "Epoch:  63  Loss: 0.2392700571\n",
            "Epoch:  64  Loss: 0.2392874143\n",
            "Epoch:  65  Loss: 0.2395927470\n",
            "\n",
            "------>  Accuracy on TestSet: 59.3656  <------\n",
            "------>  Accuracy on TrainSet: 61.2761  <------\n",
            "\n",
            "Epoch:  66  Loss: 0.2392819642\n",
            "Epoch:  67  Loss: 0.2394093813\n",
            "Epoch:  68  Loss: 0.2393308511\n",
            "Epoch:  69  Loss: 0.2395521878\n",
            "Epoch:  70  Loss: 0.2394127610\n",
            "\n",
            "------>  Accuracy on TestSet: 59.2265  <------\n",
            "------>  Accuracy on TrainSet: 60.9871  <------\n",
            "\n",
            "Epoch:  71  Loss: 0.2395179599\n",
            "Epoch:  72  Loss: 0.2394480564\n",
            "Epoch:  73  Loss: 0.2396610419\n",
            "Epoch:  74  Loss: 0.2394300412\n",
            "Epoch:  75  Loss: 0.2393531057\n",
            "\n",
            "------>  Accuracy on TestSet: 59.5975  <------\n",
            "------>  Accuracy on TrainSet: 61.4176  <------\n",
            "\n",
            "Epoch:  76  Loss: 0.2393180628\n",
            "Epoch:  77  Loss: 0.2394736542\n",
            "Epoch:  78  Loss: 0.2393867343\n",
            "Epoch:  79  Loss: 0.2394795441\n",
            "Epoch:  80  Loss: 0.2395980480\n",
            "\n",
            "------>  Accuracy on TestSet: 59.5512  <------\n",
            "------>  Accuracy on TrainSet: 61.0418  <------\n",
            "\n",
            "Epoch:  81  Loss: 0.2392159991\n",
            "Epoch:  82  Loss: 0.2395514167\n",
            "Epoch:  83  Loss: 0.2396218648\n",
            "Epoch:  84  Loss: 0.2395228382\n",
            "Epoch:  85  Loss: 0.2393310773\n",
            "\n",
            "------>  Accuracy on TestSet: 59.1730  <------\n",
            "------>  Accuracy on TrainSet: 60.6553  <------\n",
            "\n",
            "Epoch:  86  Loss: 0.2394814218\n",
            "Epoch:  87  Loss: 0.2394975918\n",
            "Epoch:  88  Loss: 0.2395837434\n",
            "Epoch:  89  Loss: 0.2395783147\n",
            "Epoch:  90  Loss: 0.2395161670\n",
            "\n",
            "------>  Accuracy on TestSet: 59.1658  <------\n",
            "------>  Accuracy on TrainSet: 60.9395  <------\n",
            "\n",
            "Epoch:  91  Loss: 0.2395437942\n",
            "Epoch:  92  Loss: 0.2394783114\n",
            "Epoch:  93  Loss: 0.2391669827\n",
            "Epoch:  94  Loss: 0.2395133505\n",
            "Epoch:  95  Loss: 0.2392836018\n",
            "\n",
            "------>  Accuracy on TestSet: 59.2051  <------\n",
            "------>  Accuracy on TrainSet: 60.7706  <------\n",
            "\n",
            "Epoch:  96  Loss: 0.2393812976\n",
            "Epoch:  97  Loss: 0.2394745031\n",
            "Epoch:  98  Loss: 0.2394570937\n",
            "Epoch:  99  Loss: 0.2391844514\n",
            "Mean loss: 0.2395991291\n",
            "Estimated time: 64.846 min\n",
            "Epoch:  100  Loss: 0.2256572212\n",
            "\n",
            "------>  Accuracy on TestSet: 63.4223  <------\n",
            "------>  Accuracy on TrainSet: 66.4066  <------\n",
            "\n",
            "Epoch:  101  Loss: 0.2177266558\n",
            "Epoch:  102  Loss: 0.2130119205\n",
            "Epoch:  103  Loss: 0.2105941241\n",
            "Epoch:  104  Loss: 0.2082529435\n",
            "Epoch:  105  Loss: 0.2058201833\n",
            "\n",
            "------>  Accuracy on TestSet: 66.8082  <------\n",
            "------>  Accuracy on TrainSet: 71.8059  <------\n",
            "\n",
            "Epoch:  106  Loss: 0.2038900510\n",
            "Epoch:  107  Loss: 0.2012536076\n",
            "Epoch:  108  Loss: 0.1993340126\n",
            "Epoch:  109  Loss: 0.1985113027\n",
            "Epoch:  110  Loss: 0.1970308900\n",
            "\n",
            "------>  Accuracy on TestSet: 67.6752  <------\n",
            "------>  Accuracy on TrainSet: 74.0168  <------\n",
            "\n",
            "Epoch:  111  Loss: 0.1957328370\n",
            "Epoch:  112  Loss: 0.1944532791\n",
            "Epoch:  113  Loss: 0.1942983727\n",
            "Epoch:  114  Loss: 0.1927845969\n",
            "Epoch:  115  Loss: 0.1920503185\n",
            "\n",
            "------>  Accuracy on TestSet: 65.4881  <------\n",
            "------>  Accuracy on TrainSet: 70.7403  <------\n",
            "\n",
            "Epoch:  116  Loss: 0.1916050991\n",
            "Epoch:  117  Loss: 0.1906066177\n",
            "Epoch:  118  Loss: 0.1902410230\n",
            "Epoch:  119  Loss: 0.1888057236\n",
            "Epoch:  120  Loss: 0.1888050453\n",
            "\n",
            "------>  Accuracy on TestSet: 67.7715  <------\n",
            "------>  Accuracy on TrainSet: 74.2915  <------\n",
            "\n",
            "Epoch:  121  Loss: 0.1878013726\n",
            "Epoch:  122  Loss: 0.1884863721\n",
            "Epoch:  123  Loss: 0.1870395815\n",
            "Epoch:  124  Loss: 0.1868324930\n",
            "Epoch:  125  Loss: 0.1866652364\n",
            "\n",
            "------>  Accuracy on TestSet: 68.1640  <------\n",
            "------>  Accuracy on TrainSet: 75.4962  <------\n",
            "\n",
            "Epoch:  126  Loss: 0.1863067887\n",
            "Epoch:  127  Loss: 0.1859892082\n",
            "Epoch:  128  Loss: 0.1851022821\n",
            "Epoch:  129  Loss: 0.1846029463\n",
            "Epoch:  130  Loss: 0.1855286002\n",
            "\n",
            "------>  Accuracy on TestSet: 69.7053  <------\n",
            "------>  Accuracy on TrainSet: 77.6762  <------\n",
            "\n",
            "Epoch:  131  Loss: 0.1841296014\n",
            "Epoch:  132  Loss: 0.1834726162\n",
            "Epoch:  133  Loss: 0.1833289708\n",
            "Epoch:  134  Loss: 0.1834760857\n",
            "Epoch:  135  Loss: 0.1833288670\n",
            "\n",
            "------>  Accuracy on TestSet: 69.5162  <------\n",
            "------>  Accuracy on TrainSet: 77.2338  <------\n",
            "\n",
            "Epoch:  136  Loss: 0.1834970911\n",
            "Epoch:  137  Loss: 0.1822806426\n",
            "Epoch:  138  Loss: 0.1823021624\n",
            "Epoch:  139  Loss: 0.1827063371\n",
            "Epoch:  140  Loss: 0.1825737697\n",
            "\n",
            "------>  Accuracy on TestSet: 68.1069  <------\n",
            "------>  Accuracy on TrainSet: 75.1323  <------\n",
            "\n",
            "Epoch:  141  Loss: 0.1820581703\n",
            "Epoch:  142  Loss: 0.1815506644\n",
            "Epoch:  143  Loss: 0.1818514449\n",
            "Epoch:  144  Loss: 0.1810901028\n",
            "Epoch:  145  Loss: 0.1813268615\n",
            "\n",
            "------>  Accuracy on TestSet: 68.4423  <------\n",
            "------>  Accuracy on TrainSet: 75.9363  <------\n",
            "\n",
            "Epoch:  146  Loss: 0.1806540631\n",
            "Epoch:  147  Loss: 0.1812521167\n",
            "Epoch:  148  Loss: 0.1806303187\n",
            "Epoch:  149  Loss: 0.1796964376\n",
            "Epoch:  150  Loss: 0.1804624617\n",
            "\n",
            "------>  Accuracy on TestSet: 68.8526  <------\n",
            "------>  Accuracy on TrainSet: 76.8484  <------\n",
            "\n",
            "Epoch:  151  Loss: 0.1806877873\n",
            "Epoch:  152  Loss: 0.1797311244\n",
            "Epoch:  153  Loss: 0.1800011560\n",
            "Epoch:  154  Loss: 0.1801114861\n",
            "Epoch:  155  Loss: 0.1795779445\n",
            "\n",
            "------>  Accuracy on TestSet: 70.0407  <------\n",
            "------>  Accuracy on TrainSet: 78.3873  <------\n",
            "\n",
            "Epoch:  156  Loss: 0.1792421171\n",
            "Epoch:  157  Loss: 0.1797694407\n",
            "Epoch:  158  Loss: 0.1793677602\n",
            "Epoch:  159  Loss: 0.1798271176\n",
            "Epoch:  160  Loss: 0.1790851386\n",
            "\n",
            "------>  Accuracy on TestSet: 69.6696  <------\n",
            "------>  Accuracy on TrainSet: 78.0995  <------\n",
            "\n",
            "Epoch:  161  Loss: 0.1790674621\n",
            "Epoch:  162  Loss: 0.1791253139\n",
            "Epoch:  163  Loss: 0.1802221199\n",
            "Epoch:  164  Loss: 0.1785833568\n",
            "Epoch:  165  Loss: 0.1792505706\n",
            "\n",
            "------>  Accuracy on TestSet: 69.8373  <------\n",
            "------>  Accuracy on TrainSet: 78.4896  <------\n",
            "\n",
            "Epoch:  166  Loss: 0.1794846899\n",
            "Epoch:  167  Loss: 0.1785737488\n",
            "Epoch:  168  Loss: 0.1790620260\n",
            "Epoch:  169  Loss: 0.1783920441\n",
            "Epoch:  170  Loss: 0.1785612870\n",
            "\n",
            "------>  Accuracy on TestSet: 70.5081  <------\n",
            "------>  Accuracy on TrainSet: 79.3875  <------\n",
            "\n",
            "Epoch:  171  Loss: 0.1785506971\n",
            "Epoch:  172  Loss: 0.1787116272\n",
            "Epoch:  173  Loss: 0.1780765201\n",
            "Epoch:  174  Loss: 0.1776986580\n",
            "Epoch:  175  Loss: 0.1785389658\n",
            "\n",
            "------>  Accuracy on TestSet: 71.1396  <------\n",
            "------>  Accuracy on TrainSet: 80.5447  <------\n",
            "\n",
            "Epoch:  176  Loss: 0.1782022734\n",
            "Epoch:  177  Loss: 0.1779394885\n",
            "Epoch:  178  Loss: 0.1775056690\n",
            "Epoch:  179  Loss: 0.1781211421\n",
            "Epoch:  180  Loss: 0.1781246965\n",
            "\n",
            "------>  Accuracy on TestSet: 68.9882  <------\n",
            "------>  Accuracy on TrainSet: 77.3824  <------\n",
            "\n",
            "Epoch:  181  Loss: 0.1783329246\n",
            "Epoch:  182  Loss: 0.1782221943\n",
            "Epoch:  183  Loss: 0.1770797214\n",
            "Epoch:  184  Loss: 0.1777899570\n",
            "Epoch:  185  Loss: 0.1775055934\n",
            "\n",
            "------>  Accuracy on TestSet: 68.6635  <------\n",
            "------>  Accuracy on TrainSet: 76.9210  <------\n",
            "\n",
            "Epoch:  186  Loss: 0.1779810535\n",
            "Epoch:  187  Loss: 0.1776927545\n",
            "Epoch:  188  Loss: 0.1770102293\n",
            "Epoch:  189  Loss: 0.1774115161\n",
            "Epoch:  190  Loss: 0.1777494959\n",
            "\n",
            "------>  Accuracy on TestSet: 66.3979  <------\n",
            "------>  Accuracy on TrainSet: 73.1795  <------\n",
            "\n",
            "Epoch:  191  Loss: 0.1771966493\n",
            "Epoch:  192  Loss: 0.1776699686\n",
            "Epoch:  193  Loss: 0.1777025961\n",
            "Epoch:  194  Loss: 0.1774201281\n",
            "Epoch:  195  Loss: 0.1777442689\n",
            "\n",
            "------>  Accuracy on TestSet: 67.4932  <------\n",
            "------>  Accuracy on TrainSet: 75.4843  <------\n",
            "\n",
            "Epoch:  196  Loss: 0.1764802423\n",
            "Epoch:  197  Loss: 0.1760671385\n",
            "Epoch:  198  Loss: 0.1774209795\n",
            "Epoch:  199  Loss: 0.1771546863\n",
            "Mean loss: 0.1844731502\n",
            "Estimated time: 51.702 min\n",
            "Epoch:  200  Loss: 0.1576823815\n",
            "\n",
            "------>  Accuracy on TestSet: 71.0076  <------\n",
            "------>  Accuracy on TrainSet: 80.9942  <------\n",
            "\n",
            "Epoch:  201  Loss: 0.1496395117\n",
            "Epoch:  202  Loss: 0.1455345315\n",
            "Epoch:  203  Loss: 0.1430974737\n",
            "Epoch:  204  Loss: 0.1408695430\n",
            "Epoch:  205  Loss: 0.1395584877\n",
            "\n",
            "------>  Accuracy on TestSet: 72.7522  <------\n",
            "------>  Accuracy on TrainSet: 84.2683  <------\n",
            "\n",
            "Epoch:  206  Loss: 0.1378909474\n",
            "Epoch:  207  Loss: 0.1356714459\n",
            "Epoch:  208  Loss: 0.1349158934\n",
            "Epoch:  209  Loss: 0.1334237340\n",
            "Epoch:  210  Loss: 0.1331398742\n",
            "\n",
            "------>  Accuracy on TestSet: 72.9984  <------\n",
            "------>  Accuracy on TrainSet: 84.8177  <------\n",
            "\n",
            "Epoch:  211  Loss: 0.1322235533\n",
            "Epoch:  212  Loss: 0.1308229432\n",
            "Epoch:  213  Loss: 0.1302904090\n",
            "Epoch:  214  Loss: 0.1300555076\n",
            "Epoch:  215  Loss: 0.1292789613\n",
            "\n",
            "------>  Accuracy on TestSet: 71.1931  <------\n",
            "------>  Accuracy on TrainSet: 82.7520  <------\n",
            "\n",
            "Epoch:  216  Loss: 0.1294199039\n",
            "Epoch:  217  Loss: 0.1290498731\n",
            "Epoch:  218  Loss: 0.1279938115\n",
            "Epoch:  219  Loss: 0.1278037113\n",
            "Epoch:  220  Loss: 0.1280095143\n",
            "\n",
            "------>  Accuracy on TestSet: 73.3088  <------\n",
            "------>  Accuracy on TrainSet: 86.0177  <------\n",
            "\n",
            "Epoch:  221  Loss: 0.1270652038\n",
            "Epoch:  222  Loss: 0.1260176706\n",
            "Epoch:  223  Loss: 0.1263889479\n",
            "Epoch:  224  Loss: 0.1265125286\n",
            "Epoch:  225  Loss: 0.1260665368\n",
            "\n",
            "------>  Accuracy on TestSet: 72.6666  <------\n",
            "------>  Accuracy on TrainSet: 84.9902  <------\n",
            "\n",
            "Epoch:  226  Loss: 0.1249464541\n",
            "Epoch:  227  Loss: 0.1258132115\n",
            "Epoch:  228  Loss: 0.1249474082\n",
            "Epoch:  229  Loss: 0.1245204938\n",
            "Epoch:  230  Loss: 0.1249259266\n",
            "\n",
            "------>  Accuracy on TestSet: 73.4159  <------\n",
            "------>  Accuracy on TrainSet: 86.7943  <------\n",
            "\n",
            "Epoch:  231  Loss: 0.1245140698\n",
            "Epoch:  232  Loss: 0.1240675567\n",
            "Epoch:  233  Loss: 0.1242470280\n",
            "Epoch:  234  Loss: 0.1246995073\n",
            "Epoch:  235  Loss: 0.1249474886\n",
            "\n",
            "------>  Accuracy on TestSet: 71.9566  <------\n",
            "------>  Accuracy on TrainSet: 84.6049  <------\n",
            "\n",
            "Epoch:  236  Loss: 0.1252502311\n",
            "Epoch:  237  Loss: 0.1242714169\n",
            "Epoch:  238  Loss: 0.1240010535\n",
            "Epoch:  239  Loss: 0.1239588200\n",
            "Epoch:  240  Loss: 0.1237729283\n",
            "\n",
            "------>  Accuracy on TestSet: 73.3195  <------\n",
            "------>  Accuracy on TrainSet: 87.0024  <------\n",
            "\n",
            "Epoch:  241  Loss: 0.1232800395\n",
            "Epoch:  242  Loss: 0.1240183274\n",
            "Epoch:  243  Loss: 0.1231972100\n",
            "Epoch:  244  Loss: 0.1231359524\n",
            "Epoch:  245  Loss: 0.1231684332\n",
            "\n",
            "------>  Accuracy on TestSet: 74.4327  <------\n",
            "------>  Accuracy on TrainSet: 88.7840  <------\n",
            "\n",
            "Epoch:  246  Loss: 0.1243391401\n",
            "Epoch:  247  Loss: 0.1227826133\n",
            "Epoch:  248  Loss: 0.1236974754\n",
            "Epoch:  249  Loss: 0.1227763122\n",
            "Epoch:  250  Loss: 0.1239957117\n",
            "\n",
            "------>  Accuracy on TestSet: 73.4408  <------\n",
            "------>  Accuracy on TrainSet: 87.0809  <------\n",
            "\n",
            "Epoch:  251  Loss: 0.1227010391\n",
            "Epoch:  252  Loss: 0.1224892457\n",
            "Epoch:  253  Loss: 0.1237909446\n",
            "Epoch:  254  Loss: 0.1231274294\n",
            "Epoch:  255  Loss: 0.1224799681\n",
            "\n",
            "------>  Accuracy on TestSet: 73.3802  <------\n",
            "------>  Accuracy on TrainSet: 87.2558  <------\n",
            "\n",
            "Epoch:  256  Loss: 0.1226581946\n",
            "Epoch:  257  Loss: 0.1221916848\n",
            "Epoch:  258  Loss: 0.1224994907\n",
            "Epoch:  259  Loss: 0.1223564633\n",
            "Epoch:  260  Loss: 0.1228362110\n",
            "\n",
            "------>  Accuracy on TestSet: 73.8511  <------\n",
            "------>  Accuracy on TrainSet: 88.8173  <------\n",
            "\n",
            "Epoch:  261  Loss: 0.1223489668\n",
            "Epoch:  262  Loss: 0.1229390375\n",
            "Epoch:  263  Loss: 0.1216405984\n",
            "Epoch:  264  Loss: 0.1226787841\n",
            "Epoch:  265  Loss: 0.1221782951\n",
            "\n",
            "------>  Accuracy on TestSet: 73.2553  <------\n",
            "------>  Accuracy on TrainSet: 86.7646  <------\n",
            "\n",
            "Epoch:  266  Loss: 0.1231201965\n",
            "Epoch:  267  Loss: 0.1219134085\n",
            "Epoch:  268  Loss: 0.1221847915\n",
            "Epoch:  269  Loss: 0.1227608996\n",
            "Epoch:  270  Loss: 0.1209525820\n",
            "\n",
            "------>  Accuracy on TestSet: 74.5183  <------\n",
            "------>  Accuracy on TrainSet: 89.4785  <------\n",
            "\n",
            "Epoch:  271  Loss: 0.1223451171\n",
            "Epoch:  272  Loss: 0.1217749388\n",
            "Epoch:  273  Loss: 0.1220909082\n",
            "Epoch:  274  Loss: 0.1218380282\n",
            "Epoch:  275  Loss: 0.1214351391\n",
            "\n",
            "------>  Accuracy on TestSet: 74.0474  <------\n",
            "------>  Accuracy on TrainSet: 88.9897  <------\n",
            "\n",
            "Epoch:  276  Loss: 0.1217124876\n",
            "Epoch:  277  Loss: 0.1216747410\n",
            "Epoch:  278  Loss: 0.1214106282\n",
            "Epoch:  279  Loss: 0.1209893253\n",
            "Epoch:  280  Loss: 0.1212569959\n",
            "\n",
            "------>  Accuracy on TestSet: 73.9653  <------\n",
            "------>  Accuracy on TrainSet: 88.7863  <------\n",
            "\n",
            "Epoch:  281  Loss: 0.1204673436\n",
            "Epoch:  282  Loss: 0.1216403868\n",
            "Epoch:  283  Loss: 0.1211682593\n",
            "Epoch:  284  Loss: 0.1201821263\n",
            "Epoch:  285  Loss: 0.1217673831\n",
            "\n",
            "------>  Accuracy on TestSet: 74.2722  <------\n",
            "------>  Accuracy on TrainSet: 89.8924  <------\n",
            "\n",
            "Epoch:  286  Loss: 0.1212951664\n",
            "Epoch:  287  Loss: 0.1195013236\n",
            "Epoch:  288  Loss: 0.1193411339\n",
            "Epoch:  289  Loss: 0.1209206892\n",
            "Epoch:  290  Loss: 0.1213921253\n",
            "\n",
            "------>  Accuracy on TestSet: 73.9725  <------\n",
            "------>  Accuracy on TrainSet: 89.0337  <------\n",
            "\n",
            "Epoch:  291  Loss: 0.1207961627\n",
            "Epoch:  292  Loss: 0.1212214577\n",
            "Epoch:  293  Loss: 0.1204713408\n",
            "Epoch:  294  Loss: 0.1200390154\n",
            "Epoch:  295  Loss: 0.1200232029\n",
            "\n",
            "------>  Accuracy on TestSet: 74.0474  <------\n",
            "------>  Accuracy on TrainSet: 89.1336  <------\n",
            "\n",
            "Epoch:  296  Loss: 0.1195985399\n",
            "Epoch:  297  Loss: 0.1202225602\n",
            "Epoch:  298  Loss: 0.1195442071\n",
            "Epoch:  299  Loss: 0.1202413399\n",
            "Mean loss: 0.1253790801\n",
            "Estimated time: 38.742 min\n",
            "Epoch:  300  Loss: 0.1198266885\n",
            "\n",
            "------>  Accuracy on TestSet: 74.5540  <------\n",
            "------>  Accuracy on TrainSet: 89.8733  <------\n",
            "\n",
            "Epoch:  301  Loss: 0.1191197569\n",
            "Epoch:  302  Loss: 0.1195239501\n",
            "Epoch:  303  Loss: 0.1192919069\n",
            "Epoch:  304  Loss: 0.1200685049\n",
            "Epoch:  305  Loss: 0.1190737486\n",
            "\n",
            "------>  Accuracy on TestSet: 74.3185  <------\n",
            "------>  Accuracy on TrainSet: 90.3062  <------\n",
            "\n",
            "Epoch:  306  Loss: 0.1197363042\n",
            "Epoch:  307  Loss: 0.1194652804\n",
            "Epoch:  308  Loss: 0.1197484451\n",
            "Epoch:  309  Loss: 0.1197044629\n",
            "Epoch:  310  Loss: 0.1200747168\n",
            "\n",
            "------>  Accuracy on TestSet: 74.4898  <------\n",
            "------>  Accuracy on TrainSet: 90.3062  <------\n",
            "\n",
            "Epoch:  311  Loss: 0.1194577031\n",
            "Epoch:  312  Loss: 0.1184076517\n",
            "Epoch:  313  Loss: 0.1186302837\n",
            "Epoch:  314  Loss: 0.1189024530\n",
            "Epoch:  315  Loss: 0.1190452959\n",
            "\n",
            "------>  Accuracy on TestSet: 74.5968  <------\n",
            "------>  Accuracy on TrainSet: 90.2242  <------\n",
            "\n",
            "Epoch:  316  Loss: 0.1195049424\n",
            "Epoch:  317  Loss: 0.1188170867\n",
            "Epoch:  318  Loss: 0.1190243873\n",
            "Epoch:  319  Loss: 0.1184285130\n",
            "Epoch:  320  Loss: 0.1179371200\n",
            "\n",
            "------>  Accuracy on TestSet: 74.6147  <------\n",
            "------>  Accuracy on TrainSet: 90.2337  <------\n",
            "\n",
            "Epoch:  321  Loss: 0.1186891637\n",
            "Epoch:  322  Loss: 0.1186017320\n",
            "Epoch:  323  Loss: 0.1196988931\n",
            "Epoch:  324  Loss: 0.1192954165\n",
            "Epoch:  325  Loss: 0.1186144111\n",
            "\n",
            "------>  Accuracy on TestSet: 74.1758  <------\n",
            "------>  Accuracy on TrainSet: 88.9528  <------\n",
            "\n",
            "Epoch:  326  Loss: 0.1187569837\n",
            "Epoch:  327  Loss: 0.1184212992\n",
            "Epoch:  328  Loss: 0.1191509184\n",
            "Epoch:  329  Loss: 0.1184735840\n",
            "Epoch:  330  Loss: 0.1182108763\n",
            "\n",
            "------>  Accuracy on TestSet: 73.9582  <------\n",
            "------>  Accuracy on TrainSet: 89.5475  <------\n",
            "\n",
            "Epoch:  331  Loss: 0.1172212411\n",
            "Epoch:  332  Loss: 0.1175648174\n",
            "Epoch:  333  Loss: 0.1181466121\n",
            "Epoch:  334  Loss: 0.1177494715\n",
            "Epoch:  335  Loss: 0.1178557503\n",
            "\n",
            "------>  Accuracy on TestSet: 74.4256  <------\n",
            "------>  Accuracy on TrainSet: 90.0006  <------\n",
            "\n",
            "Epoch:  336  Loss: 0.1185385503\n",
            "Epoch:  337  Loss: 0.1187600904\n",
            "Epoch:  338  Loss: 0.1188899743\n",
            "Epoch:  339  Loss: 0.1176806389\n",
            "Epoch:  340  Loss: 0.1183395722\n",
            "\n",
            "------>  Accuracy on TestSet: 72.2991  <------\n",
            "------>  Accuracy on TrainSet: 86.2901  <------\n",
            "\n",
            "Epoch:  341  Loss: 0.1184082816\n",
            "Epoch:  342  Loss: 0.1170739459\n",
            "Epoch:  343  Loss: 0.1176026023\n",
            "Epoch:  344  Loss: 0.1169802177\n",
            "Epoch:  345  Loss: 0.1183076494\n",
            "\n",
            "------>  Accuracy on TestSet: 74.1972  <------\n",
            "------>  Accuracy on TrainSet: 90.1136  <------\n",
            "\n",
            "Epoch:  346  Loss: 0.1194392464\n",
            "Epoch:  347  Loss: 0.1175024012\n",
            "Epoch:  348  Loss: 0.1170705279\n",
            "Epoch:  349  Loss: 0.1166907007\n",
            "Epoch:  350  Loss: 0.1167489932\n",
            "\n",
            "------>  Accuracy on TestSet: 73.6763  <------\n",
            "------>  Accuracy on TrainSet: 89.1538  <------\n",
            "\n",
            "Epoch:  351  Loss: 0.1181453806\n",
            "Epoch:  352  Loss: 0.1167542340\n",
            "Epoch:  353  Loss: 0.1166362517\n",
            "Epoch:  354  Loss: 0.1175278703\n",
            "Epoch:  355  Loss: 0.1171326277\n",
            "\n",
            "------>  Accuracy on TestSet: 73.6121  <------\n",
            "------>  Accuracy on TrainSet: 89.4904  <------\n",
            "\n",
            "Epoch:  356  Loss: 0.1167766302\n",
            "Epoch:  357  Loss: 0.1169098413\n",
            "Epoch:  358  Loss: 0.1161175093\n",
            "Epoch:  359  Loss: 0.1169116144\n",
            "Epoch:  360  Loss: 0.1161581813\n",
            "\n",
            "------>  Accuracy on TestSet: 72.3669  <------\n",
            "------>  Accuracy on TrainSet: 87.2046  <------\n",
            "\n",
            "Epoch:  361  Loss: 0.1163403513\n",
            "Epoch:  362  Loss: 0.1155052322\n",
            "Epoch:  363  Loss: 0.1171261773\n",
            "Epoch:  364  Loss: 0.1167022927\n",
            "Epoch:  365  Loss: 0.1164069734\n",
            "\n",
            "------>  Accuracy on TestSet: 72.4740  <------\n",
            "------>  Accuracy on TrainSet: 87.1321  <------\n",
            "\n",
            "Epoch:  366  Loss: 0.1160362156\n",
            "Epoch:  367  Loss: 0.1171032953\n",
            "Epoch:  368  Loss: 0.1168304463\n",
            "Epoch:  369  Loss: 0.1159775204\n",
            "Epoch:  370  Loss: 0.1160475838\n",
            "\n",
            "------>  Accuracy on TestSet: 73.2946  <------\n",
            "------>  Accuracy on TrainSet: 88.8006  <------\n",
            "\n",
            "Epoch:  371  Loss: 0.1158479329\n",
            "Epoch:  372  Loss: 0.1154692225\n",
            "Epoch:  373  Loss: 0.1147430170\n",
            "Epoch:  374  Loss: 0.1163683826\n",
            "Epoch:  375  Loss: 0.1161102363\n",
            "\n",
            "------>  Accuracy on TestSet: 73.1483  <------\n",
            "------>  Accuracy on TrainSet: 88.0728  <------\n",
            "\n",
            "Epoch:  376  Loss: 0.1166610343\n",
            "Epoch:  377  Loss: 0.1163965424\n",
            "Epoch:  378  Loss: 0.1159690291\n",
            "Epoch:  379  Loss: 0.1162939609\n",
            "Epoch:  380  Loss: 0.1154388513\n",
            "\n",
            "------>  Accuracy on TestSet: 73.9011  <------\n",
            "------>  Accuracy on TrainSet: 90.2825  <------\n",
            "\n",
            "Epoch:  381  Loss: 0.1157063348\n",
            "Epoch:  382  Loss: 0.1158655497\n",
            "Epoch:  383  Loss: 0.1156404005\n",
            "Epoch:  384  Loss: 0.1162808037\n",
            "Epoch:  385  Loss: 0.1151648848\n",
            "\n",
            "------>  Accuracy on TestSet: 74.6040  <------\n",
            "------>  Accuracy on TrainSet: 91.0246  <------\n",
            "\n",
            "Epoch:  386  Loss: 0.1151091627\n",
            "Epoch:  387  Loss: 0.1168639006\n",
            "Epoch:  388  Loss: 0.1156458757\n",
            "Epoch:  389  Loss: 0.1152299228\n",
            "Epoch:  390  Loss: 0.1146474466\n",
            "\n",
            "------>  Accuracy on TestSet: 73.0020  <------\n",
            "------>  Accuracy on TrainSet: 88.2381  <------\n",
            "\n",
            "Epoch:  391  Loss: 0.1150346119\n",
            "Epoch:  392  Loss: 0.1157604002\n",
            "Epoch:  393  Loss: 0.1154209444\n",
            "Epoch:  394  Loss: 0.1153953590\n",
            "Epoch:  395  Loss: 0.1159185950\n",
            "\n",
            "------>  Accuracy on TestSet: 71.2395  <------\n",
            "------>  Accuracy on TrainSet: 85.0247  <------\n",
            "\n",
            "Epoch:  396  Loss: 0.1148583051\n",
            "Epoch:  397  Loss: 0.1152221563\n",
            "Epoch:  398  Loss: 0.1150550038\n",
            "Epoch:  399  Loss: 0.1146757521\n",
            "Mean loss: 0.1173421361\n",
            "Estimated time: 25.866 min\n",
            "Epoch:  400  Loss: 0.1057728034\n",
            "\n",
            "------>  Accuracy on TestSet: 69.9943  <------\n",
            "------>  Accuracy on TrainSet: 83.2800  <------\n",
            "\n",
            "Epoch:  401  Loss: 0.1010389392\n",
            "Epoch:  402  Loss: 0.0986770966\n",
            "Epoch:  403  Loss: 0.0981625786\n",
            "Epoch:  404  Loss: 0.0963470432\n",
            "Epoch:  405  Loss: 0.0958973238\n",
            "\n",
            "------>  Accuracy on TestSet: 71.6070  <------\n",
            "------>  Accuracy on TrainSet: 85.9487  <------\n",
            "\n",
            "Epoch:  406  Loss: 0.0951390193\n",
            "Epoch:  407  Loss: 0.0944672883\n",
            "Epoch:  408  Loss: 0.0938248493\n",
            "Epoch:  409  Loss: 0.0938588277\n",
            "Epoch:  410  Loss: 0.0937534782\n",
            "\n",
            "------>  Accuracy on TestSet: 72.5953  <------\n",
            "------>  Accuracy on TrainSet: 87.8302  <------\n",
            "\n",
            "Epoch:  411  Loss: 0.0923333684\n",
            "Epoch:  412  Loss: 0.0925089722\n",
            "Epoch:  413  Loss: 0.0917258767\n",
            "Epoch:  414  Loss: 0.0913815114\n",
            "Epoch:  415  Loss: 0.0904834364\n",
            "\n",
            "------>  Accuracy on TestSet: 71.7425  <------\n",
            "------>  Accuracy on TrainSet: 86.3888  <------\n",
            "\n",
            "Epoch:  416  Loss: 0.0918609561\n",
            "Epoch:  417  Loss: 0.0911943236\n",
            "Epoch:  418  Loss: 0.0911350378\n",
            "Epoch:  419  Loss: 0.0905347901\n",
            "Epoch:  420  Loss: 0.0898292774\n",
            "\n",
            "------>  Accuracy on TestSet: 73.5051  <------\n",
            "------>  Accuracy on TrainSet: 89.5070  <------\n",
            "\n",
            "Epoch:  421  Loss: 0.0907360872\n",
            "Epoch:  422  Loss: 0.0895047759\n",
            "Epoch:  423  Loss: 0.0893493288\n",
            "Epoch:  424  Loss: 0.0892808033\n",
            "Epoch:  425  Loss: 0.0888901629\n",
            "\n",
            "------>  Accuracy on TestSet: 74.4898  <------\n",
            "------>  Accuracy on TrainSet: 91.2945  <------\n",
            "\n",
            "Epoch:  426  Loss: 0.0893524789\n",
            "Epoch:  427  Loss: 0.0895317958\n",
            "Epoch:  428  Loss: 0.0900002000\n",
            "Epoch:  429  Loss: 0.0886439203\n",
            "Epoch:  430  Loss: 0.0883655054\n",
            "\n",
            "------>  Accuracy on TestSet: 74.8680  <------\n",
            "------>  Accuracy on TrainSet: 92.0164  <------\n",
            "\n",
            "Epoch:  431  Loss: 0.0881167105\n",
            "Epoch:  432  Loss: 0.0884144011\n",
            "Epoch:  433  Loss: 0.0884797123\n",
            "Epoch:  434  Loss: 0.0895857981\n",
            "Epoch:  435  Loss: 0.0882451687\n",
            "\n",
            "------>  Accuracy on TestSet: 74.9179  <------\n",
            "------>  Accuracy on TrainSet: 92.2614  <------\n",
            "\n",
            "Epoch:  436  Loss: 0.0885830605\n",
            "Epoch:  437  Loss: 0.0865616344\n",
            "Epoch:  438  Loss: 0.0877044129\n",
            "Epoch:  439  Loss: 0.0878469610\n",
            "Epoch:  440  Loss: 0.0878224344\n",
            "\n",
            "------>  Accuracy on TestSet: 74.9608  <------\n",
            "------>  Accuracy on TrainSet: 92.2602  <------\n",
            "\n",
            "Epoch:  441  Loss: 0.0868831536\n",
            "Epoch:  442  Loss: 0.0878476057\n",
            "Epoch:  443  Loss: 0.0882312717\n",
            "Epoch:  444  Loss: 0.0866359959\n",
            "Epoch:  445  Loss: 0.0876050259\n",
            "\n",
            "------>  Accuracy on TestSet: 75.0392  <------\n",
            "------>  Accuracy on TrainSet: 92.3030  <------\n",
            "\n",
            "Epoch:  446  Loss: 0.0861698963\n",
            "Epoch:  447  Loss: 0.0864130405\n",
            "Epoch:  448  Loss: 0.0872828436\n",
            "Epoch:  449  Loss: 0.0872071138\n",
            "Epoch:  450  Loss: 0.0874485274\n",
            "\n",
            "------>  Accuracy on TestSet: 74.8430  <------\n",
            "------>  Accuracy on TrainSet: 92.1127  <------\n",
            "\n",
            "Epoch:  451  Loss: 0.0873132908\n",
            "Epoch:  452  Loss: 0.0869013580\n",
            "Epoch:  453  Loss: 0.0863648004\n",
            "Epoch:  454  Loss: 0.0868593954\n",
            "Epoch:  455  Loss: 0.0860046726\n",
            "\n",
            "------>  Accuracy on TestSet: 74.5076  <------\n",
            "------>  Accuracy on TrainSet: 91.6240  <------\n",
            "\n",
            "Epoch:  456  Loss: 0.0879194152\n",
            "Epoch:  457  Loss: 0.0862854301\n",
            "Epoch:  458  Loss: 0.0864561721\n",
            "Epoch:  459  Loss: 0.0863902621\n",
            "Epoch:  460  Loss: 0.0857842230\n",
            "\n",
            "------>  Accuracy on TestSet: 74.8038  <------\n",
            "------>  Accuracy on TrainSet: 91.9474  <------\n",
            "\n",
            "Epoch:  461  Loss: 0.0870007644\n",
            "Epoch:  462  Loss: 0.0868493972\n",
            "Epoch:  463  Loss: 0.0863899691\n",
            "Epoch:  464  Loss: 0.0855636237\n",
            "Epoch:  465  Loss: 0.0865657660\n",
            "\n",
            "------>  Accuracy on TestSet: 75.1035  <------\n",
            "------>  Accuracy on TrainSet: 92.3268  <------\n",
            "\n",
            "Epoch:  466  Loss: 0.0859662959\n",
            "Epoch:  467  Loss: 0.0867072149\n",
            "Epoch:  468  Loss: 0.0862583990\n",
            "Epoch:  469  Loss: 0.0855467172\n",
            "Epoch:  470  Loss: 0.0849711659\n",
            "\n",
            "------>  Accuracy on TestSet: 74.1473  <------\n",
            "------>  Accuracy on TrainSet: 90.8771  <------\n",
            "\n",
            "Epoch:  471  Loss: 0.0854859265\n",
            "Epoch:  472  Loss: 0.0867310652\n",
            "Epoch:  473  Loss: 0.0856294520\n",
            "Epoch:  474  Loss: 0.0861644181\n",
            "Epoch:  475  Loss: 0.0869106147\n",
            "\n",
            "------>  Accuracy on TestSet: 75.0678  <------\n",
            "------>  Accuracy on TrainSet: 92.2103  <------\n",
            "\n",
            "Epoch:  476  Loss: 0.0855901816\n",
            "Epoch:  477  Loss: 0.0859229162\n",
            "Epoch:  478  Loss: 0.0859019991\n",
            "Epoch:  479  Loss: 0.0856089798\n",
            "Epoch:  480  Loss: 0.0848255349\n",
            "\n",
            "------>  Accuracy on TestSet: 75.0607  <------\n",
            "------>  Accuracy on TrainSet: 92.1389  <------\n",
            "\n",
            "Epoch:  481  Loss: 0.0849549433\n",
            "Epoch:  482  Loss: 0.0847852227\n",
            "Epoch:  483  Loss: 0.0851737460\n",
            "Epoch:  484  Loss: 0.0856490972\n",
            "Epoch:  485  Loss: 0.0852446367\n",
            "\n",
            "------>  Accuracy on TestSet: 74.8894  <------\n",
            "------>  Accuracy on TrainSet: 92.1758  <------\n",
            "\n",
            "Epoch:  486  Loss: 0.0851156283\n",
            "Epoch:  487  Loss: 0.0854355613\n",
            "Epoch:  488  Loss: 0.0848657716\n",
            "Epoch:  489  Loss: 0.0859106809\n",
            "Epoch:  490  Loss: 0.0848961736\n",
            "\n",
            "------>  Accuracy on TestSet: 74.5504  <------\n",
            "------>  Accuracy on TrainSet: 91.5205  <------\n",
            "\n",
            "Epoch:  491  Loss: 0.0852183242\n",
            "Epoch:  492  Loss: 0.0847131422\n",
            "Epoch:  493  Loss: 0.0847204095\n",
            "Epoch:  494  Loss: 0.0853484051\n",
            "Epoch:  495  Loss: 0.0853984235\n",
            "\n",
            "------>  Accuracy on TestSet: 74.7467  <------\n",
            "------>  Accuracy on TrainSet: 91.9189  <------\n",
            "\n",
            "Epoch:  496  Loss: 0.0853907552\n",
            "Epoch:  497  Loss: 0.0846494894\n",
            "Epoch:  498  Loss: 0.0847797050\n",
            "Epoch:  499  Loss: 0.0837349256\n",
            "Mean loss: 0.0882952109\n",
            "Estimated time: 13.082 min\n",
            "Epoch:  500  Loss: 0.0845086388\n",
            "\n",
            "------>  Accuracy on TestSet: 74.5041  <------\n",
            "------>  Accuracy on TrainSet: 91.8499  <------\n",
            "\n",
            "Epoch:  501  Loss: 0.0846679394\n",
            "Epoch:  502  Loss: 0.0849670310\n",
            "Epoch:  503  Loss: 0.0848692675\n",
            "Epoch:  504  Loss: 0.0848302636\n",
            "Epoch:  505  Loss: 0.0847042527\n",
            "\n",
            "------>  Accuracy on TestSet: 75.0000  <------\n",
            "------>  Accuracy on TrainSet: 92.3185  <------\n",
            "\n",
            "Epoch:  506  Loss: 0.0846647267\n",
            "Epoch:  507  Loss: 0.0834822953\n",
            "Epoch:  508  Loss: 0.0854856302\n",
            "Epoch:  509  Loss: 0.0851904852\n",
            "Epoch:  510  Loss: 0.0854303958\n",
            "\n",
            "------>  Accuracy on TestSet: 74.5041  <------\n",
            "------>  Accuracy on TrainSet: 91.8202  <------\n",
            "\n",
            "Epoch:  511  Loss: 0.0852494763\n",
            "Epoch:  512  Loss: 0.0841964871\n",
            "Epoch:  513  Loss: 0.0847042235\n",
            "Epoch:  514  Loss: 0.0841293197\n",
            "Epoch:  515  Loss: 0.0841015162\n",
            "\n",
            "------>  Accuracy on TestSet: 74.7538  <------\n",
            "------>  Accuracy on TrainSet: 92.2662  <------\n",
            "\n",
            "Epoch:  516  Loss: 0.0840999027\n",
            "Epoch:  517  Loss: 0.0845684319\n",
            "Epoch:  518  Loss: 0.0836816322\n",
            "Epoch:  519  Loss: 0.0836275451\n",
            "Epoch:  520  Loss: 0.0831952072\n",
            "\n",
            "------>  Accuracy on TestSet: 75.0856  <------\n",
            "------>  Accuracy on TrainSet: 92.2876  <------\n",
            "\n",
            "Epoch:  521  Loss: 0.0836852189\n",
            "Epoch:  522  Loss: 0.0838975910\n",
            "Epoch:  523  Loss: 0.0850272041\n",
            "Epoch:  524  Loss: 0.0841005028\n",
            "Epoch:  525  Loss: 0.0845841308\n",
            "\n",
            "------>  Accuracy on TestSet: 74.9679  <------\n",
            "------>  Accuracy on TrainSet: 92.2459  <------\n",
            "\n",
            "Epoch:  526  Loss: 0.0837054774\n",
            "Epoch:  527  Loss: 0.0846057964\n",
            "Epoch:  528  Loss: 0.0847553147\n",
            "Epoch:  529  Loss: 0.0834334195\n",
            "Epoch:  530  Loss: 0.0846573772\n",
            "\n",
            "------>  Accuracy on TestSet: 74.9465  <------\n",
            "------>  Accuracy on TrainSet: 91.8927  <------\n",
            "\n",
            "Epoch:  531  Loss: 0.0840924360\n",
            "Epoch:  532  Loss: 0.0841365363\n",
            "Epoch:  533  Loss: 0.0831456090\n",
            "Epoch:  534  Loss: 0.0836055973\n",
            "Epoch:  535  Loss: 0.0844439491\n",
            "\n",
            "------>  Accuracy on TestSet: 73.8476  <------\n",
            "------>  Accuracy on TrainSet: 90.5155  <------\n",
            "\n",
            "Epoch:  536  Loss: 0.0838923324\n",
            "Epoch:  537  Loss: 0.0839203828\n",
            "Epoch:  538  Loss: 0.0842712845\n",
            "Epoch:  539  Loss: 0.0832067888\n",
            "Epoch:  540  Loss: 0.0836987208\n",
            "\n",
            "------>  Accuracy on TestSet: 74.8180  <------\n",
            "------>  Accuracy on TrainSet: 92.2959  <------\n",
            "\n",
            "Epoch:  541  Loss: 0.0843078194\n",
            "Epoch:  542  Loss: 0.0845100606\n",
            "Epoch:  543  Loss: 0.0827768903\n",
            "Epoch:  544  Loss: 0.0834958458\n",
            "Epoch:  545  Loss: 0.0840232408\n",
            "\n",
            "------>  Accuracy on TestSet: 74.0973  <------\n",
            "------>  Accuracy on TrainSet: 91.2862  <------\n",
            "\n",
            "Epoch:  546  Loss: 0.0842671082\n",
            "Epoch:  547  Loss: 0.0843193079\n",
            "Epoch:  548  Loss: 0.0830456484\n",
            "Epoch:  549  Loss: 0.0836213910\n",
            "Epoch:  550  Loss: 0.0825227459\n",
            "\n",
            "------>  Accuracy on TestSet: 74.5647  <------\n",
            "------>  Accuracy on TrainSet: 92.1009  <------\n",
            "\n",
            "Epoch:  551  Loss: 0.0835924088\n",
            "Epoch:  552  Loss: 0.0838258610\n",
            "Epoch:  553  Loss: 0.0836157689\n",
            "Epoch:  554  Loss: 0.0839692244\n",
            "Epoch:  555  Loss: 0.0838634669\n",
            "\n",
            "------>  Accuracy on TestSet: 74.9929  <------\n",
            "------>  Accuracy on TrainSet: 91.9129  <------\n",
            "\n",
            "Epoch:  556  Loss: 0.0828590129\n",
            "Epoch:  557  Loss: 0.0833980135\n",
            "Epoch:  558  Loss: 0.0827428359\n",
            "Epoch:  559  Loss: 0.0843015664\n",
            "Epoch:  560  Loss: 0.0832682994\n",
            "\n",
            "------>  Accuracy on TestSet: 74.8466  <------\n",
            "------>  Accuracy on TrainSet: 92.3268  <------\n",
            "\n",
            "Epoch:  561  Loss: 0.0836371422\n",
            "Epoch:  562  Loss: 0.0841807814\n",
            "Epoch:  563  Loss: 0.0838676616\n",
            "Epoch:  564  Loss: 0.0834763940\n",
            "Epoch:  565  Loss: 0.0834597036\n",
            "\n",
            "------>  Accuracy on TestSet: 73.8726  <------\n",
            "------>  Accuracy on TrainSet: 90.6666  <------\n",
            "\n",
            "Epoch:  566  Loss: 0.0839131864\n",
            "Epoch:  567  Loss: 0.0833976523\n",
            "Epoch:  568  Loss: 0.0828351610\n",
            "Epoch:  569  Loss: 0.0831284793\n",
            "Epoch:  570  Loss: 0.0833383006\n",
            "\n",
            "------>  Accuracy on TestSet: 74.8787  <------\n",
            "------>  Accuracy on TrainSet: 92.3601  <------\n",
            "\n",
            "Epoch:  571  Loss: 0.0830200586\n",
            "Epoch:  572  Loss: 0.0838434126\n",
            "Epoch:  573  Loss: 0.0834994237\n",
            "Epoch:  574  Loss: 0.0841980756\n",
            "Epoch:  575  Loss: 0.0829419665\n",
            "\n",
            "------>  Accuracy on TestSet: 74.9643  <------\n",
            "------>  Accuracy on TrainSet: 92.2436  <------\n",
            "\n",
            "Epoch:  576  Loss: 0.0840742764\n",
            "Epoch:  577  Loss: 0.0835773531\n",
            "Epoch:  578  Loss: 0.0824264429\n",
            "Epoch:  579  Loss: 0.0840925830\n",
            "Epoch:  580  Loss: 0.0825720114\n",
            "\n",
            "------>  Accuracy on TestSet: 74.9429  <------\n",
            "------>  Accuracy on TrainSet: 92.1984  <------\n",
            "\n",
            "Epoch:  581  Loss: 0.0832839370\n",
            "Epoch:  582  Loss: 0.0830039569\n",
            "Epoch:  583  Loss: 0.0840472870\n",
            "Epoch:  584  Loss: 0.0838511668\n",
            "Epoch:  585  Loss: 0.0834149525\n",
            "\n",
            "------>  Accuracy on TestSet: 74.7895  <------\n",
            "------>  Accuracy on TrainSet: 91.7464  <------\n",
            "\n",
            "Epoch:  586  Loss: 0.0836907810\n",
            "Epoch:  587  Loss: 0.0837353358\n",
            "Epoch:  588  Loss: 0.0821521890\n",
            "Epoch:  589  Loss: 0.0825358034\n",
            "Epoch:  590  Loss: 0.0832755507\n",
            "\n",
            "------>  Accuracy on TestSet: 74.4113  <------\n",
            "------>  Accuracy on TrainSet: 91.6798  <------\n",
            "\n",
            "Epoch:  591  Loss: 0.0823532139\n",
            "Epoch:  592  Loss: 0.0838070170\n",
            "Epoch:  593  Loss: 0.0832130108\n",
            "Epoch:  594  Loss: 0.0826931294\n",
            "Epoch:  595  Loss: 0.0831099679\n",
            "\n",
            "------>  Accuracy on TestSet: 74.9893  <------\n",
            "------>  Accuracy on TrainSet: 92.3185  <------\n",
            "\n",
            "Epoch:  596  Loss: 0.0837743872\n",
            "Epoch:  597  Loss: 0.0830799342\n",
            "Epoch:  598  Loss: 0.0832714203\n",
            "Epoch:  599  Loss: 0.0839524632\n",
            "Mean loss: 0.0837927242\n",
            "Estimated time: 0.131 min\n",
            "Finished Training (elapsed time 78.178 min)\n",
            "\n",
            "Best net loaded!!\n",
            "\n",
            "Best Test Accuracy:75.10350 \n",
            "Epoch of best accuracy:465 \n",
            "Final Accuracy on Train:92.31850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_SQCBtu6RIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2807a33-7106-4734-b3fa-c7257533503a"
      },
      "source": [
        "net.load_state_dict(torch.load(PATH+\"net/feed_forward_12chars_MSE_1\"))\r\n",
        "correct = 0\r\n",
        "total = 0\r\n",
        "vloss = 0\r\n",
        "net.eval()\r\n",
        "with torch.no_grad():\r\n",
        "    for i, data in enumerate(testloader, 0):\r\n",
        "        inputs, labels = data\r\n",
        "        if want_cuda and torch.cuda.is_available():\r\n",
        "            inputs = inputs.cuda()\r\n",
        "            labels = labels.cuda()\r\n",
        "        #squeeze for MSE\r\n",
        "        outputs = net(inputs).squeeze()\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss += loss.item()\r\n",
        "        #this for MSELoss\r\n",
        "        predicted = torch.round(outputs)\r\n",
        "        #this for crossentropy\r\n",
        "        #_, predicted = torch.max(outputs,1)\r\n",
        "        total += labels.size(0)\r\n",
        "        correct += (predicted == labels).sum().item()\r\n",
        "print(\"Accuracy: \", round(correct/total *100, 4), \"%\")"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  75.1035 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoNg-Wmi4h6Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}