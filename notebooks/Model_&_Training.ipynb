{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_&_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DGSTBKL6xxmj",
        "ciycA-wzt_tl",
        "oLNDyzTmDV2n",
        "aROaOz7Xxggl"
      ],
      "authorship_tag": "ABX9TyPRoyuxmcI3BhlbJjqw4gmK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoMorri/Language_Classification/blob/main/notebooks/Model_%26_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKDzDhjHxpql"
      },
      "source": [
        "Define here your **PATH**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taGo7b63wL8l",
        "outputId": "872d01ea-79ab-4f63-d300-9f9b6202eed6"
      },
      "source": [
        "#this if you work with Google Drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EIns3jJxoJZ"
      },
      "source": [
        "PATH = \"/content/drive/MyDrive/language/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGSTBKL6xxmj"
      },
      "source": [
        "## Importing all the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQpj_ZgLcmbF"
      },
      "source": [
        "import torch\r\n",
        "import torchvision\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torchvision import datasets, transforms\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.optim.lr_scheduler import StepLR\r\n",
        "import time\r\n",
        "from torchsummary import summary\r\n",
        "import torch.optim as optim\r\n",
        "from torch.optim.lr_scheduler import MultiStepLR\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J3iMVaJD8BG"
      },
      "source": [
        "# Making the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpnLWOWb_i3V"
      },
      "source": [
        "There are three possible dataset, altough the only one that seems to work somewhat nicely is the Basic Word Encoding. The other two are still interesting ways to analize words, so I'll leave them there.<br/>\r\n",
        "\r\n",
        "If we already have a dataset to load we just need to compile the dataset class (linked here [Dataset Class](#Dataset-Class))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Avb8jr8t7F5"
      },
      "source": [
        "## Basic Word Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgTPUBvUEJbB"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x_W4bkzENOV"
      },
      "source": [
        "words = pd.read_csv(PATH+\"/words_all_unique.csv\", usecols=['spelling', 'lexicality'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNDUU61kRoA7"
      },
      "source": [
        "In order to make the dataset we need to encode the words in some way. We will use a simple method: simply assigning a binary vector to each letter, then putting together all the vector that make a word, eventually adding 0s at the end if the word is shorter that the longest word in the dataset.</br>\r\n",
        "</br>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOmuD6-6EbfP"
      },
      "source": [
        "def word_to_vec(word, max_length):\r\n",
        "    n = len(word)\r\n",
        "    vec = ''\r\n",
        "    for i in range(n):\r\n",
        "        cur_char = word[i]\r\n",
        "        idx = ord(cur_char) - 97\r\n",
        "        tmp = (str(0)*idx) + str(1) + (str(0)*(25-idx))\r\n",
        "        vec = vec + tmp\r\n",
        "    if n < max_length:\r\n",
        "        exce = max_length - n\r\n",
        "        vec = vec + (str(0)*26*exce)\r\n",
        "    output = []\r\n",
        "    for v in vec:\r\n",
        "        output.append(float(v))\r\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7PP7_3BMOJb"
      },
      "source": [
        "def word_to_vec2(word, max_length):\r\n",
        "    len_w = len(word)\r\n",
        "    chars = [ord(c) for c in word]\r\n",
        "    max_char = 122. # this is z\r\n",
        "    normal = [round(c/max_char, 5) for c in chars]\r\n",
        "    if len_w < max_length:\r\n",
        "        diff = max_length - len_w\r\n",
        "        zeros =  [0 for i in range(diff)]\r\n",
        "        normal.extend(zeros)\r\n",
        "    return normal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isICQ_zwNXDM",
        "outputId": "f8ad1abd-8612-4b59-d268-a9474261297a"
      },
      "source": [
        "print(word_to_vec2(\"hello\", max_length=5), word_to_vec2(\"hellk\", max_length=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.85246, 0.82787, 0.88525, 0.88525, 0.90984] [0.85246, 0.82787, 0.88525, 0.88525, 0.87705]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMhbpTvhTP4g"
      },
      "source": [
        "Now we need a basic function to make the labels vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eQnj_uXEgd4"
      },
      "source": [
        "def label_maker(lexicality):\r\n",
        "    if lexicality == 'W':\r\n",
        "        return 0\r\n",
        "    elif lexicality == 'N':\r\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O30vuD1gX3K0"
      },
      "source": [
        "\r\n",
        "We can now define the Dataset class in the standard way.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCBz0c0rKkhm"
      },
      "source": [
        "class WordsDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.samples = []\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.samples)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.to_list()\r\n",
        "        \r\n",
        "        input = self.samples[idx][0]\r\n",
        "        label = self.samples[idx][1]\r\n",
        "        return input, label\r\n",
        "    \r\n",
        "    def __addsample__(self, spelling, lexicality, max_length=4):\r\n",
        "        if (type(spelling) == type(\"str\")):\r\n",
        "            input = torch.tensor(word_to_vec(spelling, max_length=max_length), dtype=torch.float32)\r\n",
        "            #check = [False if (i > 1. or i < 0.) else True for i in input]\r\n",
        "            #if not all(check):\r\n",
        "            #    print(\"PROBLEM\")\r\n",
        "            #    print(len(input), spelling)\r\n",
        "            if (len(input) > max_length*26):\r\n",
        "                print(\"PROBLEM\")\r\n",
        "                print(len(input), spelling)\r\n",
        "            else:\r\n",
        "                # here if it is torch is for the MSELoss, if it is an INT is for the crossentropy\r\n",
        "                label = torch.tensor(label_maker(lexicality), dtype=torch.float32)\r\n",
        "                #label = label_maker(lexicality)\r\n",
        "                self.samples.append([input, label])\r\n",
        "        else:\r\n",
        "            print(\"Something Strange:\", end='\\t')\r\n",
        "            print(spelling)\r\n",
        "\r\n",
        "    def __removesample__(self, idx=0, value=None):\r\n",
        "        '''\r\n",
        "        If value is something, the element corresponding to that value is removed.\r\n",
        "        Else the element at index idx is popped.\r\n",
        "        '''\r\n",
        "        if (value is not None):\r\n",
        "            self.samples.remove(value)\r\n",
        "        else:\r\n",
        "            self.samples.pop(0)\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEwraSVgFD2u"
      },
      "source": [
        "dataset = WordsDataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKhzwqykX9on"
      },
      "source": [
        "The dataset is now empty, we can fill it with all our words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaxCiCzMViZY"
      },
      "source": [
        "MAX_LENGTH = 12\r\n",
        "\r\n",
        "counting = 0\r\n",
        "for w in words[words['lexicality']=='W']['spelling']:\r\n",
        "    if (type(w) == type(\"str\")):\r\n",
        "        if (len(w) > MAX_LENGTH):\r\n",
        "            pass\r\n",
        "        else:\r\n",
        "            dataset.__addsample__(w, 'W', max_length=MAX_LENGTH)\r\n",
        "            counting += 1\r\n",
        "\r\n",
        "count_non = 0\r\n",
        "for w in words[words['lexicality']=='N']['spelling']:\r\n",
        "    if (type(w) == type(\"str\")):\r\n",
        "        if (len(w) > MAX_LENGTH):\r\n",
        "            pass\r\n",
        "        else:\r\n",
        "            dataset.__addsample__(w, 'N', max_length=MAX_LENGTH)\r\n",
        "            count_non +=1\r\n",
        "\r\n",
        "    if count_non == counting:\r\n",
        "        break\r\n",
        "\r\n",
        "\r\n",
        "print(\"\\n\\nWords: \",counting)\r\n",
        "print(\"Non-Words: \",count_non)\r\n",
        "print(\"Tot Elements in dataset: \", dataset.__len__())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJKuajC3JHPh"
      },
      "source": [
        "ratio_test_train = 0.25\r\n",
        "test_el = round(dataset.__len__()*ratio_test_train)\r\n",
        "train_el = dataset.__len__() - test_el\r\n",
        "\r\n",
        "trainset, testset = torch.utils.data.random_split(dataset, [train_el, test_el])\r\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32)\r\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t57y0-OdxgMB"
      },
      "source": [
        "Here we can save the dataloader in order to have reproducible results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-S93brYxlQS"
      },
      "source": [
        "torch.save(trainloader, PATH+\"trainloader_12chars_MSE.pth\")\r\n",
        "torch.save(testloader, PATH+\"testloader_12chars_MSE.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciycA-wzt_tl"
      },
      "source": [
        "## Char2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzkOHyUCxKgl"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKDWMkIIxKgn"
      },
      "source": [
        "words = pd.read_csv(PATH+\"/words_all_unique.csv\", usecols=['spelling', 'lexicality'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5OihxqSuQ0b",
        "outputId": "f4ea3b86-ebd3-494e-9214-9b0cb0083073"
      },
      "source": [
        "!pip install chars2vec\r\n",
        "import chars2vec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting chars2vec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/0a/8c327aae23e0532d239ec7b30446aca765eb5d9547b4c4b09cdd82e49797/chars2vec-0.1.7.tar.gz (8.1MB)\n",
            "\u001b[K     |████████████████████████████████| 8.1MB 6.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: chars2vec\n",
            "  Building wheel for chars2vec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chars2vec: filename=chars2vec-0.1.7-cp36-none-any.whl size=8111095 sha256=eb1052fd704a7884612f4c0f4bd07ccc748d84880e16f231696409a1c244ba33\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/b6/65/d7e778ef1213ec77d315aea0f536068b96e36cc94c02abbfde\n",
            "Successfully built chars2vec\n",
            "Installing collected packages: chars2vec\n",
            "Successfully installed chars2vec-0.1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0ksEU76rW3T"
      },
      "source": [
        "from torch.utils.data import Dataset\r\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOGWzCinrbMI"
      },
      "source": [
        "def label_maker(lexicality):\r\n",
        "    if lexicality == 'W':\r\n",
        "        return 0\r\n",
        "    elif lexicality == 'N':\r\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVEZdKmfroz9"
      },
      "source": [
        "class WordsDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.samples = []\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.samples)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.to_list()\r\n",
        "        \r\n",
        "        input = self.samples[idx][0]\r\n",
        "        label = self.samples[idx][1]\r\n",
        "        return input, label\r\n",
        "    \r\n",
        "    def __addsample__(self, vector_word, lexicality):\r\n",
        "\r\n",
        "        input = torch.tensor(vector_word, dtype=torch.float32)\r\n",
        "        #label = torch.tensor(label_maker(lexicality), dtype=torch.float32)\r\n",
        "        label = label_maker(lexicality)\r\n",
        "        self.samples.append([input, label])\r\n",
        "\r\n",
        "    def __removesample__(self, idx=0, value=None):\r\n",
        "        '''\r\n",
        "        If value is something, the element corresponding to that value is removed.\r\n",
        "        Else the element at index idx is popped.\r\n",
        "        '''\r\n",
        "        if (value is not None):\r\n",
        "            self.samples.remove(value)\r\n",
        "        else:\r\n",
        "            self.samples.pop(0)\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jieM3DBNsci2"
      },
      "source": [
        "c2v_model = chars2vec.load_model('eng_50')\r\n",
        "real = []\r\n",
        "for i,w in enumerate(words[words['lexicality']=='W']['spelling']):\r\n",
        "    if (type(w) == type(\"str\")):\r\n",
        "        real.append(w)\r\n",
        "\r\n",
        "\r\n",
        "nonw = []\r\n",
        "for i,w in enumerate(words[words['lexicality']=='N']['spelling']):\r\n",
        "    if (type(w) == type(\"str\")):\r\n",
        "        nonw.append(w)\r\n",
        "\r\n",
        "\r\n",
        "real_word_embed = c2v_model.vectorize_words(real)\r\n",
        "non_word_embed = c2v_model.vectorize_words(nonw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQHdYjWysnCs",
        "outputId": "26d67604-f0f3-4f4d-e04f-a4e4f3c0778a"
      },
      "source": [
        "print(real_word_embed.shape)\r\n",
        "print(non_word_embed.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(61853, 50)\n",
            "(329845, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLhDVJhGtHbG"
      },
      "source": [
        "dataset = WordsDataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3DdDfXos9jb",
        "outputId": "858e5ea8-a13e-46a7-933a-add66d927c67"
      },
      "source": [
        "count = 0\r\n",
        "for rw in real_word_embed:\r\n",
        "    dataset.__addsample__(rw, \"W\")\r\n",
        "    count += 1\r\n",
        "count2 = 0\r\n",
        "for nw in non_word_embed:\r\n",
        "    dataset.__addsample__(nw, \"N\")\r\n",
        "    count2 +=1\r\n",
        "    if count2 == count:\r\n",
        "        break\r\n",
        "\r\n",
        "print(dataset.__len__())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "123706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaitguTntjra"
      },
      "source": [
        "ratio_test_train = 0.2\r\n",
        "test_el = round(dataset.__len__()*ratio_test_train)\r\n",
        "train_el = dataset.__len__() - test_el\r\n",
        "\r\n",
        "trainset, testset = torch.utils.data.random_split(dataset, [train_el, test_el])\r\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128)\r\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0OXJtm6tvwZ"
      },
      "source": [
        "torch.save(trainloader, PATH+\"trainloader_char2vec_cross.pth\")\r\n",
        "torch.save(testloader, PATH+\"testloader_char2vec_cross.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLNDyzTmDV2n"
      },
      "source": [
        "## Bigrams Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQBF759RDYpd"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA6rMQf6DYpf"
      },
      "source": [
        "words = pd.read_csv(PATH+\"/words_all_unique.csv\", usecols=['spelling', 'lexicality'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OInnz_GN2sR8"
      },
      "source": [
        "def create_dict(text, normal=False):\r\n",
        "    bigrams = {}\r\n",
        "    for w in text:\r\n",
        "        for i in range(0, len(w)-1):\r\n",
        "            big = w[i]+w[i+1]\r\n",
        "            keys = bigrams.keys()\r\n",
        "            check = [k == big for k in bigrams.keys()]\r\n",
        "            if any(check):\r\n",
        "                bigrams[big] += 1\r\n",
        "            else:\r\n",
        "                bigrams[big] = 1\r\n",
        "\r\n",
        "    if normal:\r\n",
        "        max_dict = max(bigrams.values())\r\n",
        "        bigrams = {k: v/max_dict for k,v in bigrams.items()}\r\n",
        "    return bigrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb6J88GI23N6"
      },
      "source": [
        "def encode_words(words, dictio):\r\n",
        "    data = []\r\n",
        "    for w in words:\r\n",
        "        vect = []\r\n",
        "        for i in range(0, len(w)-1):\r\n",
        "            big = w[i] + w[i+1]\r\n",
        "            val = dictio[big]\r\n",
        "            vect.append(val)\r\n",
        "        data.append(vect)\r\n",
        "\r\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBjEQCDF2pT8"
      },
      "source": [
        "real = []\r\n",
        "for i,w in enumerate(words[words['lexicality']=='W']['spelling']):\r\n",
        "    if (type(w) == type(\"str\")):\r\n",
        "        real.append(w)\r\n",
        "\r\n",
        "\r\n",
        "nonw = []\r\n",
        "for i,w in enumerate(words[words['lexicality']=='N']['spelling']):\r\n",
        "    if (type(w) == type(\"str\")):\r\n",
        "        nonw.append(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p-MWZe_2q7N"
      },
      "source": [
        "real_count = len(real)\r\n",
        "select_non = nonw[:real_count]\r\n",
        "tot = []\r\n",
        "tot.extend(real)\r\n",
        "tot.extend(select_non)\r\n",
        "dictionary = create_dict(tot, normal=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d1eI4yTCDtt"
      },
      "source": [
        "real_encoded = encode_words(real, dictio=dictionary)\r\n",
        "non_encoded = encode_words(select_non, dictio=dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FAruodQCb0F"
      },
      "source": [
        "def label_maker(lexicality):\r\n",
        "    if lexicality == 'W':\r\n",
        "        return 0\r\n",
        "    elif lexicality == 'N':\r\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_zLRqksCNLQ"
      },
      "source": [
        "class WordsDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.samples = []\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.samples)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.to_list()\r\n",
        "        \r\n",
        "        input = self.samples[idx][0]\r\n",
        "        label = self.samples[idx][1]\r\n",
        "        return input, label\r\n",
        "    \r\n",
        "    def __addsample__(self, vector_word, lexicality, max_length):\r\n",
        "        \r\n",
        "        if len(vector_word) < max_length:\r\n",
        "            diff = max_length-len(vector_word)\r\n",
        "            zeros = [0 for i in range(diff)]\r\n",
        "            vector_word.extend(zeros)\r\n",
        "        \r\n",
        "        input = torch.tensor(vector_word, dtype=torch.float32)\r\n",
        "        #label = torch.tensor(label_maker(lexicality), dtype=torch.float32)\r\n",
        "        label = label_maker(lexicality)\r\n",
        "        self.samples.append([input, label])\r\n",
        "\r\n",
        "    def __removesample__(self, idx=0, value=None):\r\n",
        "        '''\r\n",
        "        If value is something, the element corresponding to that value is removed.\r\n",
        "        Else the element at index idx is popped.\r\n",
        "        '''\r\n",
        "        if (value is not None):\r\n",
        "            self.samples.remove(value)\r\n",
        "        else:\r\n",
        "            self.samples.pop(0)\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT6TeFhsCmig"
      },
      "source": [
        "dataset = WordsDataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQA-G-oaCmij",
        "outputId": "93c8d686-27a9-4a7e-df7c-32a46b33d4af"
      },
      "source": [
        "MAX_LENGTH = 21\r\n",
        "\r\n",
        "count = 0\r\n",
        "for rw in real_encoded:\r\n",
        "    if len(rw) < MAX_LENGTH:\r\n",
        "        dataset.__addsample__(rw, \"W\", MAX_LENGTH)\r\n",
        "        count += 1\r\n",
        "\r\n",
        "count2 = 0\r\n",
        "for nw in non_encoded:\r\n",
        "    if len(nw) < MAX_LENGTH:\r\n",
        "        dataset.__addsample__(nw, \"N\", MAX_LENGTH)\r\n",
        "        count2 +=1\r\n",
        "        if count2 == count:\r\n",
        "            break\r\n",
        "\r\n",
        "print(dataset.__len__())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "123700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7p_2J0jCmim"
      },
      "source": [
        "ratio_test_train = 0.2\r\n",
        "test_el = round(dataset.__len__()*ratio_test_train)\r\n",
        "train_el = dataset.__len__() - test_el\r\n",
        "\r\n",
        "trainset, testset = torch.utils.data.random_split(dataset, [train_el, test_el])\r\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128)\r\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVP1AI8LCmin"
      },
      "source": [
        "torch.save(trainloader, PATH+\"trainloader_bigrams_complete_cross.pth\")\r\n",
        "torch.save(testloader, PATH+\"testloader_bigrams_complete_cross.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL2z7_hxyE5y"
      },
      "source": [
        "## Creating Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aROaOz7Xxggl"
      },
      "source": [
        "### Conv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R8kF1IA2Xsu"
      },
      "source": [
        "class Conv_Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=5) #50*1 -> 46*8\r\n",
        "        self.drop1 = nn.Dropout()\r\n",
        "        self.batch1 = nn.BatchNorm1d(num_features=8)\r\n",
        "        self.max1 = nn.MaxPool1d(kernel_size=2, stride=2) #46*8 -> 23*8\r\n",
        "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3) #23*8 -> 21*16\r\n",
        "        self.drop2 = nn.Dropout()\r\n",
        "        self.batch2 = nn.BatchNorm1d(num_features=16)\r\n",
        "        self.max2 = nn.MaxPool1d(kernel_size=3, stride=2) #21*16 -> 10*16\r\n",
        "        #self.conv3 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5) #42*16 -> 42*16\r\n",
        "        #self.drop3 = nn.Dropout()\r\n",
        "        #self.batch3 = nn.BatchNorm1d(num_features=16)\r\n",
        "\r\n",
        "        self.linear1 = nn.Linear(in_features=10*16, out_features=32)\r\n",
        "        self.linear2 = nn.Linear(in_features=32, out_features=2)\r\n",
        "\r\n",
        "        self.act = nn.ReLU()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.drop1(out)\r\n",
        "        out = self.batch1(out)\r\n",
        "        out = self.act(out)\r\n",
        "        out = self.max1(out)\r\n",
        "\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.drop2(out)\r\n",
        "        out = self.batch2(out)\r\n",
        "        out = self.act(out)\r\n",
        "        out = self.max2(out)\r\n",
        "\r\n",
        "        out = torch.flatten(out, 1)\r\n",
        "\r\n",
        "        out = self.linear1(out)\r\n",
        "        out = self.act(out)\r\n",
        "        out = self.linear2(out)\r\n",
        "\r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRFxUO6qxjRk"
      },
      "source": [
        "### Feedforward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RUst0bu-ilw"
      },
      "source": [
        "class Words_Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.linear1 = nn.Linear(312, 450)\r\n",
        "        self.batch1 = nn.BatchNorm1d(450) \r\n",
        "        self.drop1 = nn.Dropout()\r\n",
        "        self.linear2 = nn.Linear(450, 256)\r\n",
        "        self.batch2 = nn.BatchNorm1d(256)\r\n",
        "        self.drop2 = nn.Dropout()\r\n",
        "        self.linear3 = nn.Linear(256, 64)\r\n",
        "        #self.batch3 = nn.BatchNorm1d(64)\r\n",
        "        #self.drop3 = nn.Dropout()\r\n",
        "        self.linear4 = nn.Linear(64, 32)\r\n",
        "        #self.drop4 = nn.Dropout()\r\n",
        "        self.linear5 = nn.Linear(32, 1)\r\n",
        "\r\n",
        "        self.act = nn.ReLU()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.linear1(x)\r\n",
        "        out = self.drop1(out)\r\n",
        "        out = self.batch1(out)\r\n",
        "        out = self.act(out)\r\n",
        "\r\n",
        "        out = self.linear2(out)\r\n",
        "        out = self.drop2(out)\r\n",
        "        out = self.batch2(out)\r\n",
        "        out = self.act(out)\r\n",
        "\r\n",
        "        out = self.linear3(out)\r\n",
        "        #out = self.drop3(out)\r\n",
        "        #out = self.batch3(out)\r\n",
        "        out = self.act(out)\r\n",
        "\r\n",
        "        out = self.linear4(out)\r\n",
        "        #out = self.drop4(out)\r\n",
        "        out = self.act(out)\r\n",
        "\r\n",
        "        out = self.linear5(out)\r\n",
        "\r\n",
        "        return out\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAceI2n7yGZv"
      },
      "source": [
        "### Load the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rW83HVmyJCT"
      },
      "source": [
        "trainloader = torch.load(PATH+\"trainloader_12chars_MSE.pth\")\r\n",
        "testloader = torch.load(PATH+\"testloader_12chars_MSE.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EniQLxvR3M78"
      },
      "source": [
        "## Training\r\n",
        "For the training we will use the GPU, even though is a fairly small network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTgxuw3u2k4t",
        "outputId": "221b5b31-8433-41b9-bc06-30e47d9ece0b"
      },
      "source": [
        "net = Words_Net()\r\n",
        "#net.load_state_dict(torch.load(PATH+\"/net/feed_forward_10chars_MSE_2_3\"))\r\n",
        "\r\n",
        "want_cuda = True\r\n",
        "have_cuda = torch.cuda.is_available()\r\n",
        "if want_cuda and have_cuda:\r\n",
        "    net.cuda()\r\n",
        "    print(torch.cuda.get_device_name())\r\n",
        "else:\r\n",
        "    print (\"No cuda available!\\n\")\r\n",
        "summary(net, (312,))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla T4\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 450]         140,850\n",
            "           Dropout-2                  [-1, 450]               0\n",
            "       BatchNorm1d-3                  [-1, 450]             900\n",
            "              ReLU-4                  [-1, 450]               0\n",
            "            Linear-5                  [-1, 256]         115,456\n",
            "           Dropout-6                  [-1, 256]               0\n",
            "       BatchNorm1d-7                  [-1, 256]             512\n",
            "              ReLU-8                  [-1, 256]               0\n",
            "            Linear-9                   [-1, 64]          16,448\n",
            "             ReLU-10                   [-1, 64]               0\n",
            "           Linear-11                   [-1, 32]           2,080\n",
            "             ReLU-12                   [-1, 32]               0\n",
            "           Linear-13                    [-1, 1]              33\n",
            "================================================================\n",
            "Total params: 276,279\n",
            "Trainable params: 276,279\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.02\n",
            "Params size (MB): 1.05\n",
            "Estimated Total Size (MB): 1.08\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl261nA_3m2A"
      },
      "source": [
        "We define the training parameters, in this case we will use the CrossEntropy loss and the SGD algorithm to train the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lBZcMoL22iL"
      },
      "source": [
        "START = 0\r\n",
        "EPOCH = 500\r\n",
        "learn = 0.1\r\n",
        "\r\n",
        "criterion = nn.MSELoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=learn, weight_decay=4e-3)\r\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [50,150, 300], gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMdHSvyg4i77"
      },
      "source": [
        "We now define some simple functions for the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWkh8AbojODh"
      },
      "source": [
        "def accuracy():\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for i, data in enumerate(testloader, 0):\r\n",
        "            inputs, labels = data\r\n",
        "            if want_cuda and torch.cuda.is_available():\r\n",
        "                inputs = inputs.cuda()\r\n",
        "                labels = labels.cuda()\r\n",
        "            # need to squeeze if MSELoss\r\n",
        "            outputs = net(inputs).squeeze()\r\n",
        "            #loss = criterion(outputs, labels)\r\n",
        "            #loss += loss.item()\r\n",
        "            #this for MSELoss\r\n",
        "            predicted = torch.round(outputs)\r\n",
        "            #this for crossentropy\r\n",
        "            #_, predicted = torch.max(outputs,1)\r\n",
        "            total += labels.size(0)\r\n",
        "            correct += (predicted == labels).sum().item()\r\n",
        "    \r\n",
        "    return (round(correct/total *100, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3m9gBBRUXXY"
      },
      "source": [
        "def overfit_check():\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for i, data in enumerate(trainloader, 0):\r\n",
        "            inputs, labels = data\r\n",
        "            if want_cuda and torch.cuda.is_available():\r\n",
        "                inputs = inputs.cuda()\r\n",
        "                labels = labels.cuda()\r\n",
        "            # need to squeeze if MSELoss\r\n",
        "            outputs = net(inputs).squeeze()\r\n",
        "            #loss = criterion(outputs, labels)\r\n",
        "            #loss += loss.item()\r\n",
        "            #this for MSELoss\r\n",
        "            predicted = torch.round(outputs)\r\n",
        "            #this for crossentropy\r\n",
        "            #_, predicted = torch.max(outputs,1)\r\n",
        "            total += labels.size(0)\r\n",
        "            correct += (predicted == labels).sum().item()\r\n",
        "    \r\n",
        "    return (round(correct/total *100, 4))   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56PfvBJT3waO"
      },
      "source": [
        "def training(acquire = False , PATH = None):\r\n",
        "    running_loss = 0.0\r\n",
        "    losst = 0\r\n",
        "    index = 0\r\n",
        "    for i, data in enumerate(trainloader, 0):\r\n",
        "        # get the inputs, maybe they need to be tensors?\r\n",
        "        inputs, labels = data\r\n",
        "\r\n",
        "        if want_cuda and have_cuda:\r\n",
        "          inputs = inputs.cuda()\r\n",
        "          labels = labels.cuda()\r\n",
        "\r\n",
        "        # need to squeeze if MSELoss\r\n",
        "        outputs = net(inputs).squeeze()\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        losst +=loss.item()\r\n",
        "        index +=1\r\n",
        "    return losst/index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwY0Q2qF4_8d"
      },
      "source": [
        "Now we can do the actual training of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmPIGtBE46Wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe2a24f-b3a5-4a97-d59e-f828bf5acadc"
      },
      "source": [
        "start =time.time()\r\n",
        "graph_data = np.empty((0,4))\r\n",
        "old_data = False\r\n",
        "mean_loss = 0\r\n",
        "old_acc = 0\r\n",
        "acc_check = 0\r\n",
        "best_epoch = 0\r\n",
        "\r\n",
        "for epoch in range(START, EPOCH):\r\n",
        "    loss = training()    \r\n",
        "    print(\"Epoch: \", epoch, \" Loss: %.10f\"%(loss))\r\n",
        "    mean_loss += loss\r\n",
        "\r\n",
        "    if (epoch % 100 == 99):\r\n",
        "        print(\"Mean loss: %.10f\"%(mean_loss/100))\r\n",
        "        mean_loss = 0\r\n",
        "        print('Estimated time: %.3f min' %((EPOCH- epoch)*(time.time() - start)/(60*epoch)) )\r\n",
        "\r\n",
        "\r\n",
        "    if (epoch%5 == 0):\r\n",
        "        net.eval()\r\n",
        "        over_check = overfit_check()\r\n",
        "        acc = accuracy()\r\n",
        "        acc_check += 1\r\n",
        "        print(\"\\n------>  Accuracy on TestSet: %.4f  <------\"%(acc))\r\n",
        "        print(\"------>  Accuracy on TrainSet: %.4f  <------\\n\"%(over_check))\r\n",
        "\r\n",
        "        graph_data = np.append(graph_data, [[epoch, loss, acc, over_check]], axis=0)\r\n",
        "\r\n",
        "        if old_data:\r\n",
        "            f = open(PATH+\"graphs/feed_forward_12chars_MSE_2.csv\", 'a')\r\n",
        "            np.savetxt(f, graph_data)\r\n",
        "            f.close()\r\n",
        "            graph_data = np.empty((0,4))\r\n",
        "        else:\r\n",
        "            np.savetxt(PATH+\"graphs/feed_forward_12chars_MSE_2.csv\", graph_data)\r\n",
        "            graph_data = np.empty((0,4))\r\n",
        "            old_data = True\r\n",
        "        \r\n",
        "        if (acc > old_acc):\r\n",
        "            old_acc = acc\r\n",
        "            torch.save(net.state_dict(), PATH+\"net/feed_forward_12chars_MSE_2\")\r\n",
        "            acc_check = 0\r\n",
        "            best_epoch = epoch\r\n",
        "\r\n",
        "    '''\r\n",
        "    if (acc_check > 200):\r\n",
        "        print(\"NET STOPPED LEARNING!!\")\r\n",
        "        print(\"\\nEpoch:%.d \\nLoss:%.10f \\nBest Accuracy:%.4f\"%(epoch, loss, old_acc))\r\n",
        "        break\r\n",
        "    '''\r\n",
        "\r\n",
        "    scheduler.step()\r\n",
        "    net.train()\r\n",
        "        \r\n",
        "elapsed_time = time.time() - start\r\n",
        "print('Finished Training (elapsed time %.3f min)' %(elapsed_time/60))\r\n",
        "net.load_state_dict(torch.load(PATH+\"net/feed_forward_12chars_MSE_2\"))\r\n",
        "print(\"\\nBest net loaded!!\")\r\n",
        "print(\"\\nBest Test Accuracy:%.5f \\nEpoch of best accuracy:%.d \\nFinal Accuracy on Train:%.5f\"%(old_acc, best_epoch, over_check))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0  Loss: 0.2484561609\n",
            "\n",
            "------>  Accuracy on TestSet: 53.4894  <------\n",
            "------>  Accuracy on TrainSet: 54.3843  <------\n",
            "\n",
            "Epoch:  1  Loss: 0.2427892540\n",
            "Epoch:  2  Loss: 0.2406430225\n",
            "Epoch:  3  Loss: 0.2400747253\n",
            "Epoch:  4  Loss: 0.2398925996\n",
            "Epoch:  5  Loss: 0.2396327190\n",
            "\n",
            "------>  Accuracy on TestSet: 58.9054  <------\n",
            "------>  Accuracy on TrainSet: 60.2069  <------\n",
            "\n",
            "Epoch:  6  Loss: 0.2394386377\n",
            "Epoch:  7  Loss: 0.2392704584\n",
            "Epoch:  8  Loss: 0.2392933408\n",
            "Epoch:  9  Loss: 0.2394645139\n",
            "Epoch:  10  Loss: 0.2394526203\n",
            "\n",
            "------>  Accuracy on TestSet: 59.0909  <------\n",
            "------>  Accuracy on TrainSet: 60.6684  <------\n",
            "\n",
            "Epoch:  11  Loss: 0.2394073538\n",
            "Epoch:  12  Loss: 0.2392477357\n",
            "Epoch:  13  Loss: 0.2394363187\n",
            "Epoch:  14  Loss: 0.2395269474\n",
            "Epoch:  15  Loss: 0.2393962642\n",
            "\n",
            "------>  Accuracy on TestSet: 59.7189  <------\n",
            "------>  Accuracy on TrainSet: 61.2892  <------\n",
            "\n",
            "Epoch:  16  Loss: 0.2393769782\n",
            "Epoch:  17  Loss: 0.2393879363\n",
            "Epoch:  18  Loss: 0.2394066578\n",
            "Epoch:  19  Loss: 0.2391357765\n",
            "Epoch:  20  Loss: 0.2389790017\n",
            "\n",
            "------>  Accuracy on TestSet: 59.6511  <------\n",
            "------>  Accuracy on TrainSet: 61.6471  <------\n",
            "\n",
            "Epoch:  21  Loss: 0.2393057405\n",
            "Epoch:  22  Loss: 0.2392144147\n",
            "Epoch:  23  Loss: 0.2391144138\n",
            "Epoch:  24  Loss: 0.2390543739\n",
            "Epoch:  25  Loss: 0.2393278745\n",
            "\n",
            "------>  Accuracy on TestSet: 59.8259  <------\n",
            "------>  Accuracy on TrainSet: 61.5175  <------\n",
            "\n",
            "Epoch:  26  Loss: 0.2393087361\n",
            "Epoch:  27  Loss: 0.2392599350\n",
            "Epoch:  28  Loss: 0.2392131920\n",
            "Epoch:  29  Loss: 0.2391758931\n",
            "Epoch:  30  Loss: 0.2391944153\n",
            "\n",
            "------>  Accuracy on TestSet: 59.0766  <------\n",
            "------>  Accuracy on TrainSet: 60.6886  <------\n",
            "\n",
            "Epoch:  31  Loss: 0.2392714583\n",
            "Epoch:  32  Loss: 0.2390511550\n",
            "Epoch:  33  Loss: 0.2395745090\n",
            "Epoch:  34  Loss: 0.2394390648\n",
            "Epoch:  35  Loss: 0.2393017398\n",
            "\n",
            "------>  Accuracy on TestSet: 58.7662  <------\n",
            "------>  Accuracy on TrainSet: 60.2878  <------\n",
            "\n",
            "Epoch:  36  Loss: 0.2393798108\n",
            "Epoch:  37  Loss: 0.2392181100\n",
            "Epoch:  38  Loss: 0.2393820306\n",
            "Epoch:  39  Loss: 0.2390631309\n",
            "Epoch:  40  Loss: 0.2390723207\n",
            "\n",
            "------>  Accuracy on TestSet: 59.2443  <------\n",
            "------>  Accuracy on TrainSet: 60.7540  <------\n",
            "\n",
            "Epoch:  41  Loss: 0.2394352681\n",
            "Epoch:  42  Loss: 0.2391725471\n",
            "Epoch:  43  Loss: 0.2390808759\n",
            "Epoch:  44  Loss: 0.2392294553\n",
            "Epoch:  45  Loss: 0.2392812373\n",
            "\n",
            "------>  Accuracy on TestSet: 59.5083  <------\n",
            "------>  Accuracy on TrainSet: 61.2416  <------\n",
            "\n",
            "Epoch:  46  Loss: 0.2392909086\n",
            "Epoch:  47  Loss: 0.2393027999\n",
            "Epoch:  48  Loss: 0.2391330403\n",
            "Epoch:  49  Loss: 0.2392421375\n",
            "Epoch:  50  Loss: 0.2250012630\n",
            "\n",
            "------>  Accuracy on TestSet: 62.8514  <------\n",
            "------>  Accuracy on TrainSet: 66.0486  <------\n",
            "\n",
            "Epoch:  51  Loss: 0.2158803687\n",
            "Epoch:  52  Loss: 0.2124854553\n",
            "Epoch:  53  Loss: 0.2093167549\n",
            "Epoch:  54  Loss: 0.2064854473\n",
            "Epoch:  55  Loss: 0.2047333236\n",
            "\n",
            "------>  Accuracy on TestSet: 67.4825  <------\n",
            "------>  Accuracy on TrainSet: 72.8168  <------\n",
            "\n",
            "Epoch:  56  Loss: 0.2027126642\n",
            "Epoch:  57  Loss: 0.2001377783\n",
            "Epoch:  58  Loss: 0.1978502163\n",
            "Epoch:  59  Loss: 0.1966663607\n",
            "Epoch:  60  Loss: 0.1953934846\n",
            "\n",
            "------>  Accuracy on TestSet: 68.9525  <------\n",
            "------>  Accuracy on TrainSet: 75.2631  <------\n",
            "\n",
            "Epoch:  61  Loss: 0.1940786386\n",
            "Epoch:  62  Loss: 0.1929649527\n",
            "Epoch:  63  Loss: 0.1928775824\n",
            "Epoch:  64  Loss: 0.1915652168\n",
            "Epoch:  65  Loss: 0.1901000418\n",
            "\n",
            "------>  Accuracy on TestSet: 68.5208  <------\n",
            "------>  Accuracy on TrainSet: 75.3666  <------\n",
            "\n",
            "Epoch:  66  Loss: 0.1905308264\n",
            "Epoch:  67  Loss: 0.1893947817\n",
            "Epoch:  68  Loss: 0.1878894421\n",
            "Epoch:  69  Loss: 0.1874408467\n",
            "Epoch:  70  Loss: 0.1874392732\n",
            "\n",
            "------>  Accuracy on TestSet: 69.3271  <------\n",
            "------>  Accuracy on TrainSet: 77.2278  <------\n",
            "\n",
            "Epoch:  71  Loss: 0.1867481241\n",
            "Epoch:  72  Loss: 0.1860992254\n",
            "Epoch:  73  Loss: 0.1850981445\n",
            "Epoch:  74  Loss: 0.1848985860\n",
            "Epoch:  75  Loss: 0.1846125068\n",
            "\n",
            "------>  Accuracy on TestSet: 68.9882  <------\n",
            "------>  Accuracy on TrainSet: 76.9876  <------\n",
            "\n",
            "Epoch:  76  Loss: 0.1843889800\n",
            "Epoch:  77  Loss: 0.1841361952\n",
            "Epoch:  78  Loss: 0.1837476891\n",
            "Epoch:  79  Loss: 0.1830351173\n",
            "Epoch:  80  Loss: 0.1832297175\n",
            "\n",
            "------>  Accuracy on TestSet: 68.4244  <------\n",
            "------>  Accuracy on TrainSet: 76.3275  <------\n",
            "\n",
            "Epoch:  81  Loss: 0.1822858381\n",
            "Epoch:  82  Loss: 0.1816337699\n",
            "Epoch:  83  Loss: 0.1825040377\n",
            "Epoch:  84  Loss: 0.1813772198\n",
            "Epoch:  85  Loss: 0.1819092686\n",
            "\n",
            "------>  Accuracy on TestSet: 68.1140  <------\n",
            "------>  Accuracy on TrainSet: 75.4629  <------\n",
            "\n",
            "Epoch:  86  Loss: 0.1811771598\n",
            "Epoch:  87  Loss: 0.1820043824\n",
            "Epoch:  88  Loss: 0.1805498520\n",
            "Epoch:  89  Loss: 0.1807280495\n",
            "Epoch:  90  Loss: 0.1803945972\n",
            "\n",
            "------>  Accuracy on TestSet: 69.4127  <------\n",
            "------>  Accuracy on TrainSet: 77.7547  <------\n",
            "\n",
            "Epoch:  91  Loss: 0.1797999733\n",
            "Epoch:  92  Loss: 0.1797847679\n",
            "Epoch:  93  Loss: 0.1790293191\n",
            "Epoch:  94  Loss: 0.1799753697\n",
            "Epoch:  95  Loss: 0.1793405534\n",
            "\n",
            "------>  Accuracy on TestSet: 68.4958  <------\n",
            "------>  Accuracy on TrainSet: 76.5487  <------\n",
            "\n",
            "Epoch:  96  Loss: 0.1787741397\n",
            "Epoch:  97  Loss: 0.1784815060\n",
            "Epoch:  98  Loss: 0.1793744288\n",
            "Epoch:  99  Loss: 0.1781812550\n",
            "Mean loss: 0.2142404410\n",
            "Estimated time: 54.132 min\n",
            "Epoch:  100  Loss: 0.1788192176\n",
            "\n",
            "------>  Accuracy on TestSet: 69.5055  <------\n",
            "------>  Accuracy on TrainSet: 77.7190  <------\n",
            "\n",
            "Epoch:  101  Loss: 0.1788751065\n",
            "Epoch:  102  Loss: 0.1781435038\n",
            "Epoch:  103  Loss: 0.1780328759\n",
            "Epoch:  104  Loss: 0.1788613216\n",
            "Epoch:  105  Loss: 0.1779581635\n",
            "\n",
            "------>  Accuracy on TestSet: 68.5921  <------\n",
            "------>  Accuracy on TrainSet: 76.7711  <------\n",
            "\n",
            "Epoch:  106  Loss: 0.1779645084\n",
            "Epoch:  107  Loss: 0.1771550304\n",
            "Epoch:  108  Loss: 0.1777668496\n",
            "Epoch:  109  Loss: 0.1770552779\n",
            "Epoch:  110  Loss: 0.1783093128\n",
            "\n",
            "------>  Accuracy on TestSet: 70.2262  <------\n",
            "------>  Accuracy on TrainSet: 79.0248  <------\n",
            "\n",
            "Epoch:  111  Loss: 0.1769480627\n",
            "Epoch:  112  Loss: 0.1771422709\n",
            "Epoch:  113  Loss: 0.1779090296\n",
            "Epoch:  114  Loss: 0.1767562299\n",
            "Epoch:  115  Loss: 0.1770295106\n",
            "\n",
            "------>  Accuracy on TestSet: 69.8873  <------\n",
            "------>  Accuracy on TrainSet: 78.5550  <------\n",
            "\n",
            "Epoch:  116  Loss: 0.1761882446\n",
            "Epoch:  117  Loss: 0.1761071154\n",
            "Epoch:  118  Loss: 0.1767849613\n",
            "Epoch:  119  Loss: 0.1769383056\n",
            "Epoch:  120  Loss: 0.1763970307\n",
            "\n",
            "------>  Accuracy on TestSet: 69.8230  <------\n",
            "------>  Accuracy on TrainSet: 78.6109  <------\n",
            "\n",
            "Epoch:  121  Loss: 0.1761718551\n",
            "Epoch:  122  Loss: 0.1764133348\n",
            "Epoch:  123  Loss: 0.1770180478\n",
            "Epoch:  124  Loss: 0.1758435439\n",
            "Epoch:  125  Loss: 0.1761278066\n",
            "\n",
            "------>  Accuracy on TestSet: 70.8363  <------\n",
            "------>  Accuracy on TrainSet: 80.2902  <------\n",
            "\n",
            "Epoch:  126  Loss: 0.1754905891\n",
            "Epoch:  127  Loss: 0.1759376691\n",
            "Epoch:  128  Loss: 0.1761343199\n",
            "Epoch:  129  Loss: 0.1757779872\n",
            "Epoch:  130  Loss: 0.1754487713\n",
            "\n",
            "------>  Accuracy on TestSet: 69.9372  <------\n",
            "------>  Accuracy on TrainSet: 78.8666  <------\n",
            "\n",
            "Epoch:  131  Loss: 0.1755896755\n",
            "Epoch:  132  Loss: 0.1758803962\n",
            "Epoch:  133  Loss: 0.1758825010\n",
            "Epoch:  134  Loss: 0.1754691942\n",
            "Epoch:  135  Loss: 0.1756504447\n",
            "\n",
            "------>  Accuracy on TestSet: 70.8684  <------\n",
            "------>  Accuracy on TrainSet: 80.5411  <------\n",
            "\n",
            "Epoch:  136  Loss: 0.1756307060\n",
            "Epoch:  137  Loss: 0.1754159908\n",
            "Epoch:  138  Loss: 0.1755995209\n",
            "Epoch:  139  Loss: 0.1758294001\n",
            "Epoch:  140  Loss: 0.1757626251\n",
            "\n",
            "------>  Accuracy on TestSet: 69.6197  <------\n",
            "------>  Accuracy on TrainSet: 78.4088  <------\n",
            "\n",
            "Epoch:  141  Loss: 0.1752801706\n",
            "Epoch:  142  Loss: 0.1755954131\n",
            "Epoch:  143  Loss: 0.1747345784\n",
            "Epoch:  144  Loss: 0.1751328361\n",
            "Epoch:  145  Loss: 0.1754361980\n",
            "\n",
            "------>  Accuracy on TestSet: 70.7935  <------\n",
            "------>  Accuracy on TrainSet: 80.2010  <------\n",
            "\n",
            "Epoch:  146  Loss: 0.1754844203\n",
            "Epoch:  147  Loss: 0.1752351775\n",
            "Epoch:  148  Loss: 0.1748872919\n",
            "Epoch:  149  Loss: 0.1750993639\n",
            "Epoch:  150  Loss: 0.1553585919\n",
            "\n",
            "------>  Accuracy on TestSet: 71.3572  <------\n",
            "------>  Accuracy on TrainSet: 81.6293  <------\n",
            "\n",
            "Epoch:  151  Loss: 0.1472709955\n",
            "Epoch:  152  Loss: 0.1431724385\n",
            "Epoch:  153  Loss: 0.1408102708\n",
            "Epoch:  154  Loss: 0.1380141078\n",
            "Epoch:  155  Loss: 0.1358107490\n",
            "\n",
            "------>  Accuracy on TestSet: 72.5417  <------\n",
            "------>  Accuracy on TrainSet: 84.6084  <------\n",
            "\n",
            "Epoch:  156  Loss: 0.1343615039\n",
            "Epoch:  157  Loss: 0.1328639259\n",
            "Epoch:  158  Loss: 0.1316206482\n",
            "Epoch:  159  Loss: 0.1306389232\n",
            "Epoch:  160  Loss: 0.1299729175\n",
            "\n",
            "------>  Accuracy on TestSet: 72.9735  <------\n",
            "------>  Accuracy on TrainSet: 85.6764  <------\n",
            "\n",
            "Epoch:  161  Loss: 0.1291772368\n",
            "Epoch:  162  Loss: 0.1273821516\n",
            "Epoch:  163  Loss: 0.1277413596\n",
            "Epoch:  164  Loss: 0.1271234293\n",
            "Epoch:  165  Loss: 0.1255573734\n",
            "\n",
            "------>  Accuracy on TestSet: 72.8379  <------\n",
            "------>  Accuracy on TrainSet: 85.9939  <------\n",
            "\n",
            "Epoch:  166  Loss: 0.1260996426\n",
            "Epoch:  167  Loss: 0.1245688412\n",
            "Epoch:  168  Loss: 0.1243769876\n",
            "Epoch:  169  Loss: 0.1246275739\n",
            "Epoch:  170  Loss: 0.1237185893\n",
            "\n",
            "------>  Accuracy on TestSet: 73.8190  <------\n",
            "------>  Accuracy on TrainSet: 87.7469  <------\n",
            "\n",
            "Epoch:  171  Loss: 0.1230249104\n",
            "Epoch:  172  Loss: 0.1232060421\n",
            "Epoch:  173  Loss: 0.1215115927\n",
            "Epoch:  174  Loss: 0.1224090505\n",
            "Epoch:  175  Loss: 0.1215826073\n",
            "\n",
            "------>  Accuracy on TestSet: 72.6416  <------\n",
            "------>  Accuracy on TrainSet: 85.9071  <------\n",
            "\n",
            "Epoch:  176  Loss: 0.1225274564\n",
            "Epoch:  177  Loss: 0.1216027586\n",
            "Epoch:  178  Loss: 0.1210988101\n",
            "Epoch:  179  Loss: 0.1209944653\n",
            "Epoch:  180  Loss: 0.1220869071\n",
            "\n",
            "------>  Accuracy on TestSet: 73.2910  <------\n",
            "------>  Accuracy on TrainSet: 87.3164  <------\n",
            "\n",
            "Epoch:  181  Loss: 0.1212998541\n",
            "Epoch:  182  Loss: 0.1208015821\n",
            "Epoch:  183  Loss: 0.1213358540\n",
            "Epoch:  184  Loss: 0.1204846281\n",
            "Epoch:  185  Loss: 0.1204274859\n",
            "\n",
            "------>  Accuracy on TestSet: 72.6773  <------\n",
            "------>  Accuracy on TrainSet: 85.8786  <------\n",
            "\n",
            "Epoch:  186  Loss: 0.1213086649\n",
            "Epoch:  187  Loss: 0.1201616790\n",
            "Epoch:  188  Loss: 0.1204238464\n",
            "Epoch:  189  Loss: 0.1199767766\n",
            "Epoch:  190  Loss: 0.1202432641\n",
            "\n",
            "------>  Accuracy on TestSet: 72.4026  <------\n",
            "------>  Accuracy on TrainSet: 85.6467  <------\n",
            "\n",
            "Epoch:  191  Loss: 0.1198935369\n",
            "Epoch:  192  Loss: 0.1195160637\n",
            "Epoch:  193  Loss: 0.1192725210\n",
            "Epoch:  194  Loss: 0.1186268185\n",
            "Epoch:  195  Loss: 0.1190731916\n",
            "\n",
            "------>  Accuracy on TestSet: 72.8521  <------\n",
            "------>  Accuracy on TrainSet: 87.0904  <------\n",
            "\n",
            "Epoch:  196  Loss: 0.1197670217\n",
            "Epoch:  197  Loss: 0.1193891705\n",
            "Epoch:  198  Loss: 0.1189421893\n",
            "Epoch:  199  Loss: 0.1191310322\n",
            "Mean loss: 0.1510148980\n",
            "Estimated time: 40.074 min\n",
            "Epoch:  200  Loss: 0.1185768171\n",
            "\n",
            "------>  Accuracy on TestSet: 74.4577  <------\n",
            "------>  Accuracy on TrainSet: 89.5142  <------\n",
            "\n",
            "Epoch:  201  Loss: 0.1200306718\n",
            "Epoch:  202  Loss: 0.1190703548\n",
            "Epoch:  203  Loss: 0.1195388942\n",
            "Epoch:  204  Loss: 0.1184353295\n",
            "Epoch:  205  Loss: 0.1194283242\n",
            "\n",
            "------>  Accuracy on TestSet: 72.8843  <------\n",
            "------>  Accuracy on TrainSet: 86.7265  <------\n",
            "\n",
            "Epoch:  206  Loss: 0.1184525330\n",
            "Epoch:  207  Loss: 0.1188048642\n",
            "Epoch:  208  Loss: 0.1188426718\n",
            "Epoch:  209  Loss: 0.1188333694\n",
            "Epoch:  210  Loss: 0.1182935631\n",
            "\n",
            "------>  Accuracy on TestSet: 73.1840  <------\n",
            "------>  Accuracy on TrainSet: 87.4223  <------\n",
            "\n",
            "Epoch:  211  Loss: 0.1182393096\n",
            "Epoch:  212  Loss: 0.1183109521\n",
            "Epoch:  213  Loss: 0.1182397063\n",
            "Epoch:  214  Loss: 0.1183808737\n",
            "Epoch:  215  Loss: 0.1186847057\n",
            "\n",
            "------>  Accuracy on TestSet: 74.5326  <------\n",
            "------>  Accuracy on TrainSet: 89.8579  <------\n",
            "\n",
            "Epoch:  216  Loss: 0.1170982017\n",
            "Epoch:  217  Loss: 0.1168091129\n",
            "Epoch:  218  Loss: 0.1177389253\n",
            "Epoch:  219  Loss: 0.1179826493\n",
            "Epoch:  220  Loss: 0.1186687924\n",
            "\n",
            "------>  Accuracy on TestSet: 74.7324  <------\n",
            "------>  Accuracy on TrainSet: 90.2872  <------\n",
            "\n",
            "Epoch:  221  Loss: 0.1178605293\n",
            "Epoch:  222  Loss: 0.1173114457\n",
            "Epoch:  223  Loss: 0.1177266001\n",
            "Epoch:  224  Loss: 0.1175550130\n",
            "Epoch:  225  Loss: 0.1179234388\n",
            "\n",
            "------>  Accuracy on TestSet: 71.7818  <------\n",
            "------>  Accuracy on TrainSet: 84.9165  <------\n",
            "\n",
            "Epoch:  226  Loss: 0.1180341879\n",
            "Epoch:  227  Loss: 0.1160941777\n",
            "Epoch:  228  Loss: 0.1168545497\n",
            "Epoch:  229  Loss: 0.1165791045\n",
            "Epoch:  230  Loss: 0.1166061392\n",
            "\n",
            "------>  Accuracy on TestSet: 73.9903  <------\n",
            "------>  Accuracy on TrainSet: 89.0123  <------\n",
            "\n",
            "Epoch:  231  Loss: 0.1166800785\n",
            "Epoch:  232  Loss: 0.1160775946\n",
            "Epoch:  233  Loss: 0.1163506444\n",
            "Epoch:  234  Loss: 0.1156547972\n",
            "Epoch:  235  Loss: 0.1171007716\n",
            "\n",
            "------>  Accuracy on TestSet: 74.5754  <------\n",
            "------>  Accuracy on TrainSet: 90.4347  <------\n",
            "\n",
            "Epoch:  236  Loss: 0.1168640286\n",
            "Epoch:  237  Loss: 0.1172203606\n",
            "Epoch:  238  Loss: 0.1164143157\n",
            "Epoch:  239  Loss: 0.1166763357\n",
            "Epoch:  240  Loss: 0.1152419274\n",
            "\n",
            "------>  Accuracy on TestSet: 74.6503  <------\n",
            "------>  Accuracy on TrainSet: 90.5381  <------\n",
            "\n",
            "Epoch:  241  Loss: 0.1159274490\n",
            "Epoch:  242  Loss: 0.1163804818\n",
            "Epoch:  243  Loss: 0.1166436923\n",
            "Epoch:  244  Loss: 0.1156307254\n",
            "Epoch:  245  Loss: 0.1158652796\n",
            "\n",
            "------>  Accuracy on TestSet: 74.4684  <------\n",
            "------>  Accuracy on TrainSet: 90.3395  <------\n",
            "\n",
            "Epoch:  246  Loss: 0.1167914090\n",
            "Epoch:  247  Loss: 0.1149898181\n",
            "Epoch:  248  Loss: 0.1144645675\n",
            "Epoch:  249  Loss: 0.1163374400\n",
            "Epoch:  250  Loss: 0.1155116171\n",
            "\n",
            "------>  Accuracy on TestSet: 73.5479  <------\n",
            "------>  Accuracy on TrainSet: 88.4189  <------\n",
            "\n",
            "Epoch:  251  Loss: 0.1158230482\n",
            "Epoch:  252  Loss: 0.1155119764\n",
            "Epoch:  253  Loss: 0.1147469213\n",
            "Epoch:  254  Loss: 0.1153284852\n",
            "Epoch:  255  Loss: 0.1148930922\n",
            "\n",
            "------>  Accuracy on TestSet: 72.8022  <------\n",
            "------>  Accuracy on TrainSet: 88.1977  <------\n",
            "\n",
            "Epoch:  256  Loss: 0.1146620621\n",
            "Epoch:  257  Loss: 0.1144456082\n",
            "Epoch:  258  Loss: 0.1158968628\n",
            "Epoch:  259  Loss: 0.1151498657\n",
            "Epoch:  260  Loss: 0.1158607575\n",
            "\n",
            "------>  Accuracy on TestSet: 74.5897  <------\n",
            "------>  Accuracy on TrainSet: 90.5940  <------\n",
            "\n",
            "Epoch:  261  Loss: 0.1146344501\n",
            "Epoch:  262  Loss: 0.1144962478\n",
            "Epoch:  263  Loss: 0.1150179372\n",
            "Epoch:  264  Loss: 0.1144774543\n",
            "Epoch:  265  Loss: 0.1144429763\n",
            "\n",
            "------>  Accuracy on TestSet: 74.5790  <------\n",
            "------>  Accuracy on TrainSet: 90.0910  <------\n",
            "\n",
            "Epoch:  266  Loss: 0.1139457608\n",
            "Epoch:  267  Loss: 0.1147504269\n",
            "Epoch:  268  Loss: 0.1144176024\n",
            "Epoch:  269  Loss: 0.1150678397\n",
            "Epoch:  270  Loss: 0.1134527753\n",
            "\n",
            "------>  Accuracy on TestSet: 74.8109  <------\n",
            "------>  Accuracy on TrainSet: 90.8581  <------\n",
            "\n",
            "Epoch:  271  Loss: 0.1135159445\n",
            "Epoch:  272  Loss: 0.1130657526\n",
            "Epoch:  273  Loss: 0.1140996080\n",
            "Epoch:  274  Loss: 0.1147504190\n",
            "Epoch:  275  Loss: 0.1128351525\n",
            "\n",
            "------>  Accuracy on TestSet: 74.7788  <------\n",
            "------>  Accuracy on TrainSet: 90.9591  <------\n",
            "\n",
            "Epoch:  276  Loss: 0.1135078402\n",
            "Epoch:  277  Loss: 0.1143485432\n",
            "Epoch:  278  Loss: 0.1130782012\n",
            "Epoch:  279  Loss: 0.1134515531\n",
            "Epoch:  280  Loss: 0.1133837424\n",
            "\n",
            "------>  Accuracy on TestSet: 73.8155  <------\n",
            "------>  Accuracy on TrainSet: 89.4297  <------\n",
            "\n",
            "Epoch:  281  Loss: 0.1141680398\n",
            "Epoch:  282  Loss: 0.1131598224\n",
            "Epoch:  283  Loss: 0.1136919761\n",
            "Epoch:  284  Loss: 0.1133326805\n",
            "Epoch:  285  Loss: 0.1137824602\n",
            "\n",
            "------>  Accuracy on TestSet: 74.5041  <------\n",
            "------>  Accuracy on TrainSet: 90.7284  <------\n",
            "\n",
            "Epoch:  286  Loss: 0.1121373364\n",
            "Epoch:  287  Loss: 0.1132276680\n",
            "Epoch:  288  Loss: 0.1133193683\n",
            "Epoch:  289  Loss: 0.1134093721\n",
            "Epoch:  290  Loss: 0.1133031369\n",
            "\n",
            "------>  Accuracy on TestSet: 72.1564  <------\n",
            "------>  Accuracy on TrainSet: 87.1832  <------\n",
            "\n",
            "Epoch:  291  Loss: 0.1129575053\n",
            "Epoch:  292  Loss: 0.1126689090\n",
            "Epoch:  293  Loss: 0.1134129607\n",
            "Epoch:  294  Loss: 0.1124056872\n",
            "Epoch:  295  Loss: 0.1137664104\n",
            "\n",
            "------>  Accuracy on TestSet: 74.6646  <------\n",
            "------>  Accuracy on TrainSet: 91.1090  <------\n",
            "\n",
            "Epoch:  296  Loss: 0.1117368144\n",
            "Epoch:  297  Loss: 0.1129133776\n",
            "Epoch:  298  Loss: 0.1132154909\n",
            "Epoch:  299  Loss: 0.1127243313\n",
            "Mean loss: 0.1156622140\n",
            "Estimated time: 26.670 min\n",
            "Epoch:  300  Loss: 0.1024545409\n",
            "\n",
            "------>  Accuracy on TestSet: 74.1508  <------\n",
            "------>  Accuracy on TrainSet: 90.6571  <------\n",
            "\n",
            "Epoch:  301  Loss: 0.0974601517\n",
            "Epoch:  302  Loss: 0.0971155000\n",
            "Epoch:  303  Loss: 0.0955757548\n",
            "Epoch:  304  Loss: 0.0943826033\n",
            "Epoch:  305  Loss: 0.0929193379\n",
            "\n",
            "------>  Accuracy on TestSet: 75.1534  <------\n",
            "------>  Accuracy on TrainSet: 92.1853  <------\n",
            "\n",
            "Epoch:  306  Loss: 0.0928374998\n",
            "Epoch:  307  Loss: 0.0922080758\n",
            "Epoch:  308  Loss: 0.0916418409\n",
            "Epoch:  309  Loss: 0.0903472789\n",
            "Epoch:  310  Loss: 0.0895947504\n",
            "\n",
            "------>  Accuracy on TestSet: 75.3461  <------\n",
            "------>  Accuracy on TrainSet: 92.0949  <------\n",
            "\n",
            "Epoch:  311  Loss: 0.0891500985\n",
            "Epoch:  312  Loss: 0.0897027140\n",
            "Epoch:  313  Loss: 0.0895112403\n",
            "Epoch:  314  Loss: 0.0896757661\n",
            "Epoch:  315  Loss: 0.0886799700\n",
            "\n",
            "------>  Accuracy on TestSet: 75.2819  <------\n",
            "------>  Accuracy on TrainSet: 92.0997  <------\n",
            "\n",
            "Epoch:  316  Loss: 0.0888068975\n",
            "Epoch:  317  Loss: 0.0882576001\n",
            "Epoch:  318  Loss: 0.0879100658\n",
            "Epoch:  319  Loss: 0.0874921067\n",
            "Epoch:  320  Loss: 0.0872942310\n",
            "\n",
            "------>  Accuracy on TestSet: 75.4281  <------\n",
            "------>  Accuracy on TrainSet: 92.5159  <------\n",
            "\n",
            "Epoch:  321  Loss: 0.0880422211\n",
            "Epoch:  322  Loss: 0.0875592249\n",
            "Epoch:  323  Loss: 0.0866442770\n",
            "Epoch:  324  Loss: 0.0864192141\n",
            "Epoch:  325  Loss: 0.0865114439\n",
            "\n",
            "------>  Accuracy on TestSet: 75.5245  <------\n",
            "------>  Accuracy on TrainSet: 92.5552  <------\n",
            "\n",
            "Epoch:  326  Loss: 0.0860754433\n",
            "Epoch:  327  Loss: 0.0862736852\n",
            "Epoch:  328  Loss: 0.0858587594\n",
            "Epoch:  329  Loss: 0.0855397947\n",
            "Epoch:  330  Loss: 0.0855725834\n",
            "\n",
            "------>  Accuracy on TestSet: 74.9822  <------\n",
            "------>  Accuracy on TrainSet: 91.5157  <------\n",
            "\n",
            "Epoch:  331  Loss: 0.0858671733\n",
            "Epoch:  332  Loss: 0.0851918261\n",
            "Epoch:  333  Loss: 0.0855205651\n",
            "Epoch:  334  Loss: 0.0849524489\n",
            "Epoch:  335  Loss: 0.0850106736\n",
            "\n",
            "------>  Accuracy on TestSet: 75.4317  <------\n",
            "------>  Accuracy on TrainSet: 92.3530  <------\n",
            "\n",
            "Epoch:  336  Loss: 0.0850029287\n",
            "Epoch:  337  Loss: 0.0849660516\n",
            "Epoch:  338  Loss: 0.0851141844\n",
            "Epoch:  339  Loss: 0.0851964141\n",
            "Epoch:  340  Loss: 0.0850142242\n",
            "\n",
            "------>  Accuracy on TestSet: 75.7849  <------\n",
            "------>  Accuracy on TrainSet: 92.6551  <------\n",
            "\n",
            "Epoch:  341  Loss: 0.0849030312\n",
            "Epoch:  342  Loss: 0.0842811856\n",
            "Epoch:  343  Loss: 0.0840571840\n",
            "Epoch:  344  Loss: 0.0842839872\n",
            "Epoch:  345  Loss: 0.0844024170\n",
            "\n",
            "------>  Accuracy on TestSet: 75.6386  <------\n",
            "------>  Accuracy on TrainSet: 92.4981  <------\n",
            "\n",
            "Epoch:  346  Loss: 0.0846763594\n",
            "Epoch:  347  Loss: 0.0836269856\n",
            "Epoch:  348  Loss: 0.0838028927\n",
            "Epoch:  349  Loss: 0.0836673576\n",
            "Epoch:  350  Loss: 0.0843558318\n",
            "\n",
            "------>  Accuracy on TestSet: 75.2533  <------\n",
            "------>  Accuracy on TrainSet: 91.8333  <------\n",
            "\n",
            "Epoch:  351  Loss: 0.0841363083\n",
            "Epoch:  352  Loss: 0.0825969411\n",
            "Epoch:  353  Loss: 0.0827876556\n",
            "Epoch:  354  Loss: 0.0839008374\n",
            "Epoch:  355  Loss: 0.0835780841\n",
            "\n",
            "------>  Accuracy on TestSet: 75.5280  <------\n",
            "------>  Accuracy on TrainSet: 92.3316  <------\n",
            "\n",
            "Epoch:  356  Loss: 0.0838268807\n",
            "Epoch:  357  Loss: 0.0830864586\n",
            "Epoch:  358  Loss: 0.0834747961\n",
            "Epoch:  359  Loss: 0.0837117462\n",
            "Epoch:  360  Loss: 0.0833801860\n",
            "\n",
            "------>  Accuracy on TestSet: 75.5352  <------\n",
            "------>  Accuracy on TrainSet: 92.4683  <------\n",
            "\n",
            "Epoch:  361  Loss: 0.0833249397\n",
            "Epoch:  362  Loss: 0.0837891783\n",
            "Epoch:  363  Loss: 0.0838803272\n",
            "Epoch:  364  Loss: 0.0829024122\n",
            "Epoch:  365  Loss: 0.0829994687\n",
            "\n",
            "------>  Accuracy on TestSet: 75.6672  <------\n",
            "------>  Accuracy on TrainSet: 92.6907  <------\n",
            "\n",
            "Epoch:  366  Loss: 0.0835801122\n",
            "Epoch:  367  Loss: 0.0826840437\n",
            "Epoch:  368  Loss: 0.0830849781\n",
            "Epoch:  369  Loss: 0.0828894562\n",
            "Epoch:  370  Loss: 0.0832403564\n",
            "\n",
            "------>  Accuracy on TestSet: 75.2176  <------\n",
            "------>  Accuracy on TrainSet: 92.0473  <------\n",
            "\n",
            "Epoch:  371  Loss: 0.0827914826\n",
            "Epoch:  372  Loss: 0.0831792984\n",
            "Epoch:  373  Loss: 0.0817530029\n",
            "Epoch:  374  Loss: 0.0819887038\n",
            "Epoch:  375  Loss: 0.0837313514\n",
            "\n",
            "------>  Accuracy on TestSet: 75.1427  <------\n",
            "------>  Accuracy on TrainSet: 91.8475  <------\n",
            "\n",
            "Epoch:  376  Loss: 0.0835029157\n",
            "Epoch:  377  Loss: 0.0821292902\n",
            "Epoch:  378  Loss: 0.0819937839\n",
            "Epoch:  379  Loss: 0.0824562398\n",
            "Epoch:  380  Loss: 0.0824675394\n",
            "\n",
            "------>  Accuracy on TestSet: 75.3282  <------\n",
            "------>  Accuracy on TrainSet: 92.2329  <------\n",
            "\n",
            "Epoch:  381  Loss: 0.0821239276\n",
            "Epoch:  382  Loss: 0.0818698325\n",
            "Epoch:  383  Loss: 0.0820517953\n",
            "Epoch:  384  Loss: 0.0820040333\n",
            "Epoch:  385  Loss: 0.0816084934\n",
            "\n",
            "------>  Accuracy on TestSet: 75.3853  <------\n",
            "------>  Accuracy on TrainSet: 92.5968  <------\n",
            "\n",
            "Epoch:  386  Loss: 0.0820931325\n",
            "Epoch:  387  Loss: 0.0824321519\n",
            "Epoch:  388  Loss: 0.0816716318\n",
            "Epoch:  389  Loss: 0.0820704994\n",
            "Epoch:  390  Loss: 0.0816192780\n",
            "\n",
            "------>  Accuracy on TestSet: 75.2854  <------\n",
            "------>  Accuracy on TrainSet: 92.0842  <------\n",
            "\n",
            "Epoch:  391  Loss: 0.0813166293\n",
            "Epoch:  392  Loss: 0.0821123248\n",
            "Epoch:  393  Loss: 0.0819174872\n",
            "Epoch:  394  Loss: 0.0813941412\n",
            "Epoch:  395  Loss: 0.0819561036\n",
            "\n",
            "------>  Accuracy on TestSet: 75.5673  <------\n",
            "------>  Accuracy on TrainSet: 92.8120  <------\n",
            "\n",
            "Epoch:  396  Loss: 0.0816426042\n",
            "Epoch:  397  Loss: 0.0815457099\n",
            "Epoch:  398  Loss: 0.0815319710\n",
            "Epoch:  399  Loss: 0.0812966992\n",
            "Mean loss: 0.0852851561\n",
            "Estimated time: 13.403 min\n",
            "Epoch:  400  Loss: 0.0817428952\n",
            "\n",
            "------>  Accuracy on TestSet: 75.3889  <------\n",
            "------>  Accuracy on TrainSet: 92.1936  <------\n",
            "\n",
            "Epoch:  401  Loss: 0.0814638454\n",
            "Epoch:  402  Loss: 0.0814187915\n",
            "Epoch:  403  Loss: 0.0819416388\n",
            "Epoch:  404  Loss: 0.0809547320\n",
            "Epoch:  405  Loss: 0.0802451517\n",
            "\n",
            "------>  Accuracy on TestSet: 75.2248  <------\n",
            "------>  Accuracy on TrainSet: 92.6348  <------\n",
            "\n",
            "Epoch:  406  Loss: 0.0817998242\n",
            "Epoch:  407  Loss: 0.0813706031\n",
            "Epoch:  408  Loss: 0.0813606795\n",
            "Epoch:  409  Loss: 0.0805466629\n",
            "Epoch:  410  Loss: 0.0816524553\n",
            "\n",
            "------>  Accuracy on TestSet: 75.4995  <------\n",
            "------>  Accuracy on TrainSet: 92.6455  <------\n",
            "\n",
            "Epoch:  411  Loss: 0.0808148967\n",
            "Epoch:  412  Loss: 0.0819304691\n",
            "Epoch:  413  Loss: 0.0812882634\n",
            "Epoch:  414  Loss: 0.0815945908\n",
            "Epoch:  415  Loss: 0.0809523154\n",
            "\n",
            "------>  Accuracy on TestSet: 75.0071  <------\n",
            "------>  Accuracy on TrainSet: 91.4265  <------\n",
            "\n",
            "Epoch:  416  Loss: 0.0809297183\n",
            "Epoch:  417  Loss: 0.0808801798\n",
            "Epoch:  418  Loss: 0.0810190691\n",
            "Epoch:  419  Loss: 0.0812341439\n",
            "Epoch:  420  Loss: 0.0815630016\n",
            "\n",
            "------>  Accuracy on TestSet: 75.5851  <------\n",
            "------>  Accuracy on TrainSet: 92.5147  <------\n",
            "\n",
            "Epoch:  421  Loss: 0.0814712172\n",
            "Epoch:  422  Loss: 0.0807325637\n",
            "Epoch:  423  Loss: 0.0806594488\n",
            "Epoch:  424  Loss: 0.0816128915\n",
            "Epoch:  425  Loss: 0.0807208549\n",
            "\n",
            "------>  Accuracy on TestSet: 75.5352  <------\n",
            "------>  Accuracy on TrainSet: 92.5516  <------\n",
            "\n",
            "Epoch:  426  Loss: 0.0812208805\n",
            "Epoch:  427  Loss: 0.0798395988\n",
            "Epoch:  428  Loss: 0.0811812158\n",
            "Epoch:  429  Loss: 0.0806016162\n",
            "Epoch:  430  Loss: 0.0796850056\n",
            "\n",
            "------>  Accuracy on TestSet: 75.4817  <------\n",
            "------>  Accuracy on TrainSet: 92.2257  <------\n",
            "\n",
            "Epoch:  431  Loss: 0.0805455301\n",
            "Epoch:  432  Loss: 0.0809229303\n",
            "Epoch:  433  Loss: 0.0809360626\n",
            "Epoch:  434  Loss: 0.0810680425\n",
            "Epoch:  435  Loss: 0.0812816526\n",
            "\n",
            "------>  Accuracy on TestSet: 75.6636  <------\n",
            "------>  Accuracy on TrainSet: 92.7442  <------\n",
            "\n",
            "Epoch:  436  Loss: 0.0802012317\n",
            "Epoch:  437  Loss: 0.0805206490\n",
            "Epoch:  438  Loss: 0.0804644835\n",
            "Epoch:  439  Loss: 0.0810580338\n",
            "Epoch:  440  Loss: 0.0804994496\n",
            "\n",
            "------>  Accuracy on TestSet: 75.5173  <------\n",
            "------>  Accuracy on TrainSet: 92.3387  <------\n",
            "\n",
            "Epoch:  441  Loss: 0.0802213190\n",
            "Epoch:  442  Loss: 0.0801224283\n",
            "Epoch:  443  Loss: 0.0792594742\n",
            "Epoch:  444  Loss: 0.0802537076\n",
            "Epoch:  445  Loss: 0.0804992448\n",
            "\n",
            "------>  Accuracy on TestSet: 75.4174  <------\n",
            "------>  Accuracy on TrainSet: 92.6336  <------\n",
            "\n",
            "Epoch:  446  Loss: 0.0805000686\n",
            "Epoch:  447  Loss: 0.0805020659\n",
            "Epoch:  448  Loss: 0.0798912535\n",
            "Epoch:  449  Loss: 0.0800568991\n",
            "Epoch:  450  Loss: 0.0808413259\n",
            "\n",
            "------>  Accuracy on TestSet: 75.5387  <------\n",
            "------>  Accuracy on TrainSet: 92.7466  <------\n",
            "\n",
            "Epoch:  451  Loss: 0.0803954610\n",
            "Epoch:  452  Loss: 0.0795750378\n",
            "Epoch:  453  Loss: 0.0795211804\n",
            "Epoch:  454  Loss: 0.0798342246\n",
            "Epoch:  455  Loss: 0.0804509518\n",
            "\n",
            "------>  Accuracy on TestSet: 74.9179  <------\n",
            "------>  Accuracy on TrainSet: 91.1946  <------\n",
            "\n",
            "Epoch:  456  Loss: 0.0809457640\n",
            "Epoch:  457  Loss: 0.0796454863\n",
            "Epoch:  458  Loss: 0.0798765830\n",
            "Epoch:  459  Loss: 0.0799052096\n",
            "Epoch:  460  Loss: 0.0801393625\n",
            "\n",
            "------>  Accuracy on TestSet: 75.2640  <------\n",
            "------>  Accuracy on TrainSet: 92.7383  <------\n",
            "\n",
            "Epoch:  461  Loss: 0.0792388154\n",
            "Epoch:  462  Loss: 0.0799671423\n",
            "Epoch:  463  Loss: 0.0801549778\n",
            "Epoch:  464  Loss: 0.0799710580\n",
            "Epoch:  465  Loss: 0.0792723703\n",
            "\n",
            "------>  Accuracy on TestSet: 75.5031  <------\n",
            "------>  Accuracy on TrainSet: 92.7514  <------\n",
            "\n",
            "Epoch:  466  Loss: 0.0793785636\n",
            "Epoch:  467  Loss: 0.0792081964\n",
            "Epoch:  468  Loss: 0.0798774842\n",
            "Epoch:  469  Loss: 0.0799462098\n",
            "Epoch:  470  Loss: 0.0798214840\n",
            "\n",
            "------>  Accuracy on TestSet: 75.1891  <------\n",
            "------>  Accuracy on TrainSet: 92.6860  <------\n",
            "\n",
            "Epoch:  471  Loss: 0.0794733331\n",
            "Epoch:  472  Loss: 0.0798047774\n",
            "Epoch:  473  Loss: 0.0796337738\n",
            "Epoch:  474  Loss: 0.0798306022\n",
            "Epoch:  475  Loss: 0.0799022872\n",
            "\n",
            "------>  Accuracy on TestSet: 75.5637  <------\n",
            "------>  Accuracy on TrainSet: 92.7787  <------\n",
            "\n",
            "Epoch:  476  Loss: 0.0794160309\n",
            "Epoch:  477  Loss: 0.0800241579\n",
            "Epoch:  478  Loss: 0.0796698167\n",
            "Epoch:  479  Loss: 0.0795477123\n",
            "Epoch:  480  Loss: 0.0794578352\n",
            "\n",
            "------>  Accuracy on TestSet: 75.4924  <------\n",
            "------>  Accuracy on TrainSet: 92.7775  <------\n",
            "\n",
            "Epoch:  481  Loss: 0.0798904432\n",
            "Epoch:  482  Loss: 0.0796050027\n",
            "Epoch:  483  Loss: 0.0810471057\n",
            "Epoch:  484  Loss: 0.0798115746\n",
            "Epoch:  485  Loss: 0.0795519966\n",
            "\n",
            "------>  Accuracy on TestSet: 75.3211  <------\n",
            "------>  Accuracy on TrainSet: 92.2590  <------\n",
            "\n",
            "Epoch:  486  Loss: 0.0802602662\n",
            "Epoch:  487  Loss: 0.0791797384\n",
            "Epoch:  488  Loss: 0.0800408705\n",
            "Epoch:  489  Loss: 0.0798005891\n",
            "Epoch:  490  Loss: 0.0801779847\n",
            "\n",
            "------>  Accuracy on TestSet: 75.5102  <------\n",
            "------>  Accuracy on TrainSet: 92.6443  <------\n",
            "\n",
            "Epoch:  491  Loss: 0.0799030633\n",
            "Epoch:  492  Loss: 0.0791858153\n",
            "Epoch:  493  Loss: 0.0793450990\n",
            "Epoch:  494  Loss: 0.0789170728\n",
            "Epoch:  495  Loss: 0.0798696306\n",
            "\n",
            "------>  Accuracy on TestSet: 75.7207  <------\n",
            "------>  Accuracy on TrainSet: 92.8989  <------\n",
            "\n",
            "Epoch:  496  Loss: 0.0800679211\n",
            "Epoch:  497  Loss: 0.0788063781\n",
            "Epoch:  498  Loss: 0.0790434306\n",
            "Epoch:  499  Loss: 0.0789222389\n",
            "Mean loss: 0.0803138718\n",
            "Estimated time: 0.133 min\n",
            "Finished Training (elapsed time 66.137 min)\n",
            "\n",
            "Best net loaded!!\n",
            "\n",
            "Best Test Accuracy:75.78490 \n",
            "Epoch of best accuracy:340 \n",
            "Final Accuracy on Train:92.89890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoNg-Wmi4h6Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}